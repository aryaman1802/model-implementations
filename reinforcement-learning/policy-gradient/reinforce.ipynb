{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **REINFORCE Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The code in this notebook is based on [Nimish Sanghi's book on Deep RL](https://link.springer.com/book/10.1007/979-8-8688-0273-7).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to check out this notebook on Kaggle: https://www.kaggle.com/code/aryamanbansal/reinforce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Overview of REINFORCE**\n",
    "\n",
    "**REINFORCE** is one of the earliest policy gradient methods in reinforcement learning. It is a **Monte Carlo** algorithm that estimates the gradient of the expected reward by sampling complete episodes. The key idea is to update the policy parameters in the direction that increases the likelihood of actions that produced high rewards.\n",
    "\n",
    "#### **Core Objective**\n",
    "\n",
    "We want to maximize the expected total reward (or return) given by the objective function:\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]\n",
    "$$\n",
    "where:\n",
    "- $ \\pi_\\theta(a|s) $ is the policy parameterized by $ \\theta $,\n",
    "- $ \\tau $ is a trajectory $ (s_0, a_0, s_1, a_1, \\dots, s_T, a_T) $,\n",
    "- $ R(\\tau) $ is the cumulative reward (possibly discounted) received along trajectory $ \\tau $.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Deriving the REINFORCE Gradient**\n",
    "\n",
    "#### **Step 1: Write the Objective as an Expectation**\n",
    "\n",
    "We express the expected return as an integral (or sum in the discrete case):\n",
    "$$\n",
    "J(\\theta) = \\int \\pi_\\theta(\\tau) R(\\tau) \\, d\\tau\n",
    "$$\n",
    "Here, $ \\pi_\\theta(\\tau) $ is the probability of observing trajectory $ \\tau $ under policy $ \\pi_\\theta $.\n",
    "\n",
    "#### **Step 2: Differentiate the Objective**\n",
    "\n",
    "To maximize $ J(\\theta) $, we need its gradient:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\int \\pi_\\theta(\\tau) R(\\tau) \\, d\\tau\n",
    "$$\n",
    "Assuming we can interchange the gradient and the integral (which is standard under regularity conditions), we get:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\int \\nabla_\\theta \\pi_\\theta(\\tau) R(\\tau) \\, d\\tau\n",
    "$$\n",
    "\n",
    "#### **Step 3: Introduce the Log-Derivative Trick**\n",
    "\n",
    "Directly differentiating $ \\pi_\\theta(\\tau) $ can be tricky. The log-derivative trick (also known as the likelihood ratio method) simplifies this:\n",
    "$$\n",
    "\\nabla_\\theta \\pi_\\theta(\\tau) = \\pi_\\theta(\\tau) \\nabla_\\theta \\log \\pi_\\theta(\\tau)\n",
    "$$\n",
    "Substitute this back into the gradient:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\int \\pi_\\theta(\\tau) \\nabla_\\theta \\log \\pi_\\theta(\\tau) R(\\tau) \\, d\\tau\n",
    "$$\n",
    "Since the integral weighted by $ \\pi_\\theta(\\tau) $ is an expectation, we write:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(\\tau) \\, R(\\tau) \\right]\n",
    "$$\n",
    "\n",
    "#### **Step 4: Decomposing the Trajectory Probability**\n",
    "\n",
    "A trajectory $ \\tau $ is a sequence of states and actions:\n",
    "$$\n",
    "\\tau = (s_0, a_0, s_1, a_1, \\dots, s_T, a_T)\n",
    "$$\n",
    "Under the Markov property, the probability of a trajectory can be factorized as:\n",
    "$$\n",
    "\\pi_\\theta(\\tau) = p(s_0) \\prod_{t=0}^{T} \\pi_\\theta(a_t|s_t) \\, p(s_{t+1}|s_t, a_t)\n",
    "$$\n",
    "Since the environment dynamics $ p(s_{t+1}|s_t, a_t) $ and the initial state distribution $ p(s_0) $ do not depend on $ \\theta $, only the policy terms matter when we differentiate with respect to $ \\theta $.\n",
    "\n",
    "Taking the logarithm, we have:\n",
    "$$\n",
    "\\log \\pi_\\theta(\\tau) = \\log p(s_0) + \\sum_{t=0}^{T} \\left[ \\log \\pi_\\theta(a_t|s_t) + \\log p(s_{t+1}|s_t, a_t) \\right]\n",
    "$$\n",
    "Because $ \\log p(s_0) $ and $ \\log p(s_{t+1}|s_t, a_t) $ are independent of $ \\theta $, their gradients vanish. Therefore:\n",
    "$$\n",
    "\\nabla_\\theta \\log \\pi_\\theta(\\tau) = \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\n",
    "$$\n",
    "\n",
    "#### **Step 5: Final Form of the Gradient**\n",
    "\n",
    "Substitute the decomposition back into our gradient expression:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\left( \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right) R(\\tau) \\right]\n",
    "$$\n",
    "\n",
    "#### **Step 6: Variance Reduction via Rewards-to-Go**\n",
    "\n",
    "Using the total return $ R(\\tau) $ for every time step can introduce high variance. To reduce this, we often replace $ R(\\tau) $ with the **reward-to-go** $ G_t $ defined as:\n",
    "$$\n",
    "G_t = \\sum_{k=t}^{T} \\gamma^{k-t} r_k\n",
    "$$\n",
    "Now the gradient becomes:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\, G_t \\right]\n",
    "$$\n",
    "This tells us to update the parameters based on how each action contributed to the future rewards (with discounting $ \\gamma $).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Intuitive Explanation**\n",
    "\n",
    "Imagine you are learning a recipe:\n",
    "- **Trajectory:** Each complete cooking session is a trajectory - a series of steps (actions) you take (mixing, frying, etc.).\n",
    "- **Reward:** At the end, you taste the dish (the overall reward).\n",
    "- **Adjustment:** If the dish is great (high reward), you want to remember which steps helped make it good. The algorithm \"nudges\" your recipe to favor those steps.\n",
    "\n",
    "The key idea is to **increase the probability of actions** that led to high rewards. By computing the gradient of the log probability for each action and weighting it by the future rewards (or rewards-to-go), you tell the algorithm, \"These actions were good; do them more often!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Summary**\n",
    "\n",
    "- **Derivation:**  \n",
    "  Starting from the expected return, we used the log-derivative trick to derive the gradient estimator:\n",
    "  $$\n",
    "  \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\, G_t \\right]\n",
    "  $$\n",
    "- **Intuition:**  \n",
    "  Actions that lead to higher rewards are reinforced by increasing their log probabilities, while poor actions are discouraged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **REINFORCE with Rewards-to-Go Algorithm**\n",
    "\n",
    "**Given:**\n",
    "- A parameterized policy $ \\pi_\\theta(a|s) $\n",
    "- Discount factor $ \\gamma \\in [0,1] $\n",
    "- Learning rate $ \\alpha $\n",
    "- Environment $ \\mathcal{E} $\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "1. **Initialize** policy parameters $ \\theta $.\n",
    "\n",
    "2. **For** episode $ i = 1, 2, \\dots, N $:\n",
    "   1. Initialize state $ s_0 $.\n",
    "   2. **For** $ t = 0, 1, \\dots, T $ (until terminal state):\n",
    "      - Sample action $ a_t \\sim \\pi_\\theta(\\cdot|s_t) $.\n",
    "      - Execute $ a_t $, observe reward $ r_t $ and next state $ s_{t+1} $.\n",
    "   3. **Compute rewards-to-go:**  \n",
    "      $$\n",
    "      G_t = \\sum_{k=t}^{T} \\gamma^{k-t} r_k \\quad \\text{for } t=0,\\ldots,T.\n",
    "      $$\n",
    "   4. **Policy Update:**  \n",
    "      Update parameters using:\n",
    "      $$\n",
    "      \\theta \\leftarrow \\theta + \\alpha \\, \\nabla_\\theta J(\\theta)\n",
    "      $$\n",
    "      where\n",
    "      $$\n",
    "      \\nabla_\\theta J(\\theta) \\approx \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\, G_t.\n",
    "      $$\n",
    "\n",
    "3. **Repeat** until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Final Remarks**\n",
    "\n",
    "\n",
    "#### **Rewards-to-Go Trick**\n",
    "\n",
    "- **Basic Idea:**  \n",
    "  When you take an action at time *t*, you cannot affect the rewards that occurred **before** time *t*. Therefore, only the rewards from time *t* onward should be considered. This is why we sum the rewards starting at time *t* (instead of from the beginning) to compute the “reward-to-go.”\n",
    "  \n",
    "- **Mathematical Expression:**  \n",
    "  Instead of summing rewards from time 1 to *T* for every action, we modify the gradient expression so that for each time *t*, we sum only over rewards from *t* to *T*. Formally:\n",
    "  \n",
    "  $$\n",
    "  \\nabla_{\\theta} J(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\sum_{t=1}^{T} \\left( \\nabla_{\\theta} \\log \\pi_\\theta(a_t^i|s_t^i) \\sum_{t'=t}^{T} \\gamma^{t'-t} \\, r(s_{t'}^i, a_{t'}^i) \\right) \\right]\n",
    "  $$\n",
    "  \n",
    "  This means that the effect of an action at time *t* is weighted by the sum of future (discounted) rewards starting from that time.\n",
    "\n",
    "#### **Implementing Loss and Gradient Step in PyTorch**\n",
    "\n",
    "- **Pseudo Loss Function:**  \n",
    "  In practice, we want to maximize the expected reward. However, most deep learning libraries (like PyTorch) minimize a loss function. Therefore, we define a pseudo loss as the negative of our reward objective.\n",
    "  \n",
    "- **Loss Function for REINFORCE with Rewards-to-Go:**  \n",
    "  The loss is formulated as:\n",
    "  \n",
    "  $$\n",
    "  L_{\\text{Cross-entropy}}(\\theta) = - J(\\theta) = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\sum_{t=1}^{T} \\left( \\log \\pi_\\theta(a_t^i|s_t^i) \\sum_{t'=t}^{T} \\gamma^{t'-t} \\, r(s_{t'}^i, a_{t'}^i) \\right) \\right]\n",
    "  $$\n",
    "  \n",
    "  This means that for every action taken, we calculate the negative log probability (as in cross-entropy) and weight it by the rewards-to-go.\n",
    "\n",
    "- **Entropy Regularization:**  \n",
    "  To prevent the policy from becoming too certain too quickly (which would limit exploration), an entropy term is added. The entropy of a probability distribution is defined as:\n",
    "  \n",
    "  $$\n",
    "  H(X) = - \\sum_x p(x) \\log p(x)\n",
    "  $$\n",
    "  \n",
    "  A higher entropy means a more spread-out (uncertain) distribution. We add a regularization term (scaled by a coefficient $\\beta$) to encourage exploration.\n",
    "\n",
    "- **Combined Loss:**  \n",
    "  The total loss (which is minimized) becomes:\n",
    "  \n",
    "  $$\n",
    "  Loss(\\theta) = - J(\\theta) - H(\\pi_\\theta(a_t^i|s_t^i)) = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\sum_{t=1}^{T} \\left( \\log \\pi_\\theta(a_t^i|s_t^i) \\sum_{t'=t}^{T} \\gamma^{t'-t} \\, r(s_{t'}^i, a_{t'}^i) \\right) - \\beta \\sum_{a_i} \\pi_\\theta(a_t^i|s_t^i) \\log \\pi_\\theta(a_t^i|s_t^i) \\right]\n",
    "  $$\n",
    "  \n",
    "  In simple terms:\n",
    "  - We compute the (negative) weighted log probabilities of the actions.\n",
    "  - We weight each by its corresponding future rewards (reward-to-go).\n",
    "  - We add an entropy term to keep the policy \"spread out\" (promoting exploration).\n",
    "  - Finally, we perform a gradient descent step on this loss to update our policy parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports done!\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.signal import convolve\n",
    "from scipy.signal.windows import gaussian\n",
    "\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML, clear_output\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "print(\"imports done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2a8911765f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set a seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming a global device setting (CPU or CUDA)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=192):\n",
    "        super(Agent, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(agent, states):\n",
    "    \"\"\"\n",
    "    Compute action probabilities for a given batch of states using the model.\n",
    "    \n",
    "    Parameters:\n",
    "        agent (nn.Module): The neural network model of the agent.\n",
    "        states (list or array): A batch of states with shape [batch, state_dim].\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: An array of probabilities with shape [batch, n_actions].\n",
    "    \"\"\"\n",
    "    # Convert the states to a PyTorch tensor and send to device\n",
    "    states = torch.tensor(np.array(states), device=device, dtype=torch.float32)\n",
    "    \n",
    "    # Compute the logits from the model without tracking gradients\n",
    "    with torch.no_grad():\n",
    "        logits = agent(states)\n",
    "    \n",
    "    # Convert logits to probabilities using softmax and return them as numpy array\n",
    "    probs = nn.functional.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(env, agent, n_steps=1000):\n",
    "    \"\"\"\n",
    "    Generate a trajectory (sequence of states, actions, rewards) by running the policy.\n",
    "    \n",
    "    Parameters:\n",
    "        env: The environment instance.\n",
    "        agent: The agent model.\n",
    "        n_steps (int): Maximum number of steps to run in the episode.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Arrays of states, actions, and rewards collected during the episode.\n",
    "    \"\"\"\n",
    "    states, actions, rewards = [], [], []\n",
    "\n",
    "    # Initialize the environment and get the initial state.\n",
    "    s, _ = env.reset(seed=seed)    \n",
    "    \n",
    "    # Generate the trajectory for a maximum of n_steps or until termination.\n",
    "    for t in range(n_steps):\n",
    "        # Get action probabilities for the current state.\n",
    "        action_probs = predict_probs(agent, np.array([s]))[0]\n",
    "        \n",
    "        # Sample an action from the probability distribution.\n",
    "        n_actions = env.action_space.n\n",
    "        a = np.random.choice(n_actions, p=action_probs)\n",
    "        \n",
    "        # Execute the action and observe the next state, reward, and done flag.\n",
    "        next_state, r, done, _, _ = env.step(a)\n",
    "        \n",
    "        # Store the current state, chosen action, and obtained reward.\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        \n",
    "        s = next_state  # Update state to the next state.\n",
    "        if done:\n",
    "            break  # End the trajectory if the environment signals termination.\n",
    "    \n",
    "    return np.array(states), np.array(actions), np.array(rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=3, t_max=10000):\n",
    "    \"\"\"\n",
    "    Evaluate the current policy by averaging returns over several games.\n",
    "    \n",
    "    Parameters:\n",
    "        env: The environment instance for evaluation.\n",
    "        agent: The policy model (not directly used in this function).\n",
    "        n_games (int): Number of games to run for evaluation.\n",
    "        t_max (int): Maximum number of steps per game.\n",
    "    \n",
    "    Returns:\n",
    "        float: Mean return (total reward) over the evaluation games.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for i in range(n_games):\n",
    "        # Reset the environment with a different seed for each evaluation game.\n",
    "        s, _ = env.reset(seed=seed + i)\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            # Get action probabilities for the current state.\n",
    "            action_probs = predict_probs(agent, np.array([s]))[0]\n",
    "            # Sample an action based on the probabilities.\n",
    "            n_actions = env.action_space.n\n",
    "            a = np.random.choice(n_actions, p=action_probs)\n",
    "            # Execute the action and obtain the next state and reward.\n",
    "            next_state, r, terminated, _, _ = env.step(a)\n",
    "            reward += r\n",
    "            s = next_state\n",
    "            if terminated:\n",
    "                break  # End the game if terminated.\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rewards_to_go(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Compute the rewards-to-go for a given sequence of rewards.\n",
    "    \n",
    "    The rewards-to-go at time t is the discounted sum of rewards from time t onward.\n",
    "    \n",
    "    Parameters:\n",
    "        rewards (list): List of rewards for each time step in the trajectory.\n",
    "        gamma (float): Discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        list: Rewards-to-go for each time step.\n",
    "    \"\"\"\n",
    "    T = len(rewards)  # Total number of rewards.\n",
    "    rewards_to_go = [0] * T  # Initialize the rewards-to-go list.\n",
    "    rewards_to_go[T - 1] = rewards[T - 1]  # The last time step's reward-to-go is just its reward.\n",
    "    \n",
    "    # Iterate backwards from the second last reward to the first.\n",
    "    for i in range(T - 2, -1, -1):\n",
    "        rewards_to_go[i] = gamma * rewards_to_go[i + 1] + rewards[i]\n",
    "    \n",
    "    return rewards_to_go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_episode(states, actions, rewards, agent, optimizer,\n",
    "                      gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Train the policy network on one trajectory (episode) using the REINFORCE algorithm\n",
    "    with rewards-to-go and an entropy regularization term.\n",
    "    \n",
    "    The loss is computed as the negative of:\n",
    "        J(θ) = (1/T) * sum_t [ log π(a_t|s_t) * (reward-to-go)_t ]\n",
    "    plus an entropy bonus to encourage exploration.\n",
    "    \n",
    "    Parameters:\n",
    "        states (np.array): Array of states encountered in the episode.\n",
    "        actions (np.array): Array of actions taken in the episode.\n",
    "        rewards (np.array): Array of rewards received in the episode.\n",
    "        gamma (float): Discount factor for computing rewards-to-go.\n",
    "        entropy_coef (float): Coefficient for the entropy regularization term.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The loss value (detached) for monitoring training progress.\n",
    "    \"\"\"\n",
    "    # Compute rewards-to-go for the trajectory.\n",
    "    rewards_to_go = get_rewards_to_go(rewards, gamma)\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors and move them to the proper device.\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float)\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.long)\n",
    "    rewards_to_go = torch.tensor(rewards_to_go, device=device, dtype=torch.float)\n",
    "\n",
    "    # Pass the states through the model to obtain logits.\n",
    "    logits = agent(states)\n",
    "    # Convert logits to probabilities.\n",
    "    probs = nn.functional.softmax(logits, dim=-1)\n",
    "    # Compute the log probabilities.\n",
    "    log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # Select the log probabilities corresponding to the actions taken.\n",
    "    log_probs_for_actions = log_probs[range(len(actions)), actions]\n",
    "    \n",
    "    # Compute the objective J(θ) (average over the episode).\n",
    "    J = torch.mean(log_probs_for_actions * rewards_to_go)\n",
    "    # Compute the entropy of the policy to encourage exploration.\n",
    "    H = -(probs * log_probs).sum(dim=-1).mean()\n",
    "    \n",
    "    # The loss to minimize is the negative of (J + entropy bonus).\n",
    "    loss = -(J + entropy_coef * H)\n",
    "\n",
    "    # Backpropagation: clear gradients, compute new gradients, and update parameters.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Return the loss value (detached from the computation graph).\n",
    "    return loss.detach().cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothen(values):\n",
    "    \"\"\"\n",
    "    Smooths out the given values using a Gaussian filter.\n",
    "\n",
    "    Args:\n",
    "        values (list or np.array): The sequence of values to smooth.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The smoothed values.\n",
    "    \"\"\"\n",
    "    kernel = gaussian(100, std=100)\n",
    "    kernel = kernel / np.sum(kernel)\n",
    "    return convolve(values, kernel, 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train_loop(env, env_name, agent, optimizer, n_iterations=1000, \n",
    "                    eval_freq=10, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Main training loop for the REINFORCE algorithm.\n",
    "\n",
    "    This function iteratively generates trajectories from the environment,\n",
    "    trains the policy network on each trajectory using the REINFORCE update\n",
    "    with rewards-to-go and an entropy regularization term, and periodically\n",
    "    evaluates the policy performance.\n",
    "\n",
    "    Parameters:\n",
    "        env (gym.Env): The training environment.\n",
    "        env_name (str): Name of the environment (used for evaluation environment creation).\n",
    "        agent (torch.nn.Module): The policy network to be trained.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used to update the policy network.\n",
    "        n_iterations (int): Total number of training iterations (episodes).\n",
    "        eval_freq (int): Frequency (in iterations) at which to evaluate the policy.\n",
    "        gamma (float): Discount factor for computing rewards-to-go.\n",
    "        entropy_coef (float): Coefficient for the entropy regularization term.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "            - loss_history (list): Loss values recorded at each training iteration.\n",
    "            - return_history (list): Mean returns recorded at evaluation steps.\n",
    "    \"\"\"\n",
    "    # Initialize histories for monitoring progress\n",
    "    loss_history = []\n",
    "    return_history = []\n",
    "    \n",
    "    # Loop over the specified number of iterations (episodes)\n",
    "    for i in trange(n_iterations):\n",
    "        # Generate one trajectory (episode) from the environment\n",
    "        states, actions, rewards = generate_trajectory(env, agent)\n",
    "        \n",
    "        # Train the model on the collected trajectory\n",
    "        loss = train_one_episode(states, actions, rewards, agent, optimizer,\n",
    "                                 gamma=gamma, entropy_coef=entropy_coef)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # Periodically evaluate the policy\n",
    "        if i != 0 and i % eval_freq == 0:\n",
    "            # Create a new evaluation environment instance (assumes make_env is defined)\n",
    "            eval_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "            mean_return = evaluate(eval_env, agent)\n",
    "            return_history.append(mean_return)\n",
    "            eval_env.close()\n",
    "            \n",
    "            # Optionally, clear output and plot progress (if running in a notebook)\n",
    "            clear_output(wait=True)\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.figure(figsize=[16, 5])\n",
    "            \n",
    "            # Plot mean return per episode\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.title(\"Mean return per episode\")\n",
    "            plt.plot(return_history)\n",
    "            plt.grid()\n",
    "            \n",
    "            # Plot loss history (smoothened)\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.title(\"Loss history (smoothened)\")\n",
    "            plt.plot(smoothen(loss_history))\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "    \n",
    "    return loss_history, return_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setting up the training parameters**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the environment and agent networks\n",
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\", max_episode_steps=4000)    # Create the environment\n",
    "state_dim = env.observation_space.shape     # e.g., (4,) for CartPole\n",
    "n_actions = env.action_space.n              # e.g., 2 for CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment and set seed for reproducibility\n",
    "state, _ = env.reset(seed=seed)\n",
    "\n",
    "# Initialize DQN agent with initial high exploration (epsilon=1)\n",
    "agent = Agent(state_dim, n_actions).to(device)\n",
    "\n",
    "# Initialize the optimizer (Adam) for updating the agent's parameters\n",
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-3)\n",
    "\n",
    "# Define frequencies for logging and updating the target network\n",
    "eval_freq = 10                    # Evaluate the agent every 10 steps\n",
    "n_episodes = 5000                 # Total number of episodes to train the agent\n",
    "\n",
    "loss_history = []                 # Initialize a list to store the loss values\n",
    "return_history = []               # Initialize a list to store the return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Applying DQN on the CartPole**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_train_loop(env, env_name, agent, opt, n_episodes, n_episodes, eval_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = evaluate(\n",
    "  gym.make(env_name, render_mode=\"rgb_array\", max_episode_steps=4000),\n",
    "  agent, n_games=30, t_max=1000\n",
    ")\n",
    "print('final score:', final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
