{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction to Policy Gradient Algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Motivation**\n",
    "\n",
    "In reinforcement learning (RL), **policy-based methods** directly parameterize the policy (ie, the mapping from states to actions) and optimize these parameters by maximizing the expected reward. This is in contrast to value-based methods, which learn a value function to indirectly derive a policy. Policy gradient methods are particularly useful in continuous or high-dimensional action spaces, or when you want a stochastic policy.\n",
    "\n",
    "### **High-Level Overview**\n",
    "\n",
    "**Pros of Policy-Based Methods:**\n",
    "- **Direct Optimization:** They optimize the policy directly, which can yield smoother updates.\n",
    "- **Stochastic Policies:** Naturally handle stochastic policies, which can be beneficial in environments with uncertainty.\n",
    "- **Continuous Actions:** They work well in continuous action spaces where value-based methods might struggle.\n",
    "- **Convergence to Local Optima:** They often converge to a locally optimal policy, which might be more desirable in some settings.\n",
    "\n",
    "**Cons of Policy-Based Methods:**\n",
    "- **High Variance:** Gradient estimates often have high variance, making learning unstable without careful techniques.\n",
    "- **Sample Inefficiency:** They may require many samples to learn effectively.\n",
    "- **Local Optima:** Direct optimization might get stuck in suboptimal policies.\n",
    "\n",
    "**Policy Representation**\n",
    "\n",
    "A policy $\\pi_{\\theta}(a|s)$ is often represented using a neural network (for instance, a multi-layer perceptron) parameterized by $\\theta$. For discrete actions, the network may output a probability distribution over actions (e.g., via a softmax), while for continuous actions it might output the parameters (like mean and variance) of a distribution (e.g., a Gaussian)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Policy Gradient Derivation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Setting Up the Problem**\n",
    "\n",
    "#### **What Are We Trying to Do?**\n",
    "\n",
    "In reinforcement learning, our goal is to adjust the parameters $ \\theta $ of our policy (which is a function that tells the agent which action to take in a given state) so that the agent gets as much reward as possible. We denote our policy as:\n",
    "$$\n",
    "\\pi_\\theta(a|s)\n",
    "$$\n",
    "which means \"the probability of taking action $a$ when in state $s$\" given parameters $ \\theta $.\n",
    "\n",
    "#### **The Objective Function**\n",
    "\n",
    "The performance of our policy is measured by the **expected total reward** (also called the return) that we get by following the policy. We can write this as:\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]\n",
    "$$\n",
    "Here:\n",
    "- $ \\tau $ represents a **trajectory**, which is a sequence of states and actions: $ (s_0, a_0, s_1, a_1, \\dots) $.\n",
    "- $ R(\\tau) $ is the total reward accumulated along the trajectory $ \\tau $.\n",
    "- The notation $ \\tau \\sim \\pi_\\theta $ means that the trajectory is generated by following the policy $ \\pi_\\theta $.\n",
    "\n",
    "The goal is to find the parameters $ \\theta $ that maximize $ J(\\theta) $.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Taking the Gradient**\n",
    "\n",
    "Since we want to maximize $ J(\\theta) $, we can use **gradient ascent**. This means we want to compute the gradient (or derivative) of $ J(\\theta) $ with respect to $ \\theta $, denoted as $ \\nabla_\\theta J(\\theta) $, and then adjust $ \\theta $ in the direction of this gradient.\n",
    "\n",
    "#### **Step-by-Step Differentiation**\n",
    "\n",
    "1. **Express the Objective as an Integral:**\n",
    "\n",
    "   The expectation $ \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)] $ can be written as an integral over all possible trajectories:\n",
    "   $$\n",
    "   J(\\theta) = \\int \\pi_\\theta(\\tau) R(\\tau) \\, d\\tau\n",
    "   $$\n",
    "   Here, $ \\pi_\\theta(\\tau) $ is the probability of the trajectory $ \\tau $ under the policy $ \\pi_\\theta $.\n",
    "\n",
    "2. **Differentiate Under the Integral:**\n",
    "\n",
    "   To find the gradient, differentiate $ J(\\theta) $ with respect to $ \\theta $:\n",
    "   $$\n",
    "   \\nabla_\\theta J(\\theta) = \\nabla_\\theta \\int \\pi_\\theta(\\tau) R(\\tau) \\, d\\tau\n",
    "   $$\n",
    "   Under suitable conditions (which hold in our context), we can interchange the order of integration and differentiation:\n",
    "   $$\n",
    "   \\nabla_\\theta J(\\theta) = \\int \\nabla_\\theta \\pi_\\theta(\\tau) R(\\tau) \\, d\\tau\n",
    "   $$\n",
    "\n",
    "3. **Introduce the Log-Derivative Trick:**\n",
    "\n",
    "   Now, differentiating $ \\pi_\\theta(\\tau) $ directly can be hard. But we can use a very useful trick from calculus:\n",
    "   $$\n",
    "   \\nabla_\\theta \\pi_\\theta(\\tau) = \\pi_\\theta(\\tau) \\nabla_\\theta \\log \\pi_\\theta(\\tau)\n",
    "   $$\n",
    "   This is true because of the chain rule (think of it as the derivative of $\\log$ and then multiplying back by the original function). Plug this back into our expression:\n",
    "   $$\n",
    "   \\nabla_\\theta J(\\theta) = \\int \\pi_\\theta(\\tau) \\nabla_\\theta \\log \\pi_\\theta(\\tau) R(\\tau) \\, d\\tau\n",
    "   $$\n",
    "\n",
    "4. **Express as an Expectation:**\n",
    "\n",
    "   Notice that the integral\n",
    "   $$\n",
    "   \\int \\pi_\\theta(\\tau) \\, [\\cdot] \\, d\\tau\n",
    "   $$\n",
    "   is just the expectation with respect to trajectories $ \\tau $ sampled from $ \\pi_\\theta $. So, we can write:\n",
    "   $$\n",
    "   \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(\\tau) \\, R(\\tau) \\right]\n",
    "   $$\n",
    "   This is the famous **policy gradient theorem**.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Breaking Down $ \\log \\pi_\\theta(\\tau) $**\n",
    "\n",
    "#### **Understanding Trajectories**\n",
    "\n",
    "A trajectory $ \\tau $ is a sequence of states and actions:\n",
    "$$\n",
    "\\tau = (s_0, a_0, s_1, a_1, \\ldots, s_T, a_T)\n",
    "$$\n",
    "The probability of this trajectory under the policy can be written as:\n",
    "$$\n",
    "\\pi_\\theta(\\tau) = p(s_0) \\prod_{t=0}^{T} \\pi_\\theta(a_t|s_t) \\, p(s_{t+1}|s_t, a_t)\n",
    "$$\n",
    "- $ p(s_0) $ is the probability of the starting state.\n",
    "- $ \\pi_\\theta(a_t|s_t) $ is our policy.\n",
    "- $ p(s_{t+1}|s_t, a_t) $ represents the dynamics of the environment (which we assume does not depend on $ \\theta $).\n",
    "\n",
    "#### **Taking the Logarithm**\n",
    "\n",
    "We take the logarithm of $ \\pi_\\theta(\\tau) $:\n",
    "$$\n",
    "\\log \\pi_\\theta(\\tau) = \\log p(s_0) + \\sum_{t=0}^{T} \\left[ \\log \\pi_\\theta(a_t|s_t) + \\log p(s_{t+1}|s_t, a_t) \\right]\n",
    "$$\n",
    "Since the environment dynamics $ p(s_{t+1}|s_t, a_t) $ and the initial state distribution $ p(s_0) $ do not depend on $ \\theta $, their gradients will be zero. Hence, when we compute:\n",
    "$$\n",
    "\\nabla_\\theta \\log \\pi_\\theta(\\tau)\n",
    "$$\n",
    "only the terms involving $ \\pi_\\theta(a_t|s_t) $ contribute:\n",
    "$$\n",
    "\\nabla_\\theta \\log \\pi_\\theta(\\tau) = \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Putting It All Together**\n",
    "\n",
    "Substitute the above result back into our expression for the gradient:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\left( \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right) R(\\tau) \\right]\n",
    "$$\n",
    "\n",
    "#### **Intuitive Explanation**\n",
    "\n",
    "- **Intuition Behind the Log-Derivative:**  \n",
    "  Instead of directly differentiating the probability of the entire trajectory, we differentiate its logarithm, which conveniently breaks into a sum over time steps. This makes the mathematics much more manageable.\n",
    "\n",
    "- **Why Multiply by $ R(\\tau) $?**  \n",
    "  If a trajectory $ \\tau $ results in a high reward $ R(\\tau) $, then we want to **increase** the likelihood of the actions taken in that trajectory. Multiplying by $ R(\\tau) $ ensures that the update reinforces actions that lead to high rewards.\n",
    "\n",
    "- **Summing Over Time Steps:**  \n",
    "  The sum $ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) $ means that each decision in the trajectory gets credit (or blame) proportional to the final outcome $ R(\\tau) $. If an action at time $ t $ helped lead to a good result, its log probability is increased; if not, it is decreased.\n",
    "\n",
    "#### **A Simple Code Analogy**\n",
    "\n",
    "Imagine you have a recipe (the policy) and you try it out (generating a trajectory). If the dish (the outcome) is delicious (high reward), you note down what you did at each step (the actions) and try to remember to do those things again (increase their probability). If the dish is bad, you change the recipe in the opposite direction.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Summary of the Derivation**\n",
    "\n",
    "1. **Objective:**  \n",
    "   Maximize the expected return:\n",
    "   $$\n",
    "   J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]\n",
    "   $$\n",
    "\n",
    "2. **Gradient of the Objective:**  \n",
    "   Differentiate $ J(\\theta) $ to get:\n",
    "   $$\n",
    "   \\nabla_\\theta J(\\theta) = \\int \\nabla_\\theta \\pi_\\theta(\\tau) R(\\tau) \\, d\\tau\n",
    "   $$\n",
    "\n",
    "3. **Log-Derivative Trick:**  \n",
    "   Replace $ \\nabla_\\theta \\pi_\\theta(\\tau) $ with:\n",
    "   $$\n",
    "   \\pi_\\theta(\\tau) \\nabla_\\theta \\log \\pi_\\theta(\\tau)\n",
    "   $$\n",
    "   leading to:\n",
    "   $$\n",
    "   \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(\\tau) \\, R(\\tau) \\right]\n",
    "   $$\n",
    "\n",
    "4. **Breaking Down the Trajectory:**  \n",
    "   Recognize that only the action probabilities depend on $ \\theta $, so:\n",
    "   $$\n",
    "   \\nabla_\\theta \\log \\pi_\\theta(\\tau) = \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\n",
    "   $$\n",
    "\n",
    "5. **Final Expression:**  \n",
    "   Thus, the gradient becomes:\n",
    "   $$\n",
    "   \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\, R(\\tau) \\right]\n",
    "   $$\n",
    "\n",
    "This final result tells us how to adjust our policy parameters $ \\theta $ by taking into account the contribution of each action (through its log probability) weighted by how good the overall outcome was.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Intuitive Recap**\n",
    "\n",
    "- **Imagine you're learning to play a game:**  \n",
    "  Each time you play, you try out a series of moves (a trajectory). If you win (high reward), you want to remember which moves were good. The formula tells you to “nudge” your strategy to make those moves more likely in the future.\n",
    "\n",
    "- **Logarithm Simplifies Things:**  \n",
    "  Taking the log of the probability lets you add up contributions from each move rather than multiply small probabilities together, which is much easier to work with mathematically.\n",
    "\n",
    "- **Weighting by Reward:**  \n",
    "  The moves are adjusted by the overall success, so good outcomes lead to larger adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
