{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Basic DQN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **This notebook is based on [Nimish Sanghi's book on Deep RL](https://link.springer.com/book/10.1007/979-8-8688-0273-7).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to check out this notebook on Kaggle: https://www.kaggle.com/code/aryamanbansal/basic-dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Motivation**\n",
    "\n",
    "In reinforcement learning (RL), the goal is to learn a policy that maximizes the expected cumulative reward in an environment. The Q-learning algorithm is a popular off-policy method that learns an action-value function (Q-function), which estimates the expected return for taking a particular action in a given state. Deep Q-Networks (DQN) combine Q-Learning with deep neural networks to solve complex RL tasks. Traditional Q-Learning struggles with high-dimensional state spaces (like Atari games). DQN uses neural networks to approximate Q-value functions, enabling handling of complex observations (when the state space is large or continuous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:06:53.772071Z",
     "iopub.status.busy": "2025-02-02T08:06:53.771621Z",
     "iopub.status.idle": "2025-02-02T08:06:53.780178Z",
     "shell.execute_reply": "2025-02-02T08:06:53.778728Z",
     "shell.execute_reply.started": "2025-02-02T08:06:53.772038Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports done!\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.signal import convolve\n",
    "from scipy.signal.windows import gaussian\n",
    "\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML, clear_output\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "print(\"imports done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:25.136352Z",
     "iopub.status.busy": "2025-02-02T08:05:25.135639Z",
     "iopub.status.idle": "2025-02-02T08:05:25.152124Z",
     "shell.execute_reply": "2025-02-02T08:05:25.150741Z",
     "shell.execute_reply.started": "2025-02-02T08:05:25.136299Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x786050e12d90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set a seed\n",
    "seed = 132\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:25.154957Z",
     "iopub.status.busy": "2025-02-02T08:05:25.154645Z",
     "iopub.status.idle": "2025-02-02T08:05:25.187070Z",
     "shell.execute_reply": "2025-02-02T08:05:25.186022Z",
     "shell.execute_reply.started": "2025-02-02T08:05:25.154930Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming a global device setting (CPU or CUDA)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Neural Network for DQN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:25.188910Z",
     "iopub.status.busy": "2025-02-02T08:05:25.188598Z",
     "iopub.status.idle": "2025-02-02T08:05:25.207966Z",
     "shell.execute_reply": "2025-02-02T08:05:25.206801Z",
     "shell.execute_reply.started": "2025-02-02T08:05:25.188884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the DQNAgent.\n",
    "        \n",
    "        Args:\n",
    "            state_shape (tuple): Shape of the input state.\n",
    "            n_actions (int): Number of possible actions.\n",
    "            epsilon (float): Exploration rate for epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "        \n",
    "        # Define a simple feedforward neural network\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_shape[0], 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        Pass the state at time t through the network to get the Q-value Q(s,a).\n",
    "        \n",
    "        Args:\n",
    "            state_t (torch.Tensor): The state at time t.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Q-values for each action.\n",
    "        \"\"\"\n",
    "        qvalues = self.network(state_t)\n",
    "        return qvalues\n",
    "\n",
    "    \n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        Compute Q-values for a batch of states provided as arrays.\n",
    "        \n",
    "        Args:\n",
    "            states (array-like): Batch of states.\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Q-values for each state.\n",
    "        \"\"\"\n",
    "        states = torch.tensor(np.array(states), device=device, dtype=torch.float32)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    \n",
    "    def get_action(self, states):\n",
    "        \"\"\"\n",
    "        Returns the best (greedy) actions for a batch of states.\n",
    "        \n",
    "        Args:\n",
    "            states (array-like): Batch of states.\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Best actions.\n",
    "        \"\"\"\n",
    "        qvalues = self.get_qvalues(states)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "        return best_actions.cpu().numpy()\n",
    "\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"\n",
    "        Implements the epsilon-greedy policy on a batch of Q-values\n",
    "        \n",
    "        Args:\n",
    "            qvalues (np.array): Q-values for a batch of states.\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Actions selected (random with probability epsilon, otherwise best action).\n",
    "        \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "        # Randomly choose actions for exploration\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        # Greedy actions based on Q-values\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "        # Create an array of booleans indicating whether to explore (0) or exploit (1)\n",
    "        should_explore = np.random.choice([0, 1], batch_size, p=[1 - epsilon, epsilon])\n",
    "        # Choose random actions where element is 0, otherwise choose best actions\n",
    "        return np.where(should_explore, random_actions, best_actions)\n",
    "\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Saves the model parameters to a file.\n",
    "        \n",
    "        Args:\n",
    "            path (str): Path to save the model.\n",
    "        \"\"\"\n",
    "        print(\"Saving model to:\", path)\n",
    "        torch.save(self.network.state_dict(), f\"{path}.zip\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Replay Buffer**\n",
    "\n",
    "A replay buffer (or experience replay) is used to store the agent's experiences during training. These experiences are typically tuples of the form (state, action, reward, next_state, done). The replay buffer has several important benefits:\n",
    "- Breaking Correlations:\n",
    "    - Training samples are usually highly correlated if taken sequentially from the environment. The replay buffer helps break these correlations by randomizing the experiences.\n",
    "- Efficient Use of Data:\n",
    "    - Each experience can be reused multiple times for training, which improves sample efficiency.\n",
    "- Stabilizing Training:\n",
    "    - It allows the agent to learn from a diverse set of past experiences rather than only recent ones, contributing to more stable learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:25.209252Z",
     "iopub.status.busy": "2025-02-02T08:05:25.208972Z",
     "iopub.status.idle": "2025-02-02T08:05:25.230812Z",
     "shell.execute_reply": "2025-02-02T08:05:25.229527Z",
     "shell.execute_reply.started": "2025-02-02T08:05:25.209229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Initialize the ReplayBuffer.\n",
    "\n",
    "        Args:\n",
    "            size (int): Maximum number of experiences to store.\n",
    "        \"\"\"\n",
    "        self.size = size  # Maximum buffer size\n",
    "        self.buffer = []  # List to store experiences\n",
    "        self.next_id = 0  # Index pointer for cyclic buffer replacement\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current number of experiences in the buffer.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add a new experience to the buffer.\n",
    "\n",
    "        Args:\n",
    "            state (object): The current state.\n",
    "            action (int): The action taken.\n",
    "            reward (float): The reward received.\n",
    "            next_state (object): The next state after taking the action.\n",
    "            done (bool): Flag indicating whether the episode has terminated.\n",
    "        \"\"\"\n",
    "        # Pack the experience into a tuple\n",
    "        item = (state, action, reward, next_state, done)\n",
    "        \n",
    "        # If the buffer isn't full, append the experience\n",
    "        if len(self.buffer) < self.size:\n",
    "            self.buffer.append(item)\n",
    "        else:\n",
    "            # If the buffer is full, overwrite the oldest experience\n",
    "            self.buffer[self.next_id] = item\n",
    "        \n",
    "        # Update the index in a cyclic manner\n",
    "        self.next_id = (self.next_id + 1) % self.size\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample a batch of experiences from the buffer.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Number of experiences to sample.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of numpy arrays: (states, actions, rewards, next_states, done_flags)\n",
    "        \"\"\"\n",
    "        # Randomly select indices for the batch\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        # Retrieve the experiences at the selected indices\n",
    "        samples = [self.buffer[i] for i in idxs]\n",
    "        # Unzip the list of tuples into separate components\n",
    "        states, actions, rewards, next_states, done_flags = list(zip(*samples))\n",
    "        # Convert each component into a numpy array and return\n",
    "        return (np.array(states),\n",
    "                np.array(actions),\n",
    "                np.array(rewards),\n",
    "                np.array(next_states),\n",
    "                np.array(done_flags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TD Loss**\n",
    "\n",
    "The Temporal Difference (TD) loss is a combination of the TD Error ($\\delta$) and the MSE loss (instead of MSE, you can choose other variants like MAE or Huber loss).\n",
    "\n",
    "$$\\boxed{\\text{TD Error} \\;\\; = \\;\\; r + \\gamma \\max_{a'}Q(s', a') - Q(s, a)} \\qquad \\rightarrow \\;\\; \\text{tabular Q-learning version}$$\n",
    "\n",
    ", where:\n",
    "- $s$ is the current state,\n",
    "- $a$ is the action taken in the current state,\n",
    "- $r$ is the immediate reward (the reward that you get after taking action $a$ in state $s \\; \\rightarrow \\;$ it's not cumulative, ie, it doesn't include the past or future rewards),\n",
    "- $\\gamma$ is the discount factor,\n",
    "- $s'$ is the next state,\n",
    "- $a'$ is the next (greedy) action (maximizing the Q-value in the next state),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll use the MSE (Mean Squared Error) loss function as the basis of the TD loss function. Keep in mind that you can use other loss functions (like MAE or Huber loss) to form the basis of the TD loss. \n",
    "\n",
    "Recall the MSE loss function: $$\\boxed{\\text{MSE} \\;\\; = \\;\\; \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\;\\; = \\;\\; \\frac{1}{n}\\sum_{i=1}^{n} (\\text{error})^2}$$\n",
    "\n",
    ",where:\n",
    "- $n$ is the number of samples\n",
    "- $y_i$ is the actual target\n",
    "- $\\hat{y}_i$ is the predicted target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\text{TD Loss} & \\;\\; = \\;\\; \\text{Mean Squared TD Error} \\\\ \\\\\n",
    "& \\;\\; = \\;\\; \\frac{1}{n}\\sum_{i=1}^{n} \\left( \\text{TD Error} \\right)^2 \\\\ \\\\\n",
    "\\text{TD Loss} & \\;\\; = \\;\\; \\frac{1}{n}\\sum_{i=1}^{n} \\left( r_i + \\gamma \\max_{a'}Q_{\\text{target}}(s'_i, a') - Q_{\\text{current}}(s_i, a_i) \\right)^2\n",
    "\\end{align*}\n",
    "\n",
    ", where:\n",
    "- $Q_{\\text{target}}$ is the target Q-network (less frequently updated),\n",
    "- $Q_{\\text{current}}$ is the current Q-network (updated at each step),\n",
    "- $r_i$ is the immediate reward in the $i^{th}$ sample,\n",
    "- $s_i$ is the current state in the $i^{th}$ sample,\n",
    "- $a_i$ is the action taken in the current state in the $i^{th}$ sample,\n",
    "- $s'_i$ is the next state in the $i^{th}$ sample,\n",
    "- $a'_i$ is the next (greedy) action in the $i^{th}$ sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target Q-network vs Current (or Agent) Q-network**\n",
    "\n",
    "We use 2 networks - the target network and the current network. Initially, both of these are copies of the same DQN architecture. The target network is used to estimate the target Q-values, while the current network is used to estimate the predicted Q-values. The target network is updated less frequently than the current network. This helps stabilize training by reducing the chances of the network chasing its own tail. The target network is updated with the weights of the current network at regular intervals. We decide how often to update the target network - so this a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:25.232540Z",
     "iopub.status.busy": "2025-02-02T08:05:25.232117Z",
     "iopub.status.idle": "2025-02-02T08:05:25.254433Z",
     "shell.execute_reply": "2025-02-02T08:05:25.253427Z",
     "shell.execute_reply.started": "2025-02-02T08:05:25.232475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_td_loss(agent, target_network, states, actions, rewards, \n",
    "                    next_states, done_flags, gamma=0.99, device=device):\n",
    "    \"\"\"\n",
    "    Compute the TD loss for a batch of experiences.\n",
    "\n",
    "    Args:\n",
    "        agent (nn.Module): The current Q-network.\n",
    "        target_network (nn.Module): The target Q-network.\n",
    "        states (np.array): Batch of current states.\n",
    "        actions (np.array): Batch of actions taken.\n",
    "        rewards (np.array): Batch of rewards received.\n",
    "        next_states (np.array): Batch of next states.\n",
    "        done_flags (np.array): Batch of done flags (True/False).\n",
    "        gamma (float): Discount factor.\n",
    "        device: Device to run the computations on (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The computed TD loss.\n",
    "    \"\"\"\n",
    "    # Convert numpy arrays to torch tensors and move them to the appropriate device\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float)\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.long)\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    done_flags = torch.tensor(done_flags.astype('float32'), device=device, dtype=torch.float)\n",
    "\n",
    "    # Compute Q-values for all actions for the current states using the agent network.\n",
    "    predicted_qvalues = agent(states)  # shape: [batch_size, n_actions]\n",
    "\n",
    "    # Compute Q-values for all actions for the next states using the target network.\n",
    "    predicted_next_qvalues = target_network(next_states)  # shape: [batch_size, n_actions]\n",
    "\n",
    "    # For each experience in the batch, select the Q-value for the taken action.\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions]\n",
    "\n",
    "    # Compute the maximum Q-value for the next states (greedy action selection)\n",
    "    next_state_values, _ = torch.max(predicted_next_qvalues, dim=1)\n",
    "\n",
    "    # Compute the target Q-values using the TD target equation:\n",
    "    # target = reward + gamma * max_a' Q(s', a') * (1 - done)\n",
    "    # (1 - done) ensures that if the next state is terminal, \n",
    "    # we don't add the discounted future reward.\n",
    "    target_qvalues_for_actions = rewards + gamma * next_state_values * (1 - done_flags)\n",
    "\n",
    "    # Compute the Mean Squared Error (MSE) loss between the predicted Q-values and the target Q-values.\n",
    "    loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Recording video of trained agents**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:25.256122Z",
     "iopub.status.busy": "2025-02-02T08:05:25.255737Z",
     "iopub.status.idle": "2025-02-02T08:05:25.278016Z",
     "shell.execute_reply": "2025-02-02T08:05:25.276865Z",
     "shell.execute_reply.started": "2025-02-02T08:05:25.256093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def record_video(env_id, video_folder, video_length, agent):\n",
    "    \"\"\"\n",
    "    Record a video of the agent interacting with the environment.\n",
    "\n",
    "    Args:\n",
    "        env_id (str): Environment ID (e.g., 'CartPole-v1').\n",
    "        video_folder (str): Folder where the video will be saved.\n",
    "        video_length (int): Number of timesteps to record.\n",
    "        agent: Trained agent with a get_action() method.\n",
    "    \n",
    "    Returns:\n",
    "        str: The file path to the saved video.\n",
    "    \"\"\"\n",
    "    # Create a dummy vectorized environment with rendering enabled.\n",
    "    vec_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "    \n",
    "    # Wrap the environment with VecVideoRecorder.\n",
    "    vec_env = VecVideoRecorder(\n",
    "        vec_env, video_folder,\n",
    "        record_video_trigger=lambda x: x == 0,\n",
    "        video_length=video_length,\n",
    "        name_prefix=f\"{type(agent).__name__}-{env_id}\"\n",
    "    )\n",
    "\n",
    "    # Reset environment to start recording.\n",
    "    obs = vec_env.reset()\n",
    "    for _ in range(video_length + 1):\n",
    "        # Get action from the agent and step the environment.\n",
    "        action = agent.get_action(obs).detach().cpu().numpy()\n",
    "        obs, _, _, _ = vec_env.step(action)\n",
    "    \n",
    "    # Construct the file path of the recorded video.\n",
    "    file_path = \"./\" + video_folder + vec_env.video_recorder.path.split(\"/\")[-1]\n",
    "    vec_env.close()\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:25.279445Z",
     "iopub.status.busy": "2025-02-02T08:05:25.278999Z",
     "iopub.status.idle": "2025-02-02T08:05:25.296071Z",
     "shell.execute_reply": "2025-02-02T08:05:25.294595Z",
     "shell.execute_reply.started": "2025-02-02T08:05:25.279367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def play_video(file_path):\n",
    "    \"\"\"\n",
    "    Display a video file in a Jupyter Notebook.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the video file.\n",
    "\n",
    "    Returns:\n",
    "        HTML: HTML object that can display the video.\n",
    "    \"\"\"\n",
    "    # Read video file in binary mode.\n",
    "    mp4 = open(file_path, 'rb').read()\n",
    "    # Encode the video file in base64.\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    # Create HTML snippet with a video player.\n",
    "    return HTML(f\"\"\"\n",
    "        <video width=400 controls>\n",
    "            <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "        </video>\n",
    "        \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setting up the training parameters**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:25.300323Z",
     "iopub.status.busy": "2025-02-02T08:05:25.299984Z",
     "iopub.status.idle": "2025-02-02T08:05:25.318810Z",
     "shell.execute_reply": "2025-02-02T08:05:25.317517Z",
     "shell.execute_reply.started": "2025-02-02T08:05:25.300296Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def play_and_record(start_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Run the agent in the environment for a fixed number of steps and record the transitions.\n",
    "\n",
    "    This function allows the agent to interact with the environment for `n_steps` timesteps,\n",
    "    collects the transitions (state, action, reward, next state, done flag), and stores them\n",
    "    in the experience replay buffer. It also accumulates the total reward obtained during these steps.\n",
    "\n",
    "    Args:\n",
    "        start_state: The initial state from which the agent starts.\n",
    "        agent: The DQN agent that provides action selection via its get_qvalues and sample_actions methods.\n",
    "        env: The environment in which the agent is acting (should follow the Gymnasium API).\n",
    "        exp_replay: The experience replay buffer (an instance of ReplayBuffer) to store transitions.\n",
    "        n_steps (int, optional): The number of steps to run the agent in the environment. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple (sum_rewards, s) where:\n",
    "            - sum_rewards (float): The total reward accumulated over the n_steps.\n",
    "            - s: The state at the end of the n_steps, which can be used as the starting state for subsequent calls.\n",
    "    \"\"\"\n",
    "    s = start_state          # Initialize the current state.\n",
    "    sum_rewards = 0          # Initialize the reward accumulator.\n",
    "\n",
    "    # Run the agent for n_steps steps.\n",
    "    for _ in range(n_steps):\n",
    "        # Obtain Q-values for the current state.\n",
    "        qvalues = agent.get_qvalues([s])\n",
    "        \n",
    "        # Select an action using the agent's epsilon-greedy policy.\n",
    "        a = agent.sample_actions(qvalues)[0]\n",
    "        \n",
    "        # Execute the action in the environment.\n",
    "        next_s, r, terminated, truncated, _ = env.step(a)\n",
    "        \n",
    "        # Accumulate the reward.\n",
    "        sum_rewards += r\n",
    "        \n",
    "        # Check if the episode has ended.\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Record the transition in the replay buffer.\n",
    "        exp_replay.add(s, a, r, next_s, done)\n",
    "        \n",
    "        # Update the current state:\n",
    "        # If the episode ended, reset the environment; otherwise, continue with the next state.\n",
    "        if done:\n",
    "            s, _ = env.reset()\n",
    "        else:\n",
    "            s = next_s\n",
    "\n",
    "    return sum_rewards, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:25.320693Z",
     "iopub.status.busy": "2025-02-02T08:05:25.320316Z",
     "iopub.status.idle": "2025-02-02T08:05:25.370822Z",
     "shell.execute_reply": "2025-02-02T08:05:25.369830Z",
     "shell.execute_reply.started": "2025-02-02T08:05:25.320660Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup the environment and agent networks\n",
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\", max_episode_steps=4000)    # Create the environment\n",
    "state_dim = env.observation_space.shape     # e.g., (4,) for CartPole\n",
    "n_actions = env.action_space.n              # e.g., 2 for CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:25.372154Z",
     "iopub.status.busy": "2025-02-02T08:05:25.371867Z",
     "iopub.status.idle": "2025-02-02T08:05:31.533155Z",
     "shell.execute_reply": "2025-02-02T08:05:31.531996Z",
     "shell.execute_reply.started": "2025-02-02T08:05:25.372128Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Reset environment and set seed for reproducibility\n",
    "state, _ = env.reset(seed=seed)\n",
    "\n",
    "# Initialize DQN agent with initial high exploration (epsilon=1)\n",
    "agent = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())  # Synchronize target network\n",
    "\n",
    "# Populate the experience replay buffer with initial random experiences\n",
    "exp_replay = ReplayBuffer(10**4)  # Replay buffer with capacity 10,000\n",
    "for i in range(100):\n",
    "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)  # Helper function to collect experiences\n",
    "    if len(exp_replay) == 10**4:\n",
    "        break\n",
    "\n",
    "# Set up training hyperparameters\n",
    "timesteps_per_epoch = 1        # Timesteps per epoch (for logging purposes)\n",
    "batch_size = 32                # Mini-batch size for training updates\n",
    "total_steps = 50000            # Total training steps\n",
    "\n",
    "# Initialize the optimizer (Adam) for updating the agent's parameters\n",
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "# Define the exploration schedule (epsilon decay)\n",
    "start_epsilon = 1              # Starting exploration rate\n",
    "end_epsilon = 0.05             # Minimum exploration rate\n",
    "eps_decay_final_step = 2 * 10**4  # Steps over which epsilon decays to end_epsilon\n",
    "\n",
    "# Define frequencies for logging and updating the target network\n",
    "loss_freq = 20                      # Log the loss every 20 steps\n",
    "refresh_target_network_freq = 100   # Update target network every 100 steps\n",
    "eval_freq = 1000                    # Evaluate the agent every 1000 steps\n",
    "\n",
    "# Set gradient clipping threshold to stabilize training\n",
    "max_grad_norm = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:31.535081Z",
     "iopub.status.busy": "2025-02-02T08:05:31.534313Z",
     "iopub.status.idle": "2025-02-02T08:05:31.539741Z",
     "shell.execute_reply": "2025-02-02T08:05:31.538620Z",
     "shell.execute_reply.started": "2025-02-02T08:05:31.535036Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "mean_rw_history = []\n",
    "td_loss_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The main training loop**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:31.541407Z",
     "iopub.status.busy": "2025-02-02T08:05:31.541029Z",
     "iopub.status.idle": "2025-02-02T08:05:31.623271Z",
     "shell.execute_reply": "2025-02-02T08:05:31.622094Z",
     "shell.execute_reply.started": "2025-02-02T08:05:31.541372Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def epsilon_schedule(start_eps, end_eps, step, final_step):\n",
    "    \"\"\"\n",
    "    Compute the exploration epsilon for the current step using a linear decay schedule.\n",
    "\n",
    "    Args:\n",
    "        start_eps (float): The initial epsilon (e.g., 1.0).\n",
    "        end_eps (float): The final epsilon after decay (e.g., 0.05).\n",
    "        step (int): The current training step.\n",
    "        final_step (int): The step at which epsilon decays to end_eps.\n",
    "\n",
    "    Returns:\n",
    "        float: The computed epsilon value for the current step.\n",
    "    \"\"\"\n",
    "    # Ensure the step does not exceed final_step for correct interpolation.\n",
    "    return start_eps + (end_eps - start_eps) * min(step, final_step) / final_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:31.624930Z",
     "iopub.status.busy": "2025-02-02T08:05:31.624612Z",
     "iopub.status.idle": "2025-02-02T08:05:31.645263Z",
     "shell.execute_reply": "2025-02-02T08:05:31.643968Z",
     "shell.execute_reply.started": "2025-02-02T08:05:31.624892Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def smoothen(values):\n",
    "    \"\"\"\n",
    "    Smooths out the given values using a Gaussian filter.\n",
    "\n",
    "    Args:\n",
    "        values (list or np.array): The sequence of values to smooth.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The smoothed values.\n",
    "    \"\"\"\n",
    "    kernel = gaussian(100, std=100)\n",
    "    kernel = kernel / np.sum(kernel)\n",
    "    return convolve(values, kernel, 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:31.646890Z",
     "iopub.status.busy": "2025-02-02T08:05:31.646533Z",
     "iopub.status.idle": "2025-02-02T08:05:31.665291Z",
     "shell.execute_reply": "2025-02-02T08:05:31.663946Z",
     "shell.execute_reply.started": "2025-02-02T08:05:31.646856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\"\n",
    "    Evaluate the agent's performance by running it for a specified number of games.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment to evaluate in.\n",
    "        agent (DQNAgent): The DQN agent.\n",
    "        n_games (int): Number of games (episodes) to run.\n",
    "        greedy (bool): If True, use the greedy policy (argmax); otherwise use epsilon-greedy.\n",
    "        t_max (int): Maximum timesteps per episode.\n",
    "\n",
    "    Returns:\n",
    "        float: The average total reward over the evaluated games.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        for _ in range(t_max):\n",
    "            # Get Q-values from the agent.\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            # Choose action: greedy (argmax) if specified, otherwise use agent's sampling.\n",
    "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
    "            s, r, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += r\n",
    "            if terminated:\n",
    "                break\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:07:05.120645Z",
     "iopub.status.busy": "2025-02-02T08:07:05.120193Z",
     "iopub.status.idle": "2025-02-02T08:07:05.132947Z",
     "shell.execute_reply": "2025-02-02T08:07:05.131522Z",
     "shell.execute_reply.started": "2025-02-02T08:07:05.120599Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_dqn(total_steps, timesteps_per_epoch, batch_size, \n",
    "              start_epsilon, end_epsilon, eps_decay_final_step,\n",
    "              loss_freq, refresh_target_network_freq, eval_freq,\n",
    "              max_grad_norm, agent, target_network, env, exp_replay,\n",
    "              opt, td_loss_history, mean_rw_history, env_name, device):\n",
    "    \"\"\"\n",
    "    Main training loop for the DQN agent.\n",
    "\n",
    "    The function updates the agent by:\n",
    "      - Decaying the exploration rate.\n",
    "      - Collecting experiences and storing them in the replay buffer.\n",
    "      - Sampling mini-batches from the replay buffer.\n",
    "      - Computing the TD loss and performing backpropagation.\n",
    "      - Periodically updating the target network.\n",
    "      - Evaluating and logging the agent's performance.\n",
    "\n",
    "    Args:\n",
    "        total_steps (int): Total number of training steps.\n",
    "        timesteps_per_epoch (int): Number of environment steps per training epoch.\n",
    "        batch_size (int): Mini-batch size for training.\n",
    "        start_epsilon (float): Initial exploration rate.\n",
    "        end_epsilon (float): Final exploration rate after decay.\n",
    "        eps_decay_final_step (int): The step at which epsilon should reach end_epsilon.\n",
    "        loss_freq (int): Frequency (in steps) to log TD loss.\n",
    "        refresh_target_network_freq (int): Frequency (in steps) to update the target network.\n",
    "        eval_freq (int): Frequency (in steps) to evaluate the agent.\n",
    "        max_grad_norm (float): Maximum gradient norm for clipping.\n",
    "        agent (DQNAgent): The online agent network.\n",
    "        target_network (DQNAgent): The target network.\n",
    "        env (gym.Env): The environment for interaction.\n",
    "        exp_replay (ReplayBuffer): The experience replay buffer.\n",
    "        opt (torch.optim.Optimizer): The optimizer for training.\n",
    "        td_loss_history (list): List to record TD loss history.\n",
    "        mean_rw_history (list): List to record mean reward history.\n",
    "        env_name (str): Environment name (used for creating a new env during evaluation).\n",
    "        device (torch.device): Device to perform computations on.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Reset the environment to get the initial state.\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    # Main training loop.\n",
    "    for step in trange(total_steps + 1):\n",
    "        # 1. Update exploration rate (epsilon) based on schedule.\n",
    "        agent.epsilon = epsilon_schedule(start_epsilon, end_epsilon, step, eps_decay_final_step)\n",
    "\n",
    "        # 2. Interact with the environment and record experiences.\n",
    "        #    play_and_record() should update the replay buffer.\n",
    "        _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "        # 3. Sample a mini-batch from the replay buffer.\n",
    "        states, actions, rewards, next_states, done_flags = exp_replay.sample(batch_size)\n",
    "\n",
    "        # 4. Compute the TD loss using the agent and target networks.\n",
    "        loss = compute_td_loss(agent, target_network,\n",
    "                               states, actions, rewards, next_states, done_flags,\n",
    "                               gamma=0.99, device=device)\n",
    "\n",
    "        # 5. Perform backpropagation and update the network.\n",
    "        loss.backward()\n",
    "        # Clip gradients to stabilize training.\n",
    "        nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # 6. Log the TD loss at specified intervals.\n",
    "        if step % loss_freq == 0:\n",
    "            td_loss_history.append(loss.data.cpu().item())\n",
    "\n",
    "        # 7. Update the target network periodically.\n",
    "        if step % refresh_target_network_freq == 0:\n",
    "            target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "        # 8. Evaluate the agent and update logs/plots.\n",
    "        if step % eval_freq == 0:\n",
    "            # Create a fresh environment for evaluation.\n",
    "            eval_env = gym.make(env_name, render_mode=\"rgb_array\", max_episode_steps=4000)\n",
    "            mean_reward = evaluate(eval_env, agent, n_games=3, greedy=True, t_max=1000)\n",
    "            mean_rw_history.append(mean_reward)\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            print(\"Buffer size = %i, Epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
    "\n",
    "            # Plot the mean return and smoothened TD loss.\n",
    "            plt.figure(figsize=[16, 5])\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.title(\"Mean return per episode\")\n",
    "            plt.plot(mean_rw_history)\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.title(\"TD loss history (smoothened)\")\n",
    "            plt.plot(smoothen(td_loss_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some clarifications:\n",
    "1. Is `total_steps` similar to epochs?\n",
    "    - Not exactly. In reinforcement learning (RL), we typically talk about steps or timesteps rather than epochs. An epoch in supervised learning is a complete pass over the dataset, but in RL the agent continuously interacts with the environment without a fixed dataset. Thus, `total_steps` represents the total number of environment interactions (or training steps) over the entire training process, not discrete passes over a dataset.\n",
    "\n",
    "&nbsp;  \n",
    "\n",
    "2. Difference between `total_steps` and `timesteps_per_epoch`:\n",
    "    - `total_steps`: This is the overall number of steps (or interactions with the environment) during training. It sets the training budget; once you hit this number, training stops.\n",
    "    - `timesteps_per_epoch`: This parameter defines a smaller interval or block of steps used for periodic logging, evaluation, or other grouping purposes within the overall training loop. It doesn't represent an epoch in the supervised learning sense but serves as a way to structure the training process (for example, logging the performance after every certain number of steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Applying DQN on the CartPole**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:07:10.312952Z",
     "iopub.status.busy": "2025-02-02T08:07:10.312535Z",
     "iopub.status.idle": "2025-02-02T08:11:19.587252Z",
     "shell.execute_reply": "2025-02-02T08:11:19.585947Z",
     "shell.execute_reply.started": "2025-02-02T08:07:10.312884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dqn(total_steps, timesteps_per_epoch, batch_size,\n",
    "          start_epsilon, end_epsilon, eps_decay_final_step,\n",
    "          loss_freq, refresh_target_network_freq, eval_freq,\n",
    "          max_grad_norm, agent, target_network, env, exp_replay,\n",
    "          opt, td_loss_history, mean_rw_history, env_name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:11:51.248096Z",
     "iopub.status.busy": "2025-02-02T08:11:51.247746Z",
     "iopub.status.idle": "2025-02-02T08:11:52.995719Z",
     "shell.execute_reply": "2025-02-02T08:11:52.994540Z",
     "shell.execute_reply.started": "2025-02-02T08:11:51.248068Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "final_score = evaluate(\n",
    "  gym.make(env_name, render_mode=\"rgb_array\", max_episode_steps=4000),\n",
    "  agent, n_games=30, greedy=True, t_max=1000\n",
    ")\n",
    "print('final score:', final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-02T08:05:31.899453Z",
     "iopub.status.idle": "2025-02-02T08:05:31.899780Z",
     "shell.execute_reply": "2025-02-02T08:05:31.899641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# video_folder = \"\"  # enter folder location\n",
    "# video_length = 500\n",
    "\n",
    "# video_file = record_video(env_name, video_folder, video_length, agent)\n",
    "\n",
    "# play_video(video_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
