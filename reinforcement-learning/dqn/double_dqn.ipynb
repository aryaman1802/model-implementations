{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **This notebook is based on [Nimish Sanghi's book on Deep RL](https://link.springer.com/book/10.1007/979-8-8688-0273-7).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds up on the basic Deep Q-Networks (DQNs) notebook: https://www.kaggle.com/code/aryamanbansal/basic-dqn\n",
    "\n",
    "Feel free to check out this notebook on Kaggle: https://www.kaggle.com/code/aryamanbansal/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Motivation**\n",
    "\n",
    "In the original Deep Q-Network (DQN) algorithm, the action-value function is approximated using a neural network. However, a well-known issue with this approach is **overestimation bias**. \n",
    "\n",
    "When the max operator is used to select the best action during the target value computation, it tends to overestimate the Q-values, leading to suboptimal policies. **Double DQN (DDQN)** was proposed to mitigate this problem by decoupling the action selection and evaluation, thereby reducing the overoptimistic estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparative Study: DDQN vs. Basic DQN**\n",
    "\n",
    "- **Basic DQN:**\n",
    "    - Pros: Simple to implement and effective in many scenarios.\n",
    "    - Cons: Tends to overestimate action values due to the max operator.\n",
    "- **Double DQN:**\n",
    "    - Pros: Reduces overestimation bias by decoupling the action selection from evaluation.\n",
    "    - Cons: Slightly more complex to implement since it requires two networks (or at least two forward passes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports done!\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.signal import convolve\n",
    "from scipy.signal.windows import gaussian\n",
    "\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML, clear_output\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "print(\"imports done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x786050e12d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set a seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming a global device setting (CPU or CUDA)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Neural Network for DQN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the DQNAgent.\n",
    "        \n",
    "        Args:\n",
    "            state_shape (tuple): Shape of the input state.\n",
    "            n_actions (int): Number of possible actions.\n",
    "            epsilon (float): Exploration rate for epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "        \n",
    "        # Define a simple feedforward neural network\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_shape[0], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, n_actions)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        Pass the state at time t through the network to get the Q-value Q(s,a).\n",
    "        \n",
    "        Args:\n",
    "            state_t (torch.Tensor): The state at time t.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Q-values for each action.\n",
    "        \"\"\"\n",
    "        qvalues = self.network(state_t)\n",
    "        return qvalues\n",
    "\n",
    "    \n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        Compute Q-values for a batch of states provided as arrays.\n",
    "        \n",
    "        Args:\n",
    "            states (array-like): Batch of states.\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Q-values for each state.\n",
    "        \"\"\"\n",
    "        states = torch.tensor(np.array(states), device=device, dtype=torch.float32)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    \n",
    "    def get_action(self, states):\n",
    "        \"\"\"\n",
    "        Returns the best (greedy) actions for a batch of states.\n",
    "        \n",
    "        Args:\n",
    "            states (array-like): Batch of states.\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Best actions.\n",
    "        \"\"\"\n",
    "        states = torch.tensor(np.array(states), device=device, dtype=torch.float32)\n",
    "        qvalues = self.forward(states)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "        return best_actions\n",
    "\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"\n",
    "        Implements the epsilon-greedy policy on a batch of Q-values\n",
    "        \n",
    "        Args:\n",
    "            qvalues (np.array): Q-values for a batch of states.\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Actions selected (random with probability epsilon, otherwise best action).\n",
    "        \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "        # Randomly choose actions for exploration\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        # Greedy actions based on Q-values\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "        # Create an array of booleans indicating whether to explore (0) or exploit (1)\n",
    "        should_explore = np.random.choice([0, 1], batch_size, p=[1 - epsilon, epsilon])\n",
    "        # Choose random actions where element is 0, otherwise choose best actions\n",
    "        return np.where(should_explore, random_actions, best_actions)\n",
    "\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Saves the model parameters to a file.\n",
    "        \n",
    "        Args:\n",
    "            path (str): Path to save the model.\n",
    "        \"\"\"\n",
    "        print(\"Saving model to:\", path)\n",
    "        torch.save(self.network.state_dict(), f\"{path}.zip\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Replay Buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Initialize the ReplayBuffer.\n",
    "\n",
    "        Args:\n",
    "            size (int): Maximum number of experiences to store.\n",
    "        \"\"\"\n",
    "        self.size = size  # Maximum buffer size\n",
    "        self.buffer = []  # List to store experiences\n",
    "        self.next_id = 0  # Index pointer for cyclic buffer replacement\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current number of experiences in the buffer.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add a new experience to the buffer.\n",
    "\n",
    "        Args:\n",
    "            state (object): The current state.\n",
    "            action (int): The action taken.\n",
    "            reward (float): The reward received.\n",
    "            next_state (object): The next state after taking the action.\n",
    "            done (bool): Flag indicating whether the episode has terminated.\n",
    "        \"\"\"\n",
    "        # Pack the experience into a tuple\n",
    "        item = (state, action, reward, next_state, done)\n",
    "        \n",
    "        # If the buffer isn't full, append the experience\n",
    "        if len(self.buffer) < self.size:\n",
    "            self.buffer.append(item)\n",
    "        else:\n",
    "            # If the buffer is full, overwrite the oldest experience\n",
    "            self.buffer[self.next_id] = item\n",
    "        \n",
    "        # Update the index in a cyclic manner\n",
    "        self.next_id = (self.next_id + 1) % self.size\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample a batch of experiences from the buffer.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Number of experiences to sample.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of numpy arrays: (states, actions, rewards, next_states, done_flags)\n",
    "        \"\"\"\n",
    "        # Randomly select indices for the batch\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        # Retrieve the experiences at the selected indices\n",
    "        samples = [self.buffer[i] for i in idxs]\n",
    "        # Unzip the list of tuples into separate components\n",
    "        states, actions, rewards, next_states, done_flags = list(zip(*samples))\n",
    "        # Convert each component into a numpy array and return\n",
    "        return (np.array(states),\n",
    "                np.array(actions),\n",
    "                np.array(rewards),\n",
    "                np.array(next_states),\n",
    "                np.array(done_flags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TD Loss for Double DQN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Standard DQN Target Calculation**: In basic DQN, the target is computed as:\n",
    "\n",
    "$$Q_{\\text{target}} = r + \\gamma \\max\\limits_{a'} Q_{\\text{target}}(s', a')$$\n",
    "\n",
    "The max operation can lead to an overestimation because it picks the highest value even if it is overestimated.\n",
    "\n",
    "&nbsp;  \n",
    "\n",
    "2. **Double DQN Target Calculation**: To reduce overestimation, DDQN decouples the selection and evaluation steps:\n",
    "\n",
    "- **Action selection**:  $$a^* = \\argmax\\limits_{a'} \\; Q_{\\text{online}}(s', a')$$\n",
    "- **Action evaluation**: $$Q_{\\text{target}}(s,a) = r + \\gamma Q_{\\text{target}}(s', a^*)$$\n",
    "\n",
    "This approach uses the online network to decide which action to take (action selection) and the target network to estimate its Q-value (action evaluation).\n",
    "\n",
    "&nbsp;  \n",
    "\n",
    "3. **Intuitive Analogy**: Imagine you are trying to decide the best restaurant to visit. Instead of asking the same person (who might be biased by recent positive experiences) for both the recommendation and the review, you ask two different people:\n",
    "- One person (the online network) suggests the restaurant.\n",
    "- Another (the target network) provides a more impartial review. This separation helps reduce the chance of both recommendation and review being overly optimistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_loss_ddqn(agent, target_network, states, actions, rewards, next_states, done_flags,\n",
    "                 gamma=0.99, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Computes the Temporal Difference (TD) loss for Double DQN (DDQN) and returns the mean squared error (MSE) loss.\n",
    "\n",
    "    In Double DQN, the online network (agent) is used to select the best next action, while the target network\n",
    "    is used to evaluate the Q-value of that action. This function calculates the loss based on the difference between\n",
    "    the predicted Q-values for the taken actions and the target Q-values computed using the target network.\n",
    "\n",
    "    Args:\n",
    "        agent (torch.nn.Module): The online Q-network used for action selection and current Q-value estimation.\n",
    "        target_network (torch.nn.Module): The target Q-network used for evaluating the Q-value of the selected action.\n",
    "        states (np.array): Batch of current state observations.\n",
    "        actions (np.array): Batch of actions taken corresponding to the states.\n",
    "        rewards (np.array): Batch of rewards received after taking the actions.\n",
    "        next_states (np.array): Batch of next state observations following the actions.\n",
    "        done_flags (np.array): Batch of flags indicating terminal states (1 if episode ended, 0 otherwise).\n",
    "        gamma (float, optional): Discount factor for future rewards. Default is 0.99.\n",
    "        device (torch.device, optional): Device on which to perform the tensor computations (CPU/GPU). Default is CPU.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The computed mean squared error loss.\n",
    "    \"\"\"\n",
    "    # Convert input numpy arrays to torch tensors and move them to the specified device\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float)\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.long)\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    # Ensure done_flags is float and on the correct device\n",
    "    done_flags = torch.tensor(done_flags.astype('float32'), device=device, dtype=torch.float)\n",
    "\n",
    "    # Compute Q-values for current states using the online network\n",
    "    q_s = agent(states)  # Shape: [batch_size, num_actions]\n",
    "\n",
    "    # Select the Q-values for the actions that were actually taken\n",
    "    # For each state in the batch, we select the Q-value corresponding to the chosen action\n",
    "    q_s_a = q_s[range(len(actions)), actions]  # Shape: [batch_size]\n",
    "\n",
    "    # Compute Q-values for next states for action selection\n",
    "    # Use the agent (or online) network to estimate Q-values for all actions in the next state\n",
    "    # Detach from the graph to prevent gradients from flowing through the next state\n",
    "    q_s1 = agent(next_states).detach()  \n",
    "    # Compute the index of the best action for each next state using the online network\n",
    "    _, a1max = torch.max(q_s1, dim=1)  # a1max: Indices of best actions for each next state\n",
    "\n",
    "    # Evaluate the best action using the target network\n",
    "    # Use the target network to compute Q-values for all actions in the next state\n",
    "    q_s1_target = target_network(next_states)\n",
    "    # Gather the Q-value for the best action selected by the online network\n",
    "    q_s1_a1max = q_s1_target[range(len(a1max)), a1max]\n",
    "\n",
    "    # Compute the target Q-values using the Bellman equation\n",
    "    # For non-terminal states (done_flags=0), target = reward + gamma * Q(next_state, best_action)\n",
    "    # For terminal states (done_flags=1), target = reward (since no future reward is considered)\n",
    "    target_q = rewards + gamma * q_s1_a1max * (1 - done_flags)\n",
    "\n",
    "    # Compute the Mean Squared Error loss\n",
    "    # Calculate the difference between the predicted Q-values for the taken actions and the target Q-values\n",
    "    loss = torch.mean((q_s_a - target_q).pow(2))\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Recording video of trained agents**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_video(env_id, video_folder, video_length, agent):\n",
    "    \"\"\"\n",
    "    Record a video of the agent interacting with the environment.\n",
    "\n",
    "    Args:\n",
    "        env_id (str): Environment ID (e.g., 'CartPole-v1').\n",
    "        video_folder (str): Folder where the video will be saved.\n",
    "        video_length (int): Number of timesteps to record.\n",
    "        agent: Trained agent with a get_action() method.\n",
    "    \n",
    "    Returns:\n",
    "        str: The file path to the saved video.\n",
    "    \"\"\"\n",
    "    # Create a dummy vectorized environment with rendering enabled.\n",
    "    vec_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "    \n",
    "    # Wrap the environment with VecVideoRecorder.\n",
    "    vec_env = VecVideoRecorder(\n",
    "        vec_env, video_folder,\n",
    "        record_video_trigger=lambda x: x == 0,\n",
    "        video_length=video_length,\n",
    "        name_prefix=f\"{type(agent).__name__}-{env_id}\"\n",
    "    )\n",
    "\n",
    "    # Reset environment to start recording.\n",
    "    obs = vec_env.reset()\n",
    "    for _ in range(video_length + 1):\n",
    "        # Get action from the agent and step the environment.\n",
    "        action = agent.get_action(obs).detach().cpu().numpy()\n",
    "        obs, _, _, _ = vec_env.step(action)\n",
    "    \n",
    "    # Construct the file path of the recorded video.\n",
    "    file_path = \"./\" + video_folder + vec_env.video_recorder.path.split(\"/\")[-1]\n",
    "    vec_env.close()\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_video(file_path):\n",
    "    \"\"\"\n",
    "    Display a video file in a Jupyter Notebook.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the video file.\n",
    "\n",
    "    Returns:\n",
    "        HTML: HTML object that can display the video.\n",
    "    \"\"\"\n",
    "    # Read video file in binary mode.\n",
    "    mp4 = open(file_path, 'rb').read()\n",
    "    # Encode the video file in base64.\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    # Create HTML snippet with a video player.\n",
    "    return HTML(f\"\"\"\n",
    "        <video width=400 controls>\n",
    "            <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "        </video>\n",
    "        \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setting up the training parameters**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record(start_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Run the agent in the environment for a fixed number of steps and record the transitions.\n",
    "\n",
    "    This function allows the agent to interact with the environment for `n_steps` timesteps,\n",
    "    collects the transitions (state, action, reward, next state, done flag), and stores them\n",
    "    in the experience replay buffer. It also accumulates the total reward obtained during these steps.\n",
    "\n",
    "    Args:\n",
    "        start_state: The initial state from which the agent starts.\n",
    "        agent: The DQN agent that provides action selection via its get_qvalues and sample_actions methods.\n",
    "        env: The environment in which the agent is acting (should follow the Gymnasium API).\n",
    "        exp_replay: The experience replay buffer (an instance of ReplayBuffer) to store transitions.\n",
    "        n_steps (int, optional): The number of steps to run the agent in the environment. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple (sum_rewards, s) where:\n",
    "            - sum_rewards (float): The total reward accumulated over the n_steps.\n",
    "            - s: The state at the end of the n_steps, which can be used as the starting state for subsequent calls.\n",
    "    \"\"\"\n",
    "    s = start_state          # Initialize the current state.\n",
    "    sum_rewards = 0          # Initialize the reward accumulator.\n",
    "\n",
    "    # Run the agent for n_steps steps.\n",
    "    for _ in range(n_steps):\n",
    "        # Obtain Q-values for the current state.\n",
    "        qvalues = agent.get_qvalues([s])\n",
    "        \n",
    "        # Select an action using the agent's epsilon-greedy policy.\n",
    "        a = agent.sample_actions(qvalues)[0]\n",
    "        \n",
    "        # Execute the action in the environment.\n",
    "        next_s, r, terminated, truncated, _ = env.step(a)\n",
    "        \n",
    "        # Accumulate the reward.\n",
    "        sum_rewards += r\n",
    "        \n",
    "        # Check if the episode has ended.\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Record the transition in the replay buffer.\n",
    "        exp_replay.add(s, a, r, next_s, done)\n",
    "        \n",
    "        # Update the current state:\n",
    "        # If the episode ended, reset the environment; otherwise, continue with the next state.\n",
    "        if done:\n",
    "            s, _ = env.reset()\n",
    "        else:\n",
    "            s = next_s\n",
    "\n",
    "    return sum_rewards, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the environment and agent networks\n",
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\", max_episode_steps=4000)    # Create the environment\n",
    "state_dim = env.observation_space.shape     # e.g., (4,) for CartPole\n",
    "n_actions = env.action_space.n              # e.g., 2 for CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment and set seed for reproducibility\n",
    "state, _ = env.reset(seed=seed)\n",
    "\n",
    "# Initialize DQN agent with initial high exploration (epsilon=1)\n",
    "agent = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())  # Synchronize target network\n",
    "\n",
    "# Populate the experience replay buffer with initial random experiences\n",
    "exp_replay = ReplayBuffer(10**4)  # Replay buffer with capacity 10,000\n",
    "for i in range(100):\n",
    "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)  # Helper function to collect experiences\n",
    "    if len(exp_replay) == 10**4:\n",
    "        break\n",
    "\n",
    "# Set up training hyperparameters\n",
    "timesteps_per_epoch = 1        # Timesteps per epoch (for logging purposes)\n",
    "batch_size = 32                # Mini-batch size for training updates\n",
    "total_steps = 50000            # Total training steps\n",
    "\n",
    "# Initialize the optimizer (Adam) for updating the agent's parameters\n",
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "# Define the exploration schedule (epsilon decay)\n",
    "start_epsilon = 1              # Starting exploration rate\n",
    "end_epsilon = 0.05             # Minimum exploration rate\n",
    "eps_decay_final_step = 2 * 10**4  # Steps over which epsilon decays to end_epsilon\n",
    "\n",
    "# Define frequencies for logging and updating the target network\n",
    "loss_freq = 20                      # Log the loss every 20 steps\n",
    "refresh_target_network_freq = 100   # Update target network every 100 steps\n",
    "eval_freq = 1000                    # Evaluate the agent every 1000 steps\n",
    "\n",
    "# Set gradient clipping threshold to stabilize training\n",
    "max_grad_norm = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "mean_rw_history = []\n",
    "td_loss_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The main training loop**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_schedule(start_eps, end_eps, step, final_step):\n",
    "    \"\"\"\n",
    "    Compute the exploration epsilon for the current step using a linear decay schedule.\n",
    "\n",
    "    Args:\n",
    "        start_eps (float): The initial epsilon (e.g., 1.0).\n",
    "        end_eps (float): The final epsilon after decay (e.g., 0.05).\n",
    "        step (int): The current training step.\n",
    "        final_step (int): The step at which epsilon decays to end_eps.\n",
    "\n",
    "    Returns:\n",
    "        float: The computed epsilon value for the current step.\n",
    "    \"\"\"\n",
    "    # Ensure the step does not exceed final_step for correct interpolation.\n",
    "    return start_eps + (end_eps - start_eps) * min(step, final_step) / final_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothen(values):\n",
    "    \"\"\"\n",
    "    Smooths out the given values using a Gaussian filter.\n",
    "\n",
    "    Args:\n",
    "        values (list or np.array): The sequence of values to smooth.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The smoothed values.\n",
    "    \"\"\"\n",
    "    kernel = gaussian(100, std=100)\n",
    "    kernel = kernel / np.sum(kernel)\n",
    "    return convolve(values, kernel, 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\"\n",
    "    Evaluate the agent's performance by running it for a specified number of games.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment to evaluate in.\n",
    "        agent (DQNAgent): The DQN agent.\n",
    "        n_games (int): Number of games (episodes) to run.\n",
    "        greedy (bool): If True, use the greedy policy (argmax); otherwise use epsilon-greedy.\n",
    "        t_max (int): Maximum timesteps per episode.\n",
    "\n",
    "    Returns:\n",
    "        float: The average total reward over the evaluated games.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        for _ in range(t_max):\n",
    "            # Get Q-values from the agent.\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            # Choose action: greedy (argmax) if specified, otherwise use agent's sampling.\n",
    "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
    "            s, r, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += r\n",
    "            if terminated:\n",
    "                break\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(total_steps, timesteps_per_epoch, batch_size, \n",
    "              start_epsilon, end_epsilon, eps_decay_final_step,\n",
    "              loss_freq, refresh_target_network_freq, eval_freq,\n",
    "              max_grad_norm, agent, target_network, env, exp_replay,\n",
    "              opt, td_loss_history, mean_rw_history, env_name, device):\n",
    "    \"\"\"\n",
    "    Main training loop for the DQN agent.\n",
    "\n",
    "    The function updates the agent by:\n",
    "      - Decaying the exploration rate.\n",
    "      - Collecting experiences and storing them in the replay buffer.\n",
    "      - Sampling mini-batches from the replay buffer.\n",
    "      - Computing the TD loss and performing backpropagation.\n",
    "      - Periodically updating the target network.\n",
    "      - Evaluating and logging the agent's performance.\n",
    "\n",
    "    Args:\n",
    "        total_steps (int): Total number of training steps.\n",
    "        timesteps_per_epoch (int): Number of environment steps per training epoch.\n",
    "        batch_size (int): Mini-batch size for training.\n",
    "        start_epsilon (float): Initial exploration rate.\n",
    "        end_epsilon (float): Final exploration rate after decay.\n",
    "        eps_decay_final_step (int): The step at which epsilon should reach end_epsilon.\n",
    "        loss_freq (int): Frequency (in steps) to log TD loss.\n",
    "        refresh_target_network_freq (int): Frequency (in steps) to update the target network.\n",
    "        eval_freq (int): Frequency (in steps) to evaluate the agent.\n",
    "        max_grad_norm (float): Maximum gradient norm for clipping.\n",
    "        agent (DQNAgent): The online agent network.\n",
    "        target_network (DQNAgent): The target network.\n",
    "        env (gym.Env): The environment for interaction.\n",
    "        exp_replay (ReplayBuffer): The experience replay buffer.\n",
    "        opt (torch.optim.Optimizer): The optimizer for training.\n",
    "        td_loss_history (list): List to record TD loss history.\n",
    "        mean_rw_history (list): List to record mean reward history.\n",
    "        env_name (str): Environment name (used for creating a new env during evaluation).\n",
    "        device (torch.device): Device to perform computations on.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Reset the environment to get the initial state.\n",
    "    state, _ = env.reset(seed=seed)\n",
    "\n",
    "    # Main training loop.\n",
    "    for step in trange(total_steps + 1):\n",
    "        # 1. Update exploration rate (epsilon) based on schedule.\n",
    "        agent.epsilon = epsilon_schedule(start_epsilon, end_epsilon, step, eps_decay_final_step)\n",
    "\n",
    "        # 2. Interact with the environment and record experiences.\n",
    "        #    play_and_record() should update the replay buffer.\n",
    "        _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "        # 3. Sample a mini-batch from the replay buffer.\n",
    "        states, actions, rewards, next_states, done_flags = exp_replay.sample(batch_size)\n",
    "\n",
    "        # 4. Compute the TD loss using the agent and target networks.\n",
    "        opt.zero_grad()\n",
    "        loss = td_loss_ddqn(agent, target_network,\n",
    "                               states, actions, rewards, next_states, done_flags,\n",
    "                               gamma=0.99, device=device)\n",
    "\n",
    "        # 5. Perform backpropagation and update the network.\n",
    "        loss.backward()\n",
    "        # Clip gradients to stabilize training.\n",
    "        nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "        opt.step()\n",
    "\n",
    "        # 6. Log the TD loss at specified intervals.\n",
    "        if step % loss_freq == 0:\n",
    "            td_loss_history.append(loss.data.cpu().item())\n",
    "\n",
    "        # 7. Update the target network periodically.\n",
    "        if step % refresh_target_network_freq == 0:\n",
    "            target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "        # 8. Evaluate the agent and update logs/plots.\n",
    "        if step % eval_freq == 0:\n",
    "            # Create a fresh environment for evaluation.\n",
    "            eval_env = gym.make(env_name, render_mode=\"rgb_array\", max_episode_steps=4000)\n",
    "            mean_reward = evaluate(eval_env, agent, n_games=3, greedy=True, t_max=1000)\n",
    "            mean_rw_history.append(mean_reward)\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            print(\"Buffer size = %i, Epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
    "\n",
    "            # Plot the mean return and smoothened TD loss.\n",
    "            plt.figure(figsize=[16, 5])\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.title(\"Mean return per episode\")\n",
    "            plt.plot(mean_rw_history)\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.title(\"TD loss history (smoothened)\")\n",
    "            plt.plot(smoothen(td_loss_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Applying DQN on the CartPole**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dqn(total_steps, timesteps_per_epoch, batch_size,\n",
    "          start_epsilon, end_epsilon, eps_decay_final_step,\n",
    "          loss_freq, refresh_target_network_freq, eval_freq,\n",
    "          max_grad_norm, agent, target_network, env, exp_replay,\n",
    "          opt, td_loss_history, mean_rw_history, env_name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = evaluate(\n",
    "  gym.make(env_name, render_mode=\"rgb_array\", max_episode_steps=4000),\n",
    "  agent, n_games=30, greedy=True, t_max=1000\n",
    ")\n",
    "print('final score:', final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_folder = \"\"  # enter folder location\n",
    "# video_length = 500\n",
    "\n",
    "# video_file = record_video(env_name, video_folder, video_length, agent)\n",
    "\n",
    "# play_video(video_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
