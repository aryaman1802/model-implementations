{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dueling DQN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **This notebook is based on [Nimish Sanghi's book on Deep RL](https://link.springer.com/book/10.1007/979-8-8688-0273-7).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds up on the basic Deep Q-Networks (DQNs) notebook: https://www.kaggle.com/code/aryamanbansal/basic-dqn\n",
    "\n",
    "Feel free to check out this notebook on Kaggle: https://www.kaggle.com/code/aryamanbansal/dueling-dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Motivation**\n",
    "\n",
    "In many environments, the value of a state is not solely dependent on the chosen action - some states are inherently good or bad regardless of which action is taken. In standard DQN, the Q-value is computed directly for every (state, action) pair. However, this approach can be inefficient when the value of the state itself is a dominant factor.\n",
    "\n",
    "**Dueling DQN** addresses this by decomposing the Q-value function into two separate estimators:\n",
    "\n",
    "- **State Value Function**, $V(s)$: Represents how good it is to be in a given state regardless of the action taken.\n",
    "- **Advantage Function**, $A(s,a)$: Represents the relative benefit of taking a specific action compared to the other actions in that state.\n",
    "\n",
    "This separation helps the network learn the value of states and the relative advantage of actions more efficiently, especially in states where actions have little effect on the overall outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparative Study: Dueling DQN vs. Basic DQN**\n",
    "\n",
    "- **Basic DQN:**\n",
    "    - Computes a single stream that directly estimates $Q(s,a)$ for each action.\n",
    "    - May struggle to learn an effective state value when the advantage of each action is subtle.\n",
    "- **Dueling DQN:**\n",
    "    - Decomposes $Q(s,a)$ into $V(s)$ and $A(s,a)$, which can help the network focus on learning the inherent value of the state.\n",
    "    - Often achieves improved performance and more stable training, especially in environments where the choice of action has less impact on the overall state value.\n",
    "    - Typically shows enhanced sample efficiency, as the value function learning benefits from more targeted gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports done!\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.signal import convolve\n",
    "from scipy.signal.windows import gaussian\n",
    "\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML, clear_output\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "print(\"imports done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x786050e12d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set a seed\n",
    "seed = 132\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming a global device setting (CPU or CUDA)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Neural Network for Dueling DQN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Decomposition:**\n",
    "\n",
    "- In basic DQN: $$Q(s,a) \\approx \\text{some function } f(s,a)$$\n",
    "\n",
    "- The core idea of Dueling DQN is to express the Q-value function as: $$Q(s,a) = V(s) + \\left( A(s,a) - \\frac{1}{|\\mathcal{A}|} \\sum_{a'} A(s,a') \\right)$$\n",
    "\n",
    ", where:\n",
    "- $V(s)$: is the state-value function, ie, it tells the value of being in state $s$.\n",
    "- $A(s,a)$: is the advantage function, ie, it tells the relative advantage of taking action $a$ in state $s$ compared to other actions.\n",
    "- The term $\\frac{1}{|\\mathcal{A}|} \\sum_{a'} A(s,a')$ is the mean of advantages, ie, it normalizes the advantage, ensuring that the network focuses on learning the true state value.\n",
    "\n",
    "&nbsp;  \n",
    "\n",
    "2. **Why use a Dueling Architecture?**\n",
    "\n",
    "- **Efficient Learning**: In many states, the choice of action might have little impact on the value of that state. By separately estimating $V(s)$ and $A(s,a)$, the network can learn $V(s)$ even when the advantage values are noisy.\n",
    "- **Stability**: This separation can lead to more stable estimates and can help reduce overfitting of Q-values to specific actions.\n",
    "\n",
    "&nbsp;  \n",
    "\n",
    "3. **Implementation Details:**\n",
    "\n",
    "- The network typically has a common convolutional or fully connected layer (shared feature representation).\n",
    "- After the shared layers, the network splits into two streams: one for estimating $V(s)$ (usually ending in a single output) and one for estimating $A(s,a)$ (ending in as many outputs as actions).\n",
    "- The final Q-values are then computed as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQNAgent(nn.Module):\n",
    "    \"\"\"\n",
    "    Dueling Deep Q-Network (Dueling DQN) Agent.\n",
    "\n",
    "    This agent implements the dueling architecture which separates the estimation of\n",
    "    the state value function V(s) and the advantage function A(s,a) to compute Q(s,a).\n",
    "    The final Q-values are obtained by combining these two streams:\n",
    "    \n",
    "        Q(s,a) = V(s) + (A(s,a) - mean(A(s,·)))\n",
    "\n",
    "    Attributes:\n",
    "        epsilon (float): Exploration rate (can be used for epsilon-greedy policies).\n",
    "        n_actions (int): Number of possible actions.\n",
    "        state_shape (tuple): Shape of the input state.\n",
    "        fc1 (nn.Linear): First fully connected layer.\n",
    "        fc2 (nn.Linear): Second fully connected layer.\n",
    "        fc_value (nn.Linear): Fully connected layer for the value stream.\n",
    "        fc_adv (nn.Linear): Fully connected layer for the advantage stream.\n",
    "        value (nn.Linear): Final layer that outputs the state value V(s).\n",
    "        adv (nn.Linear): Final layer that outputs the advantages A(s,a) for each action.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
    "        \"\"\"\n",
    "        Initialize the DuelingDQNAgent.\n",
    "\n",
    "        Args:\n",
    "            state_shape (tuple): Shape of the input state (e.g., (state_dim,)).\n",
    "            n_actions (int): Number of possible actions.\n",
    "            epsilon (float, optional): Exploration rate for epsilon-greedy policies. Default is 0.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "        \n",
    "        # Define the shared fully-connected layers for feature extraction from the input state.\n",
    "        self.fc1 = nn.Linear(state_shape[0], 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        \n",
    "        # Define the separate streams for the value and advantage functions.\n",
    "        # The value stream processes features to estimate V(s).\n",
    "        self.fc_value = nn.Linear(128, 32)\n",
    "        self.value = nn.Linear(32, 1)\n",
    "        \n",
    "        # The advantage stream processes features to estimate A(s,a) for each action.\n",
    "        self.fc_adv = nn.Linear(128, 32)\n",
    "        self.adv = nn.Linear(32, n_actions)\n",
    "        \n",
    "\n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        Forward pass through the dueling network to compute Q-values.\n",
    "\n",
    "        Args:\n",
    "            state_t (torch.Tensor): Input tensor representing the state, \n",
    "                                    of shape [batch_size, state_dim].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The computed Q-values for each action, of shape [batch_size, n_actions].\n",
    "        \"\"\"\n",
    "        # Pass the input state through the first fully-connected layer with ReLU activation.\n",
    "        x = F.relu(self.fc1(state_t))\n",
    "        # Further process the features with a second fully-connected layer.\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # --- Value Stream ---\n",
    "        # Process features to estimate the state value V(s).\n",
    "        v = F.relu(self.fc_value(x))\n",
    "        v = self.value(v)  # Output shape: [batch_size, 1]\n",
    "        \n",
    "        # --- Advantage Stream ---\n",
    "        # Process features to estimate the advantages A(s,a) for each action.\n",
    "        adv = F.relu(self.fc_adv(x))\n",
    "        adv = self.adv(adv)  # Output shape: [batch_size, n_actions]\n",
    "        \n",
    "        # Normalize the advantage values by subtracting the mean advantage for each sample.\n",
    "        adv_avg = torch.mean(adv, dim=1, keepdim=True)\n",
    "        \n",
    "        # Combine the value and normalized advantage streams to obtain Q-values.\n",
    "        # Q(s,a) = V(s) + (A(s,a) - mean(A(s,·)))\n",
    "        qvalues = v + adv - adv_avg\n",
    "        \n",
    "        return qvalues\n",
    "\n",
    "    \n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        Compute Q-values for a batch of states provided as arrays.\n",
    "        \n",
    "        Args:\n",
    "            states (array-like): Batch of states.\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Q-values for each state.\n",
    "        \"\"\"\n",
    "        states = torch.tensor(np.array(states), device=device, dtype=torch.float32)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    \n",
    "    def get_action(self, states):\n",
    "        \"\"\"\n",
    "        Returns the best (greedy) actions for a batch of states.\n",
    "        \n",
    "        Args:\n",
    "            states (array-like): Batch of states.\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Best actions.\n",
    "        \"\"\"\n",
    "        states = torch.tensor(np.array(states), device=device, dtype=torch.float32)\n",
    "        qvalues = self.forward(states)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "        return best_actions\n",
    "\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"\n",
    "        Implements the epsilon-greedy policy on a batch of Q-values\n",
    "        \n",
    "        Args:\n",
    "            qvalues (np.array): Q-values for a batch of states.\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Actions selected (random with probability epsilon, otherwise best action).\n",
    "        \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "        # Randomly choose actions for exploration\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        # Greedy actions based on Q-values\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "        # Create an array of booleans indicating whether to explore (0) or exploit (1)\n",
    "        should_explore = np.random.choice([0, 1], batch_size, p=[1 - epsilon, epsilon])\n",
    "        # Choose random actions where element is 0, otherwise choose best actions\n",
    "        return np.where(should_explore, random_actions, best_actions)\n",
    "\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Saves the model parameters to a file.\n",
    "        \n",
    "        Args:\n",
    "            path (str): Path to save the model.\n",
    "        \"\"\"\n",
    "        print(\"Saving model to:\", path)\n",
    "        torch.save(self.network.state_dict(), f\"{path}.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Replay Buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Initialize the ReplayBuffer.\n",
    "\n",
    "        Args:\n",
    "            size (int): Maximum number of experiences to store.\n",
    "        \"\"\"\n",
    "        self.size = size  # Maximum buffer size\n",
    "        self.buffer = []  # List to store experiences\n",
    "        self.next_id = 0  # Index pointer for cyclic buffer replacement\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current number of experiences in the buffer.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add a new experience to the buffer.\n",
    "\n",
    "        Args:\n",
    "            state (object): The current state.\n",
    "            action (int): The action taken.\n",
    "            reward (float): The reward received.\n",
    "            next_state (object): The next state after taking the action.\n",
    "            done (bool): Flag indicating whether the episode has terminated.\n",
    "        \"\"\"\n",
    "        # Pack the experience into a tuple\n",
    "        item = (state, action, reward, next_state, done)\n",
    "        \n",
    "        # If the buffer isn't full, append the experience\n",
    "        if len(self.buffer) < self.size:\n",
    "            self.buffer.append(item)\n",
    "        else:\n",
    "            # If the buffer is full, overwrite the oldest experience\n",
    "            self.buffer[self.next_id] = item\n",
    "        \n",
    "        # Update the index in a cyclic manner\n",
    "        self.next_id = (self.next_id + 1) % self.size\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample a batch of experiences from the buffer.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Number of experiences to sample.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of numpy arrays: (states, actions, rewards, next_states, done_flags)\n",
    "        \"\"\"\n",
    "        # Randomly select indices for the batch\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        # Retrieve the experiences at the selected indices\n",
    "        samples = [self.buffer[i] for i in idxs]\n",
    "        # Unzip the list of tuples into separate components\n",
    "        states, actions, rewards, next_states, done_flags = list(zip(*samples))\n",
    "        # Convert each component into a numpy array and return\n",
    "        return (np.array(states),\n",
    "                np.array(actions),\n",
    "                np.array(rewards),\n",
    "                np.array(next_states),\n",
    "                np.array(done_flags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TD Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(agent, target_network, states, actions, rewards, \n",
    "                    next_states, done_flags, gamma=0.99, device=device):\n",
    "    \"\"\"\n",
    "    Compute the TD loss for a batch of experiences.\n",
    "\n",
    "    Args:\n",
    "        agent (nn.Module): The current Q-network.\n",
    "        target_network (nn.Module): The target Q-network.\n",
    "        states (np.array): Batch of current states.\n",
    "        actions (np.array): Batch of actions taken.\n",
    "        rewards (np.array): Batch of rewards received.\n",
    "        next_states (np.array): Batch of next states.\n",
    "        done_flags (np.array): Batch of done flags (True/False).\n",
    "        gamma (float): Discount factor.\n",
    "        device: Device to run the computations on (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The computed TD loss.\n",
    "    \"\"\"\n",
    "    # Convert numpy arrays to torch tensors and move them to the appropriate device\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float)\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.long)\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    done_flags = torch.tensor(done_flags.astype('float32'), device=device, dtype=torch.float)\n",
    "\n",
    "    # Compute Q-values for all actions for the current states using the agent network.\n",
    "    predicted_qvalues = agent(states)  # shape: [batch_size, n_actions]\n",
    "\n",
    "    # Compute Q-values for all actions for the next states using the target network.\n",
    "    predicted_next_qvalues = target_network(next_states)  # shape: [batch_size, n_actions]\n",
    "\n",
    "    # For each experience in the batch, select the Q-value for the taken action.\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions]\n",
    "\n",
    "    # Compute the maximum Q-value for the next states (greedy action selection)\n",
    "    next_state_values, _ = torch.max(predicted_next_qvalues, dim=1)\n",
    "\n",
    "    # Compute the target Q-values using the TD target equation:\n",
    "    # target = reward + gamma * max_a' Q(s', a') * (1 - done)\n",
    "    # (1 - done) ensures that if the next state is terminal, \n",
    "    # we don't add the discounted future reward.\n",
    "    target_qvalues_for_actions = rewards + gamma * next_state_values * (1 - done_flags)\n",
    "\n",
    "    # Compute the Mean Squared Error (MSE) loss between the predicted Q-values and the target Q-values.\n",
    "    loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Recording video of trained agents**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_video(env_id, video_folder, video_length, agent):\n",
    "    \"\"\"\n",
    "    Record a video of the agent interacting with the environment.\n",
    "\n",
    "    Args:\n",
    "        env_id (str): Environment ID (e.g., 'CartPole-v1').\n",
    "        video_folder (str): Folder where the video will be saved.\n",
    "        video_length (int): Number of timesteps to record.\n",
    "        agent: Trained agent with a get_action() method.\n",
    "    \n",
    "    Returns:\n",
    "        str: The file path to the saved video.\n",
    "    \"\"\"\n",
    "    # Create a dummy vectorized environment with rendering enabled.\n",
    "    vec_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "    \n",
    "    # Wrap the environment with VecVideoRecorder.\n",
    "    vec_env = VecVideoRecorder(\n",
    "        vec_env, video_folder,\n",
    "        record_video_trigger=lambda x: x == 0,\n",
    "        video_length=video_length,\n",
    "        name_prefix=f\"{type(agent).__name__}-{env_id}\"\n",
    "    )\n",
    "\n",
    "    # Reset environment to start recording.\n",
    "    obs = vec_env.reset()\n",
    "    for _ in range(video_length + 1):\n",
    "        # Get action from the agent and step the environment.\n",
    "        action = agent.get_action(obs).detach().cpu().numpy()\n",
    "        obs, _, _, _ = vec_env.step(action)\n",
    "    \n",
    "    # Construct the file path of the recorded video.\n",
    "    file_path = \"./\" + video_folder + vec_env.video_recorder.path.split(\"/\")[-1]\n",
    "    vec_env.close()\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_video(file_path):\n",
    "    \"\"\"\n",
    "    Display a video file in a Jupyter Notebook.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the video file.\n",
    "\n",
    "    Returns:\n",
    "        HTML: HTML object that can display the video.\n",
    "    \"\"\"\n",
    "    # Read video file in binary mode.\n",
    "    mp4 = open(file_path, 'rb').read()\n",
    "    # Encode the video file in base64.\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    # Create HTML snippet with a video player.\n",
    "    return HTML(f\"\"\"\n",
    "        <video width=400 controls>\n",
    "            <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "        </video>\n",
    "        \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setting up the training parameters**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record(start_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Run the agent in the environment for a fixed number of steps and record the transitions.\n",
    "\n",
    "    This function allows the agent to interact with the environment for `n_steps` timesteps,\n",
    "    collects the transitions (state, action, reward, next state, done flag), and stores them\n",
    "    in the experience replay buffer. It also accumulates the total reward obtained during these steps.\n",
    "\n",
    "    Args:\n",
    "        start_state: The initial state from which the agent starts.\n",
    "        agent: The DQN agent that provides action selection via its get_qvalues and sample_actions methods.\n",
    "        env: The environment in which the agent is acting (should follow the Gymnasium API).\n",
    "        exp_replay: The experience replay buffer (an instance of ReplayBuffer) to store transitions.\n",
    "        n_steps (int, optional): The number of steps to run the agent in the environment. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple (sum_rewards, s) where:\n",
    "            - sum_rewards (float): The total reward accumulated over the n_steps.\n",
    "            - s: The state at the end of the n_steps, which can be used as the starting state for subsequent calls.\n",
    "    \"\"\"\n",
    "    s = start_state          # Initialize the current state.\n",
    "    sum_rewards = 0          # Initialize the reward accumulator.\n",
    "\n",
    "    # Run the agent for n_steps steps.\n",
    "    for _ in range(n_steps):\n",
    "        # Obtain Q-values for the current state.\n",
    "        qvalues = agent.get_qvalues([s])\n",
    "        \n",
    "        # Select an action using the agent's epsilon-greedy policy.\n",
    "        a = agent.sample_actions(qvalues)[0]\n",
    "        \n",
    "        # Execute the action in the environment.\n",
    "        next_s, r, terminated, truncated, _ = env.step(a)\n",
    "        \n",
    "        # Accumulate the reward.\n",
    "        sum_rewards += r\n",
    "        \n",
    "        # Check if the episode has ended.\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Record the transition in the replay buffer.\n",
    "        exp_replay.add(s, a, r, next_s, done)\n",
    "        \n",
    "        # Update the current state:\n",
    "        # If the episode ended, reset the environment; otherwise, continue with the next state.\n",
    "        if done:\n",
    "            s, _ = env.reset()\n",
    "        else:\n",
    "            s = next_s\n",
    "\n",
    "    return sum_rewards, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the environment and agent networks\n",
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\", max_episode_steps=4000)    # Create the environment\n",
    "state_dim = env.observation_space.shape     # e.g., (4,) for CartPole\n",
    "n_actions = env.action_space.n              # e.g., 2 for CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment and set seed for reproducibility\n",
    "state, _ = env.reset(seed=seed)\n",
    "\n",
    "# Initialize DQN agent with initial high exploration (epsilon=1)\n",
    "agent = DuelingDQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network = DuelingDQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())  # Synchronize target network\n",
    "\n",
    "# Populate the experience replay buffer with initial random experiences\n",
    "exp_replay = ReplayBuffer(10**4)  # Replay buffer with capacity 10,000\n",
    "for i in range(100):\n",
    "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)  # Helper function to collect experiences\n",
    "    if len(exp_replay) == 10**4:\n",
    "        break\n",
    "\n",
    "# Set up training hyperparameters\n",
    "timesteps_per_epoch = 1        # Timesteps per epoch (for logging purposes)\n",
    "batch_size = 32                # Mini-batch size for training updates\n",
    "total_steps = 50000            # Total training steps\n",
    "\n",
    "# Initialize the optimizer (Adam) for updating the agent's parameters\n",
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "# Define the exploration schedule (epsilon decay)\n",
    "start_epsilon = 1              # Starting exploration rate\n",
    "end_epsilon = 0.05             # Minimum exploration rate\n",
    "eps_decay_final_step = 2 * 10**4  # Steps over which epsilon decays to end_epsilon\n",
    "\n",
    "# Define frequencies for logging and updating the target network\n",
    "loss_freq = 20                      # Log the loss every 20 steps\n",
    "refresh_target_network_freq = 100   # Update target network every 100 steps\n",
    "eval_freq = 1000                    # Evaluate the agent every 1000 steps\n",
    "\n",
    "# Set gradient clipping threshold to stabilize training\n",
    "max_grad_norm = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "mean_rw_history = []\n",
    "td_loss_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The main training loop**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_schedule(start_eps, end_eps, step, final_step):\n",
    "    \"\"\"\n",
    "    Compute the exploration epsilon for the current step using a linear decay schedule.\n",
    "\n",
    "    Args:\n",
    "        start_eps (float): The initial epsilon (e.g., 1.0).\n",
    "        end_eps (float): The final epsilon after decay (e.g., 0.05).\n",
    "        step (int): The current training step.\n",
    "        final_step (int): The step at which epsilon decays to end_eps.\n",
    "\n",
    "    Returns:\n",
    "        float: The computed epsilon value for the current step.\n",
    "    \"\"\"\n",
    "    # Ensure the step does not exceed final_step for correct interpolation.\n",
    "    return start_eps + (end_eps - start_eps) * min(step, final_step) / final_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothen(values):\n",
    "    \"\"\"\n",
    "    Smooths out the given values using a Gaussian filter.\n",
    "\n",
    "    Args:\n",
    "        values (list or np.array): The sequence of values to smooth.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The smoothed values.\n",
    "    \"\"\"\n",
    "    kernel = gaussian(100, std=100)\n",
    "    kernel = kernel / np.sum(kernel)\n",
    "    return convolve(values, kernel, 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\"\n",
    "    Evaluate the agent's performance by running it for a specified number of games.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment to evaluate in.\n",
    "        agent (DQNAgent): The DQN agent.\n",
    "        n_games (int): Number of games (episodes) to run.\n",
    "        greedy (bool): If True, use the greedy policy (argmax); otherwise use epsilon-greedy.\n",
    "        t_max (int): Maximum timesteps per episode.\n",
    "\n",
    "    Returns:\n",
    "        float: The average total reward over the evaluated games.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        for _ in range(t_max):\n",
    "            # Get Q-values from the agent.\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            # Choose action: greedy (argmax) if specified, otherwise use agent's sampling.\n",
    "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
    "            s, r, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += r\n",
    "            if terminated:\n",
    "                break\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(total_steps, timesteps_per_epoch, batch_size, \n",
    "              start_epsilon, end_epsilon, eps_decay_final_step,\n",
    "              loss_freq, refresh_target_network_freq, eval_freq,\n",
    "              max_grad_norm, agent, target_network, env, exp_replay,\n",
    "              opt, td_loss_history, mean_rw_history, env_name, device):\n",
    "    \"\"\"\n",
    "    Main training loop for the DQN agent.\n",
    "\n",
    "    The function updates the agent by:\n",
    "      - Decaying the exploration rate.\n",
    "      - Collecting experiences and storing them in the replay buffer.\n",
    "      - Sampling mini-batches from the replay buffer.\n",
    "      - Computing the TD loss and performing backpropagation.\n",
    "      - Periodically updating the target network.\n",
    "      - Evaluating and logging the agent's performance.\n",
    "\n",
    "    Args:\n",
    "        total_steps (int): Total number of training steps.\n",
    "        timesteps_per_epoch (int): Number of environment steps per training epoch.\n",
    "        batch_size (int): Mini-batch size for training.\n",
    "        start_epsilon (float): Initial exploration rate.\n",
    "        end_epsilon (float): Final exploration rate after decay.\n",
    "        eps_decay_final_step (int): The step at which epsilon should reach end_epsilon.\n",
    "        loss_freq (int): Frequency (in steps) to log TD loss.\n",
    "        refresh_target_network_freq (int): Frequency (in steps) to update the target network.\n",
    "        eval_freq (int): Frequency (in steps) to evaluate the agent.\n",
    "        max_grad_norm (float): Maximum gradient norm for clipping.\n",
    "        agent (DQNAgent): The online agent network.\n",
    "        target_network (DQNAgent): The target network.\n",
    "        env (gym.Env): The environment for interaction.\n",
    "        exp_replay (ReplayBuffer): The experience replay buffer.\n",
    "        opt (torch.optim.Optimizer): The optimizer for training.\n",
    "        td_loss_history (list): List to record TD loss history.\n",
    "        mean_rw_history (list): List to record mean reward history.\n",
    "        env_name (str): Environment name (used for creating a new env during evaluation).\n",
    "        device (torch.device): Device to perform computations on.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Reset the environment to get the initial state.\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    # Main training loop.\n",
    "    for step in trange(total_steps + 1):\n",
    "        # 1. Update exploration rate (epsilon) based on schedule.\n",
    "        agent.epsilon = epsilon_schedule(start_epsilon, end_epsilon, step, eps_decay_final_step)\n",
    "\n",
    "        # 2. Interact with the environment and record experiences.\n",
    "        #    play_and_record() should update the replay buffer.\n",
    "        _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "        # 3. Sample a mini-batch from the replay buffer.\n",
    "        states, actions, rewards, next_states, done_flags = exp_replay.sample(batch_size)\n",
    "\n",
    "        # 4. Compute the TD loss using the agent and target networks.\n",
    "        loss = compute_td_loss(agent, target_network,\n",
    "                               states, actions, rewards, next_states, done_flags,\n",
    "                               gamma=0.99, device=device)\n",
    "\n",
    "        # 5. Perform backpropagation and update the network.\n",
    "        loss.backward()\n",
    "        # Clip gradients to stabilize training.\n",
    "        nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # 6. Log the TD loss at specified intervals.\n",
    "        if step % loss_freq == 0:\n",
    "            td_loss_history.append(loss.data.cpu().item())\n",
    "\n",
    "        # 7. Update the target network periodically.\n",
    "        if step % refresh_target_network_freq == 0:\n",
    "            target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "        # 8. Evaluate the agent and update logs/plots.\n",
    "        if step % eval_freq == 0:\n",
    "            # Create a fresh environment for evaluation.\n",
    "            eval_env = gym.make(env_name, render_mode=\"rgb_array\", max_episode_steps=4000)\n",
    "            mean_reward = evaluate(eval_env, agent, n_games=3, greedy=True, t_max=1000)\n",
    "            mean_rw_history.append(mean_reward)\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            print(\"Buffer size = %i, Epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
    "\n",
    "            # Plot the mean return and smoothened TD loss.\n",
    "            plt.figure(figsize=[16, 5])\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.title(\"Mean return per episode\")\n",
    "            plt.plot(mean_rw_history)\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.title(\"TD loss history (smoothened)\")\n",
    "            plt.plot(smoothen(td_loss_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Applying DQN on the CartPole**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dqn(total_steps, timesteps_per_epoch, batch_size,\n",
    "          start_epsilon, end_epsilon, eps_decay_final_step,\n",
    "          loss_freq, refresh_target_network_freq, eval_freq,\n",
    "          max_grad_norm, agent, target_network, env, exp_replay,\n",
    "          opt, td_loss_history, mean_rw_history, env_name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = evaluate(\n",
    "  gym.make(env_name, render_mode=\"rgb_array\", max_episode_steps=4000),\n",
    "  agent, n_games=30, greedy=True, t_max=1000\n",
    ")\n",
    "print('final score:', final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_folder = \"\"  # enter folder location\n",
    "# video_length = 500\n",
    "\n",
    "# video_file = record_video(env_name, video_folder, video_length, agent)\n",
    "\n",
    "# play_video(video_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
