{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DQN with Prioritized Replay**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **This notebook is based on [Nimish Sanghi's book on Deep RL](https://link.springer.com/book/10.1007/979-8-8688-0273-7).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds up on the basic Deep Q-Networks (DQNs) notebook: https://www.kaggle.com/code/aryamanbansal/basic-dqn\n",
    "\n",
    "Feel free to check out this notebook on Kaggle: https://www.kaggle.com/code/aryamanbansal/prioritized-replay-dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Motivation**\n",
    "\n",
    "In basic Deep Q-Networks (DQNs), the agent stores past experiences (state, action, reward, next state) in a replay buffer and samples them uniformly during training. However, not all experiences are equally valuable for learning.\n",
    "Prioritized Replay addresses this by sampling more frequently those experiences that have a higher expected learning value - usually measured by the magnitude of their temporal-difference (TD) error. The intuition is: if an experience produces a large TD error, then it means the network did not predict that outcome well, and thus, learning from it should be more beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparative Study with Basic DQN**\n",
    "\n",
    "-  Basic DQN: Uses uniform sampling. Every experience is equally likely to be drawn, regardless of how \"surprising\" or \"informative\" it is.\n",
    "\n",
    "- Prioritized Replay DQN:\n",
    "    - Advantages: Focuses learning on experiences that have higher TD errors, potentially speeding up learning and reducing wasted updates on already well-learned experiences.\n",
    "    - Drawbacks: Introduces bias because the sampling is no longer uniform. This bias is corrected via importance sampling, but it adds extra computational overhead and complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:06:53.772071Z",
     "iopub.status.busy": "2025-02-02T08:06:53.771621Z",
     "iopub.status.idle": "2025-02-02T08:06:53.780178Z",
     "shell.execute_reply": "2025-02-02T08:06:53.778728Z",
     "shell.execute_reply.started": "2025-02-02T08:06:53.772038Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports done!\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.signal import convolve\n",
    "from scipy.signal.windows import gaussian\n",
    "\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML, clear_output\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "print(\"imports done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:25.136352Z",
     "iopub.status.busy": "2025-02-02T08:05:25.135639Z",
     "iopub.status.idle": "2025-02-02T08:05:25.152124Z",
     "shell.execute_reply": "2025-02-02T08:05:25.150741Z",
     "shell.execute_reply.started": "2025-02-02T08:05:25.136299Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x786050e12d90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set a seed\n",
    "seed = 132\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:25.154957Z",
     "iopub.status.busy": "2025-02-02T08:05:25.154645Z",
     "iopub.status.idle": "2025-02-02T08:05:25.187070Z",
     "shell.execute_reply": "2025-02-02T08:05:25.186022Z",
     "shell.execute_reply.started": "2025-02-02T08:05:25.154930Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming a global device setting (CPU or CUDA)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Neural Network for DQN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:25.188910Z",
     "iopub.status.busy": "2025-02-02T08:05:25.188598Z",
     "iopub.status.idle": "2025-02-02T08:05:25.207966Z",
     "shell.execute_reply": "2025-02-02T08:05:25.206801Z",
     "shell.execute_reply.started": "2025-02-02T08:05:25.188884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the DQNAgent.\n",
    "        \n",
    "        Args:\n",
    "            state_shape (tuple): Shape of the input state.\n",
    "            n_actions (int): Number of possible actions.\n",
    "            epsilon (float): Exploration rate for epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "        \n",
    "        # Define a simple feedforward neural network\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_shape[0], 192),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(192, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_actions)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        Pass the state at time t through the network to get the Q-value Q(s,a).\n",
    "        \n",
    "        Args:\n",
    "            state_t (torch.Tensor): The state at time t.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Q-values for each action.\n",
    "        \"\"\"\n",
    "        qvalues = self.network(state_t)\n",
    "        return qvalues\n",
    "\n",
    "    \n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        Compute Q-values for a batch of states provided as arrays.\n",
    "        \n",
    "        Args:\n",
    "            states (array-like): Batch of states.\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Q-values for each state.\n",
    "        \"\"\"\n",
    "        states = torch.tensor(np.array(states), device=device, dtype=torch.float32)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    \n",
    "    def get_action(self, states):\n",
    "        \"\"\"\n",
    "        Returns the best (greedy) actions for a batch of states.\n",
    "        \n",
    "        Args:\n",
    "            states (array-like): Batch of states.\n",
    "            \n",
    "        Returns:\n",
    "            torch.tensor: Best actions.\n",
    "        \"\"\"\n",
    "        states = torch.tensor(np.array(states), device=device, dtype=torch.float32)\n",
    "        qvalues = self.forward(states)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "        return best_actions\n",
    "\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"\n",
    "        Implements the epsilon-greedy policy on a batch of Q-values\n",
    "        \n",
    "        Args:\n",
    "            qvalues (np.array): Q-values for a batch of states.\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Actions selected (random with probability epsilon, otherwise best action).\n",
    "        \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "        # Randomly choose actions for exploration\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        # Greedy actions based on Q-values\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "        # Create an array of booleans indicating whether to explore (0) or exploit (1)\n",
    "        should_explore = np.random.choice([0, 1], batch_size, p=[1 - epsilon, epsilon])\n",
    "        # Choose random actions where element is 0, otherwise choose best actions\n",
    "        return np.where(should_explore, random_actions, best_actions)\n",
    "\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Saves the model parameters to a file.\n",
    "        \n",
    "        Args:\n",
    "            path (str): Path to save the model.\n",
    "        \"\"\"\n",
    "        print(\"Saving model to:\", path)\n",
    "        torch.save(self.network.state_dict(), f\"{path}.zip\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prioritized Replay Buffer**\n",
    "\n",
    "#### Overview\n",
    "\n",
    "1. Calculate TD Error $\\delta_i$ for each experience $i$.\n",
    "2. Calculate priority $p_i$ for each experience $i$.\n",
    "3. Sample each experience $i$ with probability $P(i)$.\n",
    "4. Use importance sampling weights to correct the bias introduced by prioritized sampling.\n",
    "\n",
    "#### In-depth explanation\n",
    "\n",
    "1. **TD Error Calculation**:\n",
    "\n",
    "$$\\delta_i \\;\\; = \\;\\; r_i + \\gamma \\max_{a'}Q_{\\text{target}}(s'_i, a') - Q_{\\text{current}}(s_i, a_i)$$\n",
    "\n",
    ", where:\n",
    "- $Q_{\\text{target}}$ is the target Q-network (less frequently updated),\n",
    "- $Q_{\\text{current}}$ is the current Q-network (updated at each step),\n",
    "- $r_i$ is the immediate reward in the $i^{th}$ sample,\n",
    "- $s_i$ is the current state in the $i^{th}$ sample,\n",
    "- $a_i$ is the action taken in the current state in the $i^{th}$ sample,\n",
    "- $s'_i$ is the next state in the $i^{th}$ sample,\n",
    "- $a'_i$ is the next (greedy) action in the $i^{th}$ sample.\n",
    "\n",
    "2. **Priority Assignment**:\n",
    "\n",
    "Instead of uniform sampling, we assign a priority $p_i$ to each experience $i$ in the replay buffer. A common choice is to use: \n",
    "\n",
    "$$ p_i = |\\delta_i| + \\epsilon $$\n",
    "\n",
    "where:\n",
    "- $\\delta_i$ is the TD error of experience $i$\n",
    "- $\\epsilon$ is a small positive constant to ensure every experience has some chance of being sampled, ie, even experiences with a very low TD error have a non-zero chance of being sampled.\n",
    "\n",
    "3. **Sampling Probability**:\n",
    "\n",
    "The probability of sampling a given experience is:\n",
    "\n",
    "$$ P(i) = \\frac{p_i^\\alpha}{\\sum_{k=1}^n p_k^\\alpha} $$\n",
    "\n",
    ", where: \n",
    "- $\\alpha$ determines how much prioritization is used: \n",
    "    - $\\alpha = 0$: corresponding to uniform sampling\n",
    "    - $\\alpha = 1$: corresponding to full prioritization\n",
    "- $n$ is the length of the replay buffer.\n",
    "\n",
    "4. **Importance Sampling Weights**:\n",
    "\n",
    "To correct for the bias introduced by prioritized sampling, we use importance sampling (IS) weights:\n",
    "\n",
    "$$w_i = (N \\cdot P(i))^{-\\beta}$$\n",
    "\n",
    ", where:\n",
    "- $N$ is the size of the replay buffer\n",
    "- $\\beta$ is a hyperparameter that controls how much importance sampling is used, and $\\beta$ is annealed from an initial value (eg:, 0.4) to 1 during training:\n",
    "    - $\\beta = 0$: no importance sampling\n",
    "    - $\\beta = 1$: full importance sampling\n",
    "- These weights $w_i$ are then used to scale the loss when updating the network, ensuring unbiased updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:25.209252Z",
     "iopub.status.busy": "2025-02-02T08:05:25.208972Z",
     "iopub.status.idle": "2025-02-02T08:05:25.230812Z",
     "shell.execute_reply": "2025-02-02T08:05:25.229527Z",
     "shell.execute_reply.started": "2025-02-02T08:05:25.209229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"\n",
    "    Prioritized Replay Buffer for experience replay in Deep Q-Networks (DQNs).\n",
    "\n",
    "    This buffer stores experiences (state, action, reward, next_state, done) and assigns\n",
    "    a priority to each experience so that experiences with higher TD errors are more likely\n",
    "    to be sampled during training.\n",
    "\n",
    "    Attributes:\n",
    "        size (int): Maximum number of experiences to store.\n",
    "        buffer (list): List storing the experiences.\n",
    "        next_id (int): The next index for inserting a new experience (used in a circular manner).\n",
    "        alpha (float): Degree of prioritization (0 means no prioritization, 1 means full prioritization).\n",
    "        beta (float): Degree of importance sampling correction for bias introduced by prioritization.\n",
    "        priorities (np.array): Array holding the priority values for each experience.\n",
    "        epsilon (float): Small constant added to priorities to avoid zero priority.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size, alpha=0.6, beta=0.4):\n",
    "        \"\"\"\n",
    "        Initializes the Prioritized Replay Buffer.\n",
    "\n",
    "        Args:\n",
    "            size (int): Maximum number of experiences to store.\n",
    "            alpha (float): Degree of prioritization.\n",
    "            beta (float): Degree of importance sampling correction.\n",
    "        \"\"\"\n",
    "        self.size = size                   # Maximum number of items in the buffer\n",
    "        self.buffer = []                   # List to hold experiences\n",
    "        self.next_id = 0                   # Next index to insert new experience\n",
    "        self.alpha = alpha                 # Exponent for prioritization\n",
    "        self.beta = beta                   # Exponent for importance sampling weights\n",
    "        self.priorities = np.ones(size)    # Initialize priorities to 1 for all possible entries\n",
    "        self.epsilon = 1e-5                # Small constant to avoid zero priority\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the current number of experiences in the buffer.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Adds a new experience to the buffer.\n",
    "\n",
    "        If the buffer is not full, the experience is appended.\n",
    "        Otherwise, the experience at the current insertion index is overwritten.\n",
    "\n",
    "        Args:\n",
    "            state: The state observed.\n",
    "            action: The action taken.\n",
    "            reward: The reward received.\n",
    "            next_state: The next state observed.\n",
    "            done: Boolean flag indicating if the episode ended.\n",
    "        \"\"\"\n",
    "        # Create an experience tuple\n",
    "        item = (state, action, reward, next_state, done)\n",
    "        \n",
    "        # Get the maximum current priority (for new experience assignment)\n",
    "        max_priority = self.priorities.max()\n",
    "        \n",
    "        # If buffer is not full, append the new experience; otherwise, overwrite the oldest experience.\n",
    "        if len(self.buffer) < self.size:\n",
    "            self.buffer.append(item)\n",
    "        else:\n",
    "            self.buffer[self.next_id] = item\n",
    "        \n",
    "        # Set the priority of the new experience to the maximum current priority\n",
    "        self.priorities[self.next_id] = max_priority\n",
    "        \n",
    "        # Update the index for the next insertion in a circular manner\n",
    "        self.next_id = (self.next_id + 1) % self.size\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Samples a batch of experiences according to their priorities.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Number of experiences to sample.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Contains the following arrays:\n",
    "                - states (np.array): Batch of states.\n",
    "                - actions (np.array): Batch of actions.\n",
    "                - rewards (np.array): Batch of rewards.\n",
    "                - next_states (np.array): Batch of next states.\n",
    "                - done_flags (np.array): Batch of done flags.\n",
    "                - weights (np.array): Batch of importance sampling weights.\n",
    "                - idxs (np.array): Indices of the sampled experiences.\n",
    "        \"\"\"\n",
    "        # Total number of experiences currently in the buffer\n",
    "        N = len(self.buffer)\n",
    "        \n",
    "        # Get the current priorities for stored experiences\n",
    "        priorities = self.priorities[:N]\n",
    "        \n",
    "        # Compute the probability distribution for sampling using priorities and alpha\n",
    "        probabilities = priorities ** self.alpha\n",
    "        probabilities /= probabilities.sum()\n",
    "        \n",
    "        # Compute importance sampling weights to correct for the non-uniform probability\n",
    "        weights = (N * probabilities) ** (-self.beta)\n",
    "        weights /= weights.max()  # Normalize weights for stability\n",
    "\n",
    "        # Sample indices based on the computed probability distribution\n",
    "        idxs = np.random.choice(N, batch_size, p=probabilities)\n",
    "\n",
    "        # Retrieve experiences based on the sampled indices\n",
    "        samples = [self.buffer[i] for i in idxs]\n",
    "        states, actions, rewards, next_states, done_flags = list(zip(*samples))\n",
    "        \n",
    "        # Get the weights corresponding to the sampled indices\n",
    "        weights = weights[idxs]\n",
    "        \n",
    "        return (np.array(states), np.array(actions), np.array(rewards), \n",
    "                np.array(next_states), np.array(done_flags), np.array(weights), np.array(idxs))\n",
    "        \n",
    "    def update_priorities(self, idxs, new_priorities):\n",
    "        \"\"\"\n",
    "        Updates the priorities of experiences at the specified indices.\n",
    "\n",
    "        Args:\n",
    "            idxs (np.array or list): Indices of experiences to update.\n",
    "            new_priorities (np.array): New priority values corresponding to each index.\n",
    "        \"\"\"\n",
    "        # Update priorities with the new computed values, adding epsilon to avoid zero priorities\n",
    "        self.priorities[idxs] = new_priorities + self.epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TD Loss for Prioritized Replay**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:25.232540Z",
     "iopub.status.busy": "2025-02-02T08:05:25.232117Z",
     "iopub.status.idle": "2025-02-02T08:05:25.254433Z",
     "shell.execute_reply": "2025-02-02T08:05:25.253427Z",
     "shell.execute_reply.started": "2025-02-02T08:05:25.232475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_td_loss_priority_replay(agent, target_network, replay_buffer,\n",
    "                                    states, actions, rewards, next_states, done_flags, weights, buffer_idxs,\n",
    "                                    gamma=0.99, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Computes the TD loss for a batch of experiences sampled from a prioritized replay buffer,\n",
    "    and updates the priorities in the buffer based on the new TD errors.\n",
    "\n",
    "    Args:\n",
    "        agent (torch.nn.Module): The Q-network used for predicting current Q-values.\n",
    "        target_network (torch.nn.Module): The target Q-network used for computing target Q-values.\n",
    "        replay_buffer (PrioritizedReplayBuffer): The replay buffer from which experiences were sampled.\n",
    "        states (np.array): Batch of states.\n",
    "        actions (np.array): Batch of actions taken.\n",
    "        rewards (np.array): Batch of rewards received.\n",
    "        next_states (np.array): Batch of next states observed.\n",
    "        done_flags (np.array): Batch of done flags (1 if episode ended, 0 otherwise).\n",
    "        weights (np.array): Batch of importance sampling weights for each experience.\n",
    "        buffer_idxs (np.array): Indices of the sampled experiences in the replay buffer.\n",
    "        gamma (float): Discount factor for future rewards.\n",
    "        device (torch.device): Device on which to perform tensor computations (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The computed mean TD loss.\n",
    "    \"\"\"\n",
    "    # Convert numpy arrays to torch tensors and send them to the specified device\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float)\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.long)\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    done_flags = torch.tensor(done_flags.astype('float32'), device=device, dtype=torch.float)\n",
    "    weights = torch.tensor(weights, device=device, dtype=torch.float)\n",
    "\n",
    "    # Get Q-values for all actions in the current states using the agent network\n",
    "    predicted_qvalues = agent(states)\n",
    "\n",
    "    # Get Q-values for all actions in the next states using the target network\n",
    "    predicted_next_qvalues = target_network(next_states)\n",
    "    \n",
    "    # Select the Q-values corresponding to the actions that were actually taken\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions]\n",
    "\n",
    "    # Compute the maximum Q-value for next states for each experience\n",
    "    next_state_values, _ = torch.max(predicted_next_qvalues, dim=1)\n",
    "\n",
    "    # Compute target Q-values using the Bellman equation:\n",
    "    # target = reward + gamma * max(Q(next_state)) * (1 - done)\n",
    "    target_qvalues_for_actions = rewards + gamma * next_state_values * (1 - done_flags)\n",
    "    \n",
    "    # Compute the squared TD error for each sample, scaled by the importance sampling weights\n",
    "    loss = ((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2) * weights\n",
    "    \n",
    "    # Calculate the mean loss over the batch\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    # Update priorities in the replay buffer using the absolute TD errors\n",
    "    with torch.no_grad():\n",
    "        # Compute new priorities as the absolute TD error (detach to avoid gradient computation)\n",
    "        new_priorities = predicted_qvalues_for_actions.detach() - target_qvalues_for_actions.detach()\n",
    "        new_priorities = np.absolute(new_priorities.detach().cpu().numpy())\n",
    "        \n",
    "        # Update the replay buffer priorities at the corresponding indices\n",
    "        replay_buffer.update_priorities(buffer_idxs, new_priorities)\n",
    "        \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Recording video of trained agents**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:25.256122Z",
     "iopub.status.busy": "2025-02-02T08:05:25.255737Z",
     "iopub.status.idle": "2025-02-02T08:05:25.278016Z",
     "shell.execute_reply": "2025-02-02T08:05:25.276865Z",
     "shell.execute_reply.started": "2025-02-02T08:05:25.256093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def record_video(env_id, video_folder, video_length, agent):\n",
    "    \"\"\"\n",
    "    Record a video of the agent interacting with the environment.\n",
    "\n",
    "    Args:\n",
    "        env_id (str): Environment ID (e.g., 'CartPole-v1').\n",
    "        video_folder (str): Folder where the video will be saved.\n",
    "        video_length (int): Number of timesteps to record.\n",
    "        agent: Trained agent with a get_action() method.\n",
    "    \n",
    "    Returns:\n",
    "        str: The file path to the saved video.\n",
    "    \"\"\"\n",
    "    # Create a dummy vectorized environment with rendering enabled.\n",
    "    vec_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "    \n",
    "    # Wrap the environment with VecVideoRecorder.\n",
    "    vec_env = VecVideoRecorder(\n",
    "        vec_env, video_folder,\n",
    "        record_video_trigger=lambda x: x == 0,\n",
    "        video_length=video_length,\n",
    "        name_prefix=f\"{type(agent).__name__}-{env_id}\"\n",
    "    )\n",
    "\n",
    "    # Reset environment to start recording.\n",
    "    obs = vec_env.reset()\n",
    "    for _ in range(video_length + 1):\n",
    "        # Get action from the agent and step the environment.\n",
    "        action = agent.get_action(obs).detach().cpu().numpy()\n",
    "        obs, _, _, _ = vec_env.step(action)\n",
    "    \n",
    "    # Construct the file path of the recorded video.\n",
    "    file_path = \"./\" + video_folder + vec_env.video_recorder.path.split(\"/\")[-1]\n",
    "    vec_env.close()\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:25.279445Z",
     "iopub.status.busy": "2025-02-02T08:05:25.278999Z",
     "iopub.status.idle": "2025-02-02T08:05:25.296071Z",
     "shell.execute_reply": "2025-02-02T08:05:25.294595Z",
     "shell.execute_reply.started": "2025-02-02T08:05:25.279367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def play_video(file_path):\n",
    "    \"\"\"\n",
    "    Display a video file in a Jupyter Notebook.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the video file.\n",
    "\n",
    "    Returns:\n",
    "        HTML: HTML object that can display the video.\n",
    "    \"\"\"\n",
    "    # Read video file in binary mode.\n",
    "    mp4 = open(file_path, 'rb').read()\n",
    "    # Encode the video file in base64.\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    # Create HTML snippet with a video player.\n",
    "    return HTML(f\"\"\"\n",
    "        <video width=400 controls>\n",
    "            <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "        </video>\n",
    "        \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setting up the training parameters**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:25.300323Z",
     "iopub.status.busy": "2025-02-02T08:05:25.299984Z",
     "iopub.status.idle": "2025-02-02T08:05:25.318810Z",
     "shell.execute_reply": "2025-02-02T08:05:25.317517Z",
     "shell.execute_reply.started": "2025-02-02T08:05:25.300296Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def play_and_record(start_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Run the agent in the environment for a fixed number of steps and record the transitions.\n",
    "\n",
    "    This function allows the agent to interact with the environment for `n_steps` timesteps,\n",
    "    collects the transitions (state, action, reward, next state, done flag), and stores them\n",
    "    in the experience replay buffer. It also accumulates the total reward obtained during these steps.\n",
    "\n",
    "    Args:\n",
    "        start_state: The initial state from which the agent starts.\n",
    "        agent: The DQN agent that provides action selection via its get_qvalues and sample_actions methods.\n",
    "        env: The environment in which the agent is acting (should follow the Gymnasium API).\n",
    "        exp_replay: The experience replay buffer (an instance of ReplayBuffer) to store transitions.\n",
    "        n_steps (int, optional): The number of steps to run the agent in the environment. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple (sum_rewards, s) where:\n",
    "            - sum_rewards (float): The total reward accumulated over the n_steps.\n",
    "            - s: The state at the end of the n_steps, which can be used as the starting state for subsequent calls.\n",
    "    \"\"\"\n",
    "    s = start_state          # Initialize the current state.\n",
    "    sum_rewards = 0          # Initialize the reward accumulator.\n",
    "\n",
    "    # Run the agent for n_steps steps and record transitions in buffer\n",
    "    for i in range(n_steps):\n",
    "        # Obtain Q-values for the current state.\n",
    "        qvalues = agent.get_qvalues([s])\n",
    "        \n",
    "        # Select an action using the agent's epsilon-greedy policy.\n",
    "        a = agent.sample_actions(qvalues)[0]\n",
    "        \n",
    "        # Execute the action in the environment.\n",
    "        next_s, r, terminated, truncated, _ = env.step(a)\n",
    "        \n",
    "        # Accumulate the reward.\n",
    "        sum_rewards += r\n",
    "        \n",
    "        # Check if the episode has ended.\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Record the transition in the replay buffer.\n",
    "        exp_replay.add(s, a, r, next_s, done)\n",
    "        \n",
    "        # Update the current state:\n",
    "        # If the episode ended, reset the environment; otherwise, continue with the next state.\n",
    "        if done:\n",
    "            s, _ = env.reset(seed=seed+i)\n",
    "        else:\n",
    "            s = next_s\n",
    "\n",
    "    return sum_rewards, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:25.320693Z",
     "iopub.status.busy": "2025-02-02T08:05:25.320316Z",
     "iopub.status.idle": "2025-02-02T08:05:25.370822Z",
     "shell.execute_reply": "2025-02-02T08:05:25.369830Z",
     "shell.execute_reply.started": "2025-02-02T08:05:25.320660Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup the environment and agent networks\n",
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\", max_episode_steps=4000)    # Create the environment\n",
    "state_dim = env.observation_space.shape     # e.g., (4,) for CartPole\n",
    "n_actions = env.action_space.n              # e.g., 2 for CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:25.372154Z",
     "iopub.status.busy": "2025-02-02T08:05:25.371867Z",
     "iopub.status.idle": "2025-02-02T08:05:31.533155Z",
     "shell.execute_reply": "2025-02-02T08:05:31.531996Z",
     "shell.execute_reply.started": "2025-02-02T08:05:25.372128Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Reset environment and set seed for reproducibility\n",
    "state, _ = env.reset(seed=seed)\n",
    "\n",
    "# Initialize DQN agent with initial high exploration (epsilon=1)\n",
    "agent = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())  # Synchronize target network\n",
    "\n",
    "# Populate the experience replay buffer with initial random experiences\n",
    "exp_replay = PrioritizedReplayBuffer(10**4)  # Replay buffer with capacity 10,000\n",
    "for i in range(100):\n",
    "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)  # Helper function to collect experiences\n",
    "    if len(exp_replay) == 10**4:\n",
    "        break\n",
    "\n",
    "# Set up training hyperparameters\n",
    "timesteps_per_epoch = 1        # Timesteps per epoch (for logging purposes)\n",
    "batch_size = 32                # Mini-batch size for training updates\n",
    "total_steps = 45000            # Total training steps\n",
    "\n",
    "# Initialize the optimizer (Adam) for updating the agent's parameters\n",
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "# Define the exploration schedule (epsilon decay)\n",
    "start_epsilon = 1              # Starting exploration rate\n",
    "end_epsilon = 0.05             # Minimum exploration rate\n",
    "eps_decay_final_step = 2 * 10**4  # Steps over which epsilon decays to end_epsilon\n",
    "\n",
    "# Define frequencies for logging and updating the target network\n",
    "loss_freq = 20                      # Log the loss every 20 steps\n",
    "refresh_target_network_freq = 100   # Update target network every 100 steps\n",
    "eval_freq = 1000                    # Evaluate the agent every 1000 steps\n",
    "\n",
    "# Set gradient clipping threshold to stabilize training\n",
    "max_grad_norm = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rw_history = []\n",
    "td_loss_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The main training loop**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:31.541407Z",
     "iopub.status.busy": "2025-02-02T08:05:31.541029Z",
     "iopub.status.idle": "2025-02-02T08:05:31.623271Z",
     "shell.execute_reply": "2025-02-02T08:05:31.622094Z",
     "shell.execute_reply.started": "2025-02-02T08:05:31.541372Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def epsilon_schedule(start_eps, end_eps, step, final_step):\n",
    "    \"\"\"\n",
    "    Compute the exploration epsilon for the current step using a linear decay schedule.\n",
    "\n",
    "    Args:\n",
    "        start_eps (float): The initial epsilon (e.g., 1.0).\n",
    "        end_eps (float): The final epsilon after decay (e.g., 0.05).\n",
    "        step (int): The current training step.\n",
    "        final_step (int): The step at which epsilon decays to end_eps.\n",
    "\n",
    "    Returns:\n",
    "        float: The computed epsilon value for the current step.\n",
    "    \"\"\"\n",
    "    # Ensure the step does not exceed final_step for correct interpolation.\n",
    "    return start_eps + (end_eps - start_eps) * min(step, final_step) / final_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:31.624930Z",
     "iopub.status.busy": "2025-02-02T08:05:31.624612Z",
     "iopub.status.idle": "2025-02-02T08:05:31.645263Z",
     "shell.execute_reply": "2025-02-02T08:05:31.643968Z",
     "shell.execute_reply.started": "2025-02-02T08:05:31.624892Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def smoothen(values):\n",
    "    \"\"\"\n",
    "    Smooths out the given values using a Gaussian filter.\n",
    "\n",
    "    Args:\n",
    "        values (list or np.array): The sequence of values to smooth.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The smoothed values.\n",
    "    \"\"\"\n",
    "    kernel = gaussian(100, std=100)\n",
    "    kernel = kernel / np.sum(kernel)\n",
    "    return convolve(values, kernel, 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:05:31.646890Z",
     "iopub.status.busy": "2025-02-02T08:05:31.646533Z",
     "iopub.status.idle": "2025-02-02T08:05:31.665291Z",
     "shell.execute_reply": "2025-02-02T08:05:31.663946Z",
     "shell.execute_reply.started": "2025-02-02T08:05:31.646856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000, seed=42):\n",
    "    \"\"\"\n",
    "    Evaluate the agent's performance by running it for a specified number of games.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment to evaluate in.\n",
    "        agent (DQNAgent): The DQN agent.\n",
    "        n_games (int): Number of games (episodes) to run.\n",
    "        greedy (bool): If True, use the greedy policy (argmax); otherwise use epsilon-greedy.\n",
    "        t_max (int): Maximum timesteps per episode.\n",
    "\n",
    "    Returns:\n",
    "        float: The average total reward over the evaluated games.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s, _ = env.reset(seed=seed+i)\n",
    "        total_reward = 0\n",
    "        for _ in range(t_max):\n",
    "            # Get Q-values from the agent.\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            # Choose action: greedy (argmax) if specified, otherwise use agent's sampling.\n",
    "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
    "            s, r, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += r\n",
    "            if terminated:\n",
    "                break\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:07:05.120645Z",
     "iopub.status.busy": "2025-02-02T08:07:05.120193Z",
     "iopub.status.idle": "2025-02-02T08:07:05.132947Z",
     "shell.execute_reply": "2025-02-02T08:07:05.131522Z",
     "shell.execute_reply.started": "2025-02-02T08:07:05.120599Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_dqn(total_steps, timesteps_per_epoch, batch_size, \n",
    "              start_epsilon, end_epsilon, eps_decay_final_step,\n",
    "              loss_freq, refresh_target_network_freq, eval_freq,\n",
    "              max_grad_norm, agent, target_network, env, exp_replay,\n",
    "              opt, td_loss_history, mean_rw_history, env_name, device):\n",
    "    \"\"\"\n",
    "    Main training loop for the DQN agent.\n",
    "\n",
    "    The function updates the agent by:\n",
    "      - Decaying the exploration rate.\n",
    "      - Collecting experiences and storing them in the replay buffer.\n",
    "      - Sampling mini-batches from the replay buffer.\n",
    "      - Computing the TD loss and performing backpropagation.\n",
    "      - Periodically updating the target network.\n",
    "      - Evaluating and logging the agent's performance.\n",
    "\n",
    "    Args:\n",
    "        total_steps (int): Total number of training steps.\n",
    "        timesteps_per_epoch (int): Number of environment steps per training epoch.\n",
    "        batch_size (int): Mini-batch size for training.\n",
    "        start_epsilon (float): Initial exploration rate.\n",
    "        end_epsilon (float): Final exploration rate after decay.\n",
    "        eps_decay_final_step (int): The step at which epsilon should reach end_epsilon.\n",
    "        loss_freq (int): Frequency (in steps) to log TD loss.\n",
    "        refresh_target_network_freq (int): Frequency (in steps) to update the target network.\n",
    "        eval_freq (int): Frequency (in steps) to evaluate the agent.\n",
    "        max_grad_norm (float): Maximum gradient norm for clipping.\n",
    "        agent (DQNAgent): The online agent network.\n",
    "        target_network (DQNAgent): The target network.\n",
    "        env (gym.Env): The environment for interaction.\n",
    "        exp_replay (ReplayBuffer): The experience replay buffer.\n",
    "        opt (torch.optim.Optimizer): The optimizer for training.\n",
    "        td_loss_history (list): List to record TD loss history.\n",
    "        mean_rw_history (list): List to record mean reward history.\n",
    "        env_name (str): Environment name (used for creating a new env during evaluation).\n",
    "        device (torch.device): Device to perform computations on.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Reset the environment to get the initial state.\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    # Main training loop.\n",
    "    for step in trange(total_steps + 1):\n",
    "        # 1. Update exploration rate (epsilon) based on schedule.\n",
    "        agent.epsilon = epsilon_schedule(start_epsilon, end_epsilon, step, eps_decay_final_step)\n",
    "\n",
    "        # 2. Interact with the environment and record experiences.\n",
    "        #    play_and_record() should update the replay buffer.\n",
    "        _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "        # 3. Sample a mini-batch from the replay buffer.\n",
    "        states, actions, rewards, next_states, done_flags, weights, idxs = exp_replay.sample(batch_size)\n",
    "\n",
    "        # 4. Compute the TD loss using the agent and target networks.\n",
    "        loss = compute_td_loss_priority_replay(agent, target_network, exp_replay,\n",
    "                               states, actions, rewards, next_states, done_flags, weights, idxs,\n",
    "                               gamma=0.99, device=device)\n",
    "\n",
    "        # 5. Perform backpropagation and update the network.\n",
    "        loss.backward()\n",
    "        # Clip gradients to stabilize training.\n",
    "        nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # 6. Log the TD loss at specified intervals.\n",
    "        if step % loss_freq == 0:\n",
    "            td_loss_history.append(loss.data.cpu().item())\n",
    "\n",
    "        # 7. Update the target network periodically.\n",
    "        if step % refresh_target_network_freq == 0:\n",
    "            target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "        # 8. Evaluate the agent and update logs/plots.\n",
    "        if step % eval_freq == 0:\n",
    "            # Create a fresh environment for evaluation.\n",
    "            eval_env = gym.make(env_name, render_mode=\"rgb_array\", max_episode_steps=4000)\n",
    "            mean_reward = evaluate(eval_env, agent, n_games=3, greedy=True, t_max=1000)\n",
    "            mean_rw_history.append(mean_reward)\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            print(\"Buffer size = %i, Epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
    "\n",
    "            # Plot the mean return and smoothened TD loss.\n",
    "            plt.figure(figsize=[16, 5])\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.title(\"Mean return per episode\")\n",
    "            plt.plot(mean_rw_history)\n",
    "            plt.grid()\n",
    "\n",
    "            assert not np.isnan(td_loss_history[-1])\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.title(\"TD loss history (smoothened)\")\n",
    "            plt.plot(smoothen(td_loss_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Applying DQN on the CartPole**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:07:10.312952Z",
     "iopub.status.busy": "2025-02-02T08:07:10.312535Z",
     "iopub.status.idle": "2025-02-02T08:11:19.587252Z",
     "shell.execute_reply": "2025-02-02T08:11:19.585947Z",
     "shell.execute_reply.started": "2025-02-02T08:07:10.312884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dqn(total_steps, timesteps_per_epoch, batch_size,\n",
    "          start_epsilon, end_epsilon, eps_decay_final_step,\n",
    "          loss_freq, refresh_target_network_freq, eval_freq,\n",
    "          max_grad_norm, agent, target_network, env, exp_replay,\n",
    "          opt, td_loss_history, mean_rw_history, env_name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T08:11:51.248096Z",
     "iopub.status.busy": "2025-02-02T08:11:51.247746Z",
     "iopub.status.idle": "2025-02-02T08:11:52.995719Z",
     "shell.execute_reply": "2025-02-02T08:11:52.994540Z",
     "shell.execute_reply.started": "2025-02-02T08:11:51.248068Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "final_score = evaluate(\n",
    "  gym.make(env_name, render_mode=\"rgb_array\", max_episode_steps=4000),\n",
    "  agent, n_games=30, greedy=True, t_max=1000\n",
    ")\n",
    "print('final score:', final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-02T08:05:31.899453Z",
     "iopub.status.idle": "2025-02-02T08:05:31.899780Z",
     "shell.execute_reply": "2025-02-02T08:05:31.899641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# video_folder = \"\"  # enter folder location\n",
    "# video_length = 500\n",
    "\n",
    "# video_file = record_video(env_name, video_folder, video_length, agent)\n",
    "\n",
    "# play_video(video_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
