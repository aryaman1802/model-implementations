{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Algorithms\n",
    "\n",
    "#### Roadmap\n",
    "\n",
    "**1. Monte Carlo Prediction**\n",
    "Estimate the value of states $V(s)$ under a given policy by averaging returns from multiple episodes.\n",
    "\n",
    "**2. Monte Carlo Control**\n",
    "Optimize the policy by learning $Q(s, a)$, the expected return for state-action pairs, and iteratively improving the policy.\n",
    "\n",
    "**3. GLIE (Greedy in the Limit with Infinite Exploration) MC Control**\n",
    "Ensures sufficient exploration of the state-action space while gradually transitioning to greedy exploitation.\n",
    "\n",
    "**4. Off-policy MC Control**\n",
    "Learns the optimal policy $\\pi^*$ while generating data using a different behavior policy $b(a|s)$ through importance sampling.\n",
    "\n",
    "**5. First Visit and Every Visit Algorithms**\n",
    "Methods for updating $V(s)$ or $Q(s, a)$ based on observed returns:\n",
    "- **First Visit**: Updates a state or state-action value only the first time it is encountered in an episode.\n",
    "- **Every Visit**: Updates the value every time a state or state-action pair is encountered.\n",
    "\n",
    "**6. Exploring Starts**\n",
    "Ensures that all state-action pairs are explored by randomly initializing episodes at different state-action pairs.\n",
    "\n",
    "**7. Importance Sampling**\n",
    "Allows off-policy learning by correcting returns sampled from a behavior policy to reflect a target policy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Monte Carlo Prediction\n",
    "- **Goal**: To estimate the value of states ($V(s)$) under a fixed policy $\\pi(a|s)$.\n",
    "- **How it works**:\n",
    "  - Run multiple episodes following the policy $\\pi(a|s)$.\n",
    "  - For each visited state $s$, calculate the return ($G_t$) from the time $t$ it is encountered to the episode's end.\n",
    "  - Update $V(s)$ as the average of all observed returns:\n",
    "    $$V(s) = \\mathbb{E}[G_t \\mid S_t = s]$$\n",
    "- **When to use**: For policy evaluation tasks where the policy is fixed and does not need improvement.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Monte Carlo Control\n",
    "- **Goal**: To optimize a policy by learning $Q(s, a)$, the expected return for each state-action pair, and iteratively improving the policy.\n",
    "- **How it works**:\n",
    "  1. Start with an arbitrary policy $\\pi(a|s)$ and initialize $Q(s, a)$ arbitrarily.\n",
    "  2. Run multiple episodes, updating $Q(s, a)$ as:\n",
    "     $$Q(s, a) = \\mathbb{E}[G_t \\mid S_t = s, A_t = a]$$\n",
    "  3. Improve the policy $\\pi(a|s)$ by choosing actions greedily with respect to $Q(s, a)$:  \n",
    "     $$\\pi(s) = \\arg\\max_a Q(s, a)$$\n",
    "- **Key Insight**: Optimizing $Q(s, a)$ helps determine the best action for each state, enabling policy improvement.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. GLIE (Greedy in the Limit with Infinite Exploration) MC Control\n",
    "- **Goal**: To balance exploration and exploitation effectively.\n",
    "- **How it works**:\n",
    "  1. Initially, explore sufficiently to gather data about all state-action pairs.\n",
    "  2. Gradually reduce exploration (e.g., $\\varepsilon$-greedy policy with a decaying $\\varepsilon$) so that, eventually, actions are chosen greedily:\n",
    "     $$\\lim_{t \\to \\infty} \\varepsilon_t = 0$$\n",
    "  3. Ensure that every state-action pair $(s, a)$ is explored infinitely often while refining $Q(s, a)$ toward optimality.\n",
    "- **Advantages**: Ensures convergence to the optimal policy $\\pi^*$.\n",
    "- **Example**: Gradually decreasing random actions in a grid-world navigation task until only optimal moves are taken.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Importance Sampling\n",
    "- **Goal**: Enable off-policy learning, where data is collected using a behavior policy $b(a|s)$ but the target policy $\\pi(a|s)$ is improved.\n",
    "- **How it works**:\n",
    "  - Compute the return ($G_t$) generated by $b(a|s)$.\n",
    "  - Apply an importance sampling weight to account for the mismatch between $b$ and $\\pi$:\n",
    "    $$W = \\prod_{k=t}^T \\frac{\\pi(a_k|s_k)}{b(a_k|s_k)}$$\n",
    "  - Use $W \\cdot G_t$ to update $Q(s, a)$.\n",
    "- **Advantages**: Allows data generated from one policy to improve a different policy.\n",
    "- **Limitations**: Large differences between $b$ and $\\pi$ can lead to high variance.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Off-policy MC Control\n",
    "- **Goal**: Learn the optimal policy $\\pi^*$ using data generated from a different policy $b(a|s)$.\n",
    "- **How it works**:\n",
    "  - Collect episodes using the behavior policy $b(a|s)$, which might explore more than the target policy $\\pi(a|s)$.\n",
    "  - Correct for the difference between $b$ and $\\pi$ using **importance sampling** weights:\n",
    "    $$W = \\prod_{k=t}^T \\frac{\\pi(a_k|s_k)}{b(a_k|s_k)}$$\n",
    "  - Use $W \\cdot G_t$ to update $Q(s, a)$.\n",
    "- **Advantages**: Allows learning from exploratory policies while optimizing a greedy or deterministic target policy.\n",
    "- **Limitations**: High variance if $b(a|s)$ differs significantly from $\\pi(a|s)$.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. First Visit and Every Visit Variants\n",
    "- **First Visit**:\n",
    "  - Updates $Q(s, a)$ or $V(s)$ only the first time $(s, a)$ or $s$ is encountered in an episode.\n",
    "  - Reduces variance because each state or state-action pair contributes once per episode.\n",
    "- **Every Visit**:\n",
    "  - Updates $Q(s, a)$ or $V(s)$ every time $(s, a)$ or $s$ is encountered in an episode.\n",
    "  - Uses more data for each episode, potentially speeding up convergence but introducing higher variance.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Exploring Starts\n",
    "- **Goal**: Ensure that all state-action pairs are explored, preventing the algorithm from overlooking regions of the state space.\n",
    "- **How it works**:\n",
    "  - Initialize each episode at a random state $s$ and action $a$.\n",
    "  - Follow the current policy $\\pi(a|s)$ afterward.\n",
    "- **Advantages**: Simplifies exploration requirements.\n",
    "- **Limitations**: Impractical in environments where random starts are not feasible.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Table\n",
    "\n",
    "| Topic                    | Focus                            | Output        | Strengths                                  | Weaknesses                                 |\n",
    "|--------------------------|----------------------------------|---------------|-------------------------------------------|-------------------------------------------|\n",
    "| **MC Prediction**        | Policy evaluation               | $V(s)$      | Simple, no model required                 | Needs many episodes for accuracy          |\n",
    "| **MC Control**           | Policy optimization             | $Q(s, a)$   | Finds optimal policy                      | Needs sufficient exploration               |\n",
    "| **GLIE MC Control**      | Balanced exploration-exploitation| $Q(s, a)$   | Guarantees convergence to optimal policy  | Requires proper decay rate for $\\varepsilon$|\n",
    "| **Importance Sampling**  | Off-policy learning             | $Q(s, a)$   | Leverages off-policy data                 | High variance with large policy mismatch   |\n",
    "| **Off-policy MC Control**| Off-policy optimization         | $Q(s, a)$   | Leverages off-policy data                 | High variance with large policy mismatch   |\n",
    "| **First Visit**          | Update frequency                | $Q$ or $V$       | Low variance                              | Slower convergence                         |\n",
    "| **Every Visit**          | Update frequency                | $Q$ or $V$       | Faster convergence                        | Higher variance                            |\n",
    "| **Exploring Starts**     | Ensures exploration             | $Q(s, a)$   | Covers full state-action space            | Often impractical in real-world scenarios  |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
