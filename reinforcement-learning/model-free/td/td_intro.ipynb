{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference (TD) Algorithms: Detailed Notes\n",
    "\n",
    "### Roadmap: Temporal Difference (TD) Algorithms\n",
    "\n",
    "Here’s an overview of each algorithm to understand their role in reinforcement learning:\n",
    "\n",
    "1. **TD(0)**: A one-step Temporal Difference method that updates the value function using the immediate reward and next state’s value estimate.\n",
    "2. **On-policy SARSA**: Learns action values $Q(s, a)$ for a policy by following it; updates are made using the actual actions taken by the policy.\n",
    "3. **Q-learning**: An off-policy method that learns the optimal action-value function $Q^*(s, a)$, updating based on the maximum future $Q$-value, regardless of the policy followed.\n",
    "4. **Expected SARSA**: A variation of SARSA that updates using the expected $Q$-value over all actions, weighted by the policy’s action probabilities.\n",
    "5. **Replay Buffer and Off-policy Learning**: Enables efficient off-policy learning by storing and reusing past experiences to stabilize and enhance training.\n",
    "6. **Q-learning for Continuous State Spaces**: Extends Q-learning to work with continuous states using function approximators like neural networks.\n",
    "7. **n-step Returns**: Generalizes TD(0) by considering multi-step returns, balancing bias and variance in updates.\n",
    "8. **Eligibility Traces**: Maintains a record of visited states to assign credit for rewards to past states/actions, enabling faster learning.\n",
    "9. **TD($\\lambda$)**: Combines n-step returns and eligibility traces into a single framework for more comprehensive credit assignment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. TD(0)**\n",
    "\n",
    "- **Key Idea**: Update the value of a state after one step by bootstrapping (using the estimated value of the next state).\n",
    "- **Update Rule**:\n",
    "  $$\n",
    "  V(s) \\leftarrow V(s) + \\alpha \\left[ R_{t+1} + \\gamma V(s') - V(s) \\right]\n",
    "  $$\n",
    "- **Example**:\n",
    "  If you move from state $s$ to $s'$ with a reward $R_{t+1} = 5$ and a discount factor $\\gamma = 0.9$, TD(0) adjusts $V(s)$ toward $5 + 0.9 \\cdot V(s')$.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. On-policy SARSA**\n",
    "\n",
    "- **Key Idea**: Learn $Q(s, a)$ by following the current policy and updating $Q$-values based on the actions actually taken.\n",
    "- **Update Rule**:\n",
    "  $$\n",
    "  Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ R_{t+1} + \\gamma Q(s', a') - Q(s, a) \\right]\n",
    "  $$\n",
    "- **Example**:\n",
    "  If $a'$ is the next action chosen by the policy at state $s'$, $Q(s, a)$ adjusts using $Q(s', a')$ (not the maximum $Q$-value).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Q-learning**\n",
    "\n",
    "- **Key Idea**: Learn $Q^*(s, a)$ by updating $Q$-values based on the maximum expected reward for the next state.\n",
    "- **Update Rule**:\n",
    "  $$\n",
    "  Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ R_{t+1} + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
    "  $$\n",
    "- **Off-policy Nature**: The update depends on the optimal action (not necessarily the one taken), enabling learning independent of the behavior policy.\n",
    "- **Example**:\n",
    "  In a grid-world game, Q-learning can learn the shortest path even while exploring suboptimal actions.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Expected SARSA**\n",
    "\n",
    "- **Key Idea**: Similar to SARSA but updates $Q(s, a)$ using the expected $Q(s', a')$ weighted by the action probabilities of the policy.\n",
    "- **Update Rule**:\n",
    "  $$\n",
    "  Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ R_{t+1} + \\gamma \\sum_{a'} \\pi(a'|s') Q(s', a') - Q(s, a) \\right]\n",
    "  $$\n",
    "- **Advantage**: Reduces variance compared to SARSA by averaging over all actions instead of depending on one sampled action.\n",
    "- **Example**:\n",
    "  In stochastic environments, Expected SARSA provides smoother updates by considering multiple future outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Replay Buffer and Off-policy Learning**\n",
    "\n",
    "- **Key Idea**: Store past transitions $(s, a, R, s')$ in a replay buffer to reuse them for updates.\n",
    "- **How it works**:\n",
    "  1. Store transitions during episode execution.\n",
    "  2. Sample mini-batches of transitions randomly for training.\n",
    "  3. Update $Q(s, a)$ using these sampled experiences (off-policy updates).\n",
    "- **Advantages**:\n",
    "  - Breaks correlations in consecutive samples.\n",
    "  - Allows updates from diverse experiences.\n",
    "- **Common Application**: Deep Q-Learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Q-learning for Continuous State Spaces**\n",
    "\n",
    "- **Key Idea**: Use function approximators (e.g., neural networks) to generalize $Q(s, a)$ across continuous state spaces.\n",
    "- **Challenges**:\n",
    "  - Large state-action spaces.\n",
    "  - Stability of learning.\n",
    "- **Solution**:\n",
    "  - Use experience replay and target networks (in Deep Q-Learning).\n",
    "  - Approximate $Q(s, a; \\theta)$ as a parametric function.\n",
    "- **Example**: Training an autonomous car with continuous steering angles as actions.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. n-step Returns**\n",
    "\n",
    "- **Key Idea**: Combine benefits of Monte Carlo and TD(0) by updating states using cumulative rewards over $n$ steps.\n",
    "- **Update Rule**:\n",
    "  $$\n",
    "  G_t^{(n)} = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{n-1} R_{t+n} + \\gamma^n V(s_{t+n})\n",
    "  $$\n",
    "  Update:\n",
    "  $$\n",
    "  V(s_t) \\leftarrow V(s_t) + \\alpha \\left[ G_t^{(n)} - V(s_t) \\right]\n",
    "  $$\n",
    "- **Advantages**:\n",
    "  - Longer returns incorporate more future information.\n",
    "  - $n$-step return balances bias and variance.\n",
    "- **Example**:\n",
    "  Using $n = 3$, you update $V(s)$ based on rewards from the next three steps.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Eligibility Traces**\n",
    "\n",
    "- **Key Idea**: Maintain a \"trace\" for visited states and update their values based on this trace.\n",
    "- **How it works**:\n",
    "  - Each visited state accumulates an \"eligibility\" measure that decays over time.\n",
    "  - Updates are applied proportionally to eligibility traces.\n",
    "- **Update Rule** (for SARSA):\n",
    "  $$\n",
    "  e(s, a) = \\gamma \\lambda e(s, a) + 1 \\text{ (if } s, a \\text{ visited)}\n",
    "  $$\n",
    "  $$\n",
    "  Q(s, a) \\leftarrow Q(s, a) + \\alpha \\delta_t e(s, a)\n",
    "  $$\n",
    "  where $\\delta_t$ is the TD error.\n",
    "- **Advantages**: Faster learning by crediting all recent states for the rewards.\n",
    "- **Example**: Accelerating credit assignment in long episodes.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. TD($\\lambda$)**\n",
    "\n",
    "- **Key Idea**: Generalize n-step returns with a weighted average of all possible $n$-step returns using a parameter $\\lambda$.\n",
    "- **How it works**:\n",
    "  - Assign greater weight to recent rewards and decay weight for older ones.\n",
    "  - Update value using the weighted return:\n",
    "    $$\n",
    "    V(s) \\leftarrow V(s) + \\alpha \\sum_{n=1}^\\infty \\lambda^{n-1} \\left[ G_t^{(n)} - V(s) \\right]\n",
    "    $$\n",
    "- **Advantages**:\n",
    "  - Unifies Monte Carlo and TD learning.\n",
    "  - Fine-tunes bias-variance trade-off using $\\lambda$.\n",
    "- **Example**:\n",
    "  Setting $\\lambda = 0$ reduces TD($\\lambda$) to TD(0), while $\\lambda = 1$ makes it equivalent to Monte Carlo.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Table\n",
    "\n",
    "| **Algorithm**          | **Goal**                             | **Key Idea**                       | **Pros**                               | **Cons**                               |\n",
    "|-------------------------|---------------------------------------|-------------------------------------|----------------------------------------|----------------------------------------|\n",
    "| TD(0)                  | Policy evaluation                    | One-step bootstrapping             | Simple, computationally efficient      | May update slowly                      |\n",
    "| SARSA                  | On-policy control                    | Update based on policy’s actions   | Stable, tracks policy’s performance    | Sensitive to policy quality            |\n",
    "| Q-learning             | Off-policy control                   | Update using max future reward     | Finds optimal policy                   | May diverge without sufficient exploration |\n",
    "| Expected SARSA         | On-policy control                    | Update using expected rewards      | Reduces variance                       | More computation for expectations      |\n",
    "| Replay Buffer          | Improve stability for Q-learning     | Store and reuse transitions        | Breaks sample correlation              | Requires memory and additional logic   |\n",
    "| Continuous Q-learning  | Control in continuous spaces         | Function approximation for $Q$     | Scalable, flexible                     | Needs careful design of approximators  |\n",
    "| n-step Returns         | Extend TD to multiple steps          | Use multi-step rewards             | Balances bias and variance             | Selecting $n$ is tricky              |\n",
    "| Eligibility Traces     | Assign credit to recent states       | Decaying eligibility over time     | Speeds up learning                     | Adds complexity                        |\n",
    "| TD($\\lambda$)         | Combine Monte Carlo and TD learning  | Weighted average of returns        | General-purpose, tunable               | Sensitive to $\\lambda$ setting         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
