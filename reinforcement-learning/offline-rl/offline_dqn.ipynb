{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "034a447f",
   "metadata": {},
   "source": [
    "## Offline Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff7cfde",
   "metadata": {},
   "source": [
    "Resources used while writing this notebook:\n",
    "- [d3rlpy library](https://github.com/takuseno/d3rlpy/blob/master/d3rlpy/algos/qlearning/dqn.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba40f91",
   "metadata": {},
   "source": [
    "Check my [Kaggle notebook](https://www.kaggle.com/code/aryamanbansal/offline-dqn) for the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa185451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import random\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define transition tuple for experience replay\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class DQNNetwork(nn.Module):\n",
    "    \"\"\"Deep Q-Network neural network\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for storing transitions\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = Transition(state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"Sample a batch of transitions\"\"\"\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class OfflineDQN:\n",
    "    \"\"\"Offline Deep Q-Network implementation\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        lr: float = 1e-3,\n",
    "        gamma: float = 0.99,\n",
    "        hidden_dim: int = 256,\n",
    "        device: str = 'cpu'\n",
    "    ):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize Q-networks\n",
    "        self.q_network = DQNNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_network = DQNNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        \n",
    "        # Copy weights to target network\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.MSELoss()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update target network with current Q-network weights\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def train_step(self, batch: list) -> float:\n",
    "        \"\"\"Perform one training step\"\"\"\n",
    "        if len(batch) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Convert batch to tensors\n",
    "        states = torch.FloatTensor([t.state for t in batch]).to(self.device)\n",
    "        actions = torch.LongTensor([t.action for t in batch]).to(self.device)\n",
    "        rewards = torch.FloatTensor([t.reward for t in batch]).to(self.device)\n",
    "        next_states = torch.FloatTensor([t.next_state for t in batch]).to(self.device)\n",
    "        dones = torch.BoolTensor([t.done for t in batch]).to(self.device)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Next Q values from target network\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.criterion(current_q_values.squeeze(), target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def get_action(self, state, epsilon: float = 0.0):\n",
    "        \"\"\"Get action using epsilon-greedy policy\"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def train_offline(\n",
    "        self,\n",
    "        dataset: list,\n",
    "        batch_size: int = 64,\n",
    "        num_epochs: int = 100,\n",
    "        target_update_freq: int = 10\n",
    "    ):\n",
    "        \"\"\"Train the DQN on offline dataset\"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffle dataset\n",
    "            random.shuffle(dataset)\n",
    "            \n",
    "            epoch_losses = []\n",
    "            for i in range(0, len(dataset), batch_size):\n",
    "                batch = dataset[i:i + batch_size]\n",
    "                loss = self.train_step(batch)\n",
    "                epoch_losses.append(loss)\n",
    "            \n",
    "            # Update target network\n",
    "            if epoch % target_update_freq == 0:\n",
    "                self.update_target_network()\n",
    "            \n",
    "            avg_loss = np.mean(epoch_losses) if epoch_losses else 0\n",
    "            losses.append(avg_loss)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def save_model(self, path: str):\n",
    "        \"\"\"Save the trained model\"\"\"\n",
    "        torch.save({\n",
    "            'q_network_state_dict': self.q_network.state_dict(),\n",
    "            'target_network_state_dict': self.target_network.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, path)\n",
    "    \n",
    "    def load_model(self, path: str):\n",
    "        \"\"\"Load a trained model\"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6cfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "import d3rlpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load CartPole dataset from d3rlpy\n",
    "dataset, env = d3rlpy.datasets.get_cartpole()\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"State dimension: {dataset.get_observation_shape()[0]}\")\n",
    "print(f\"Action dimension: {dataset.get_action_size()}\")\n",
    "\n",
    "# Convert d3rlpy dataset to our format\n",
    "def convert_dataset_to_transitions(dataset):\n",
    "    transitions = []\n",
    "    for episode in dataset.episodes:\n",
    "        for i in range(len(episode.observations) - 1):\n",
    "            state = episode.observations[i]\n",
    "            action = episode.actions[i]\n",
    "            reward = episode.rewards[i]\n",
    "            next_state = episode.observations[i + 1]\n",
    "            done = (i == len(episode.observations) - 2)\n",
    "            transitions.append(Transition(state, action, reward, next_state, done))\n",
    "    return transitions\n",
    "\n",
    "offline_data = convert_dataset_to_transitions(dataset)\n",
    "print(f\"Converted {len(offline_data)} transitions\")\n",
    "\n",
    "# Split dataset for evaluation\n",
    "train_data, eval_data = train_test_split(offline_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize our Offline DQN\n",
    "state_dim = dataset.get_observation_shape()[0]\n",
    "action_dim = dataset.get_action_size()\n",
    "our_dqn = OfflineDQN(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "# Train our implementation\n",
    "print(\"Training our Offline DQN...\")\n",
    "our_losses = our_dqn.train_offline(\n",
    "    dataset=train_data,\n",
    "    batch_size=64,\n",
    "    num_epochs=100,\n",
    "    target_update_freq=10\n",
    ")\n",
    "\n",
    "# Initialize d3rlpy DQN\n",
    "print(\"\\nTraining d3rlpy DQN...\")\n",
    "d3rlpy_dqn = d3rlpy.algos.DQNConfig().create(device='cuda:0' if torch.cuda.is_available() else 'cpu:0')\n",
    "\n",
    "# Train d3rlpy implementation\n",
    "d3rlpy_dqn.fit(dataset, n_steps=10000, show_progress=True)\n",
    "\n",
    "# Evaluate both models using FQE\n",
    "print(\"\\nEvaluating models using FQE...\")\n",
    "\n",
    "# Create FQE evaluator\n",
    "fqe = d3rlpy.algos.FQEConfig().create(device='cuda:0' if torch.cuda.is_available() else 'cpu:0')\n",
    "\n",
    "# Evaluate d3rlpy DQN\n",
    "d3rlpy_value = fqe.fit(dataset, d3rlpy_dqn, n_steps=5000, show_progress=True)\n",
    "\n",
    "# For our implementation, we need to create a compatible policy\n",
    "class OurDQNPolicy:\n",
    "    def __init__(self, dqn_model):\n",
    "        self.dqn_model = dqn_model\n",
    "    \n",
    "    def predict(self, x):\n",
    "        actions = []\n",
    "        for state in x:\n",
    "            action = self.dqn_model.get_action(state, epsilon=0.0)\n",
    "            actions.append(action)\n",
    "        return np.array(actions)\n",
    "\n",
    "our_policy = OurDQNPolicy(our_dqn)\n",
    "\n",
    "# Create a mock algorithm object for FQE evaluation\n",
    "class MockAlgorithm:\n",
    "    def __init__(self, policy):\n",
    "        self.policy = policy\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.policy.predict(x)\n",
    "\n",
    "mock_algo = MockAlgorithm(our_policy)\n",
    "\n",
    "# Evaluate our implementation\n",
    "our_value = fqe.fit(dataset, mock_algo, n_steps=5000, show_progress=True)\n",
    "\n",
    "# Plot training losses for our implementation\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(our_losses)\n",
    "plt.title('Our Offline DQN Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(['Our DQN', 'd3rlpy DQN'], [our_value, d3rlpy_value])\n",
    "plt.title('FQE Evaluation Results')\n",
    "plt.ylabel('Estimated Value')\n",
    "plt.grid(True, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Our Offline DQN FQE Value: {our_value:.4f}\")\n",
    "print(f\"d3rlpy DQN FQE Value: {d3rlpy_value:.4f}\")\n",
    "print(f\"Difference: {abs(our_value - d3rlpy_value):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c0b0fc",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming you have the indices of patients with different actions\n",
    "# Replace 'different_action_indices' with your actual variable name containing the indices\n",
    "# and 'healthcare_data' with your actual dataframe name\n",
    "\n",
    "# If you don't have these variables yet, here's how you might create them:\n",
    "# different_action_indices = [list of patient indices where physician != RL agent actions]\n",
    "# healthcare_data = [your dataframe with 173 columns/features]\n",
    "\n",
    "# Create a large figure with subplots for all 173 features\n",
    "fig, axes = plt.subplots(nrows=14, ncols=13, figsize=(50, 40))  # 14*13 = 182 > 173\n",
    "axes = axes.flatten()  # Flatten to 1D array for easier indexing\n",
    "\n",
    "# Get the subset of data for patients with different actions\n",
    "patients_with_diff_actions = healthcare_data.iloc[different_action_indices]\n",
    "\n",
    "# Create scatterplots for each feature/column\n",
    "for i, column in enumerate(healthcare_data.columns):\n",
    "    if i < len(axes):  # Make sure we don't exceed available subplots\n",
    "        # Create scatter plot with patient index on x-axis and feature value on y-axis\n",
    "        axes[i].scatter(different_action_indices, \n",
    "                       patients_with_diff_actions[column], \n",
    "                       alpha=0.6, s=20)\n",
    "        axes[i].set_title(f'{column}', fontsize=8)\n",
    "        axes[i].set_xlabel('Patient Index', fontsize=6)\n",
    "        axes[i].set_ylabel('Feature Value', fontsize=6)\n",
    "        axes[i].tick_params(labelsize=5)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(len(healthcare_data.columns), len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.suptitle('Feature Distributions for Patients with Different Physician vs RL Agent Actions', \n",
    "             fontsize=16, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe4520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75650ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
