{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e76999",
   "metadata": {},
   "source": [
    "## Pareto Q-Learning with Deep Learning (or **Deep Pareto Q-Learning**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3634239b",
   "metadata": {},
   "source": [
    "This notebook contains an implementation of an **offline** & **continuous** version of the [Pareto Q-Learning](https://jmlr.org/papers/volume15/vanmoffaert14a/vanmoffaert14a.pdf) (PQL) algorithm, a multi-objective reinforcement learning algorithm. The implementation is inspired from the [MORL baselines PQL](https://github.com/LucasAlegre/morl-baselines/blob/main/morl_baselines/multi_policy/pareto_q_learning/pql.py) implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282b05a",
   "metadata": {},
   "source": [
    "### Pareto Q-Learning (PQL) with Deep Learning: Big Picture ðŸ–¼ï¸\n",
    "\n",
    "Imagine you're teaching a robot to make decisions where there isn't just one \"best\" outcome, but several good outcomes that represent different trade-offs (e.g., getting a task done quickly vs. using minimal energy). PQL helps the robot learn about these trade-offs.\n",
    "\n",
    "**Core Idea:** Instead of learning a single Q-value (a score for taking an action in a state), PQL learns a set of Q-value vectors for each state-action pair. Each vector in this set represents a possible long-term trade-off. For instance, if there are two objectives (like speed and energy), a Q-vector might be `[high_speed_score, low_energy_score]`. The goal is to find actions leading to \"non-dominated\" sets of these Q-vectors - meaning no other action offers a universally better set of trade-offs.\n",
    "\n",
    "**Adapting to Deep Learning (for continuous states):** Since we can't use tables for continuous states, we use a neural network as a function approximator. This network will predict Q-value vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316373ca",
   "metadata": {},
   "source": [
    "Following is a code skeleton of the PQL algorithm using deep learning:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# helper function\n",
    "def get_non_dominated():\n",
    "    pass\n",
    "\n",
    "# helper function\n",
    "def hypervolume():\n",
    "    pass\n",
    "\n",
    "\n",
    "# Neural Network class\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(...):\n",
    "        ...\n",
    "\n",
    "    def forward(...):\n",
    "        ...\n",
    "\n",
    "\n",
    "class DeepPQLAgent:\n",
    "    def __init__(self, network, num_actions, num_objectives, hyperparameters...):\n",
    "        # Initialize network, target network, optimizer, etc.\n",
    "        # Store hyperparameters (gamma, learning rate, target update strategy, etc.)\n",
    "        pass\n",
    "\n",
    "    def _get_q_vectors_from_network(self, network_to_use, state_tensor):\n",
    "        # Use the provided network (main or target) to predict Q-vectors for all actions given a state.\n",
    "        pass\n",
    "\n",
    "    def _calculate_target(self, rewards_batch, next_states_batch, dones_batch):\n",
    "        # This is the heart of PQL logic:\n",
    "        # 1. Use the target network to predict Q-vectors for all actions in `next_states_batch`.\n",
    "        # 2. For each next state in the batch, determine a set of \"good\" future Q-vectors\n",
    "        #    (eg: the non-dominated set among predicted Q-vectors for that next state).\n",
    "        # 3. From this set, select or construct the specific target Q-vector(s) for the Bellman update.\n",
    "        #    (eg: based on a strategy like \"max per objective,\" \"scalarized,\" \"closest to current prediction\").\n",
    "        # 4. Compute final target: R + gamma * (selected future Q-vector)\n",
    "        pass\n",
    "\n",
    "    def train_batch(self, states_batch, actions_batch, rewards_batch, next_states_batch, dones_batch):\n",
    "        # 1. Get current Q-vector predictions from the main network for (states_batch, actions_batch).\n",
    "        # 2. Calculate target Q-vectors using `_calculate_target`.\n",
    "        # 3. Compute loss (eg: MSE) between predicted and target Q-vectors.\n",
    "        # 4. Backpropagate and update the main network.\n",
    "        # 5. Periodically update the target network.\n",
    "        pass\n",
    "\n",
    "    def select_action(self, state, evaluation_strategy):\n",
    "        # 1. Use the main network to predict Q-vectors for all actions in the given state.\n",
    "        # 2. Based on the `evaluation_strategy` (eg: \"hypervolume,\" \"scalarized_greedy\"),\n",
    "        #    score each action.\n",
    "        # 3. Return the action with the best score.\n",
    "        pass\n",
    "\n",
    "\n",
    "# 4. Training Loop\n",
    "#    - Load your offline dataset (states, actions, rewards, next_states, dones).\n",
    "#    - Instantiate your network and the DeepPQLAgent.\n",
    "#    - For a number of epochs:\n",
    "#        - Sample batches from the dataset.\n",
    "#        - Call agent.train_batch(...)\n",
    "#        - Optionally, evaluate the agent's performance on a validation set.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ac5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import copy\n",
    "from typing import Callable, List, Optional, Set, Tuple, Dict, Union\n",
    "\n",
    "print(\"imports done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46cc92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def get_non_dominated(candidates):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7035676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def hypervolume(ref_point, points):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a62e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, state_dim: int, num_actions: int, num_objectives: int, hidden_size: int = 128):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.num_objectives = num_objectives\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_actions * num_objectives)\n",
    "        )\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        flat_q_values = self.network(state)\n",
    "        # Reshape to (batch_size, num_actions, num_objectives)\n",
    "        return flat_q_values.view(-1, self.num_actions, self.num_objectives)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f378a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepPQLAgent:\n",
    "    def __init__(self,\n",
    "                 network: nn.Module,\n",
    "                 num_actions: int,\n",
    "                 num_objectives: int,\n",
    "                 state_dim: int, # Added for clarity, though network might already know\n",
    "                 ref_point: np.ndarray, # For evaluation scoring\n",
    "                 gamma: float = 0.9,\n",
    "                 learning_rate: float = 1e-4,\n",
    "                 optimizer_type: str = \"adam\",\n",
    "                 target_network_update_rule: str = \"hard_copy\", # \"hard_copy\" or \"polyak\"\n",
    "                 target_network_update_freq: int = 100, # For \"hard_copy\"\n",
    "                 polyak_tau: float = 0.005, # For \"polyak\"\n",
    "                 target_q_calculation_strategy: str = \"dqn_max_per_objective\", # Default basic strategy\n",
    "                 loss_function_type: str = \"mse\",\n",
    "                 device: Optional[str] = None,\n",
    "                 seed: Optional[int] = None):\n",
    "\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self.device = torch.device(device)\n",
    "        # self.device = torch.device(device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        print(f\"DeepPQLAgent using device: {self.device}\")\n",
    "\n",
    "        self.network = network.to(self.device)\n",
    "        self.target_network = copy.deepcopy(network).to(self.device) # Create a target network\n",
    "        self.target_network.load_state_dict(self.network.state_dict()) # Initialize with same weights\n",
    "        self.target_network.eval() # Target network is only for inference, not direct training\n",
    "\n",
    "        # Store parameters\n",
    "        self.num_actions = num_actions\n",
    "        self.num_objectives = num_objectives\n",
    "        self.state_dim = state_dim\n",
    "        self.ref_point = np.array(ref_point, dtype=np.float32)\n",
    "        self.gamma = gamma\n",
    "        self.lr = learning_rate # Renamed to avoid conflict if passed to optimizer directly\n",
    "        self.target_update_rule = target_network_update_rule\n",
    "        self.target_update_freq = target_network_update_freq\n",
    "        self.polyak_tau = polyak_tau\n",
    "        self.target_q_strat = target_q_calculation_strategy\n",
    "        self.loss_type = loss_function_type\n",
    "\n",
    "        if optimizer_type.lower() == \"adam\":\n",
    "            self.optimizer = optim.Adam(self.network.parameters(), lr=self.lr)\n",
    "        elif optimizer_type.lower() == \"rmsprop\":\n",
    "            self.optimizer = optim.RMSprop(self.network.parameters(), lr=self.lr)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer type: {optimizer_type}\")\n",
    "\n",
    "        if self.loss_type.lower() == \"mse\":\n",
    "            self.loss_fn = nn.MSELoss()\n",
    "        elif self.loss_type.lower() == \"smooth_l1\":\n",
    "            self.loss_fn = nn.SmoothL1Loss()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss type: {self.loss_type}\")\n",
    "\n",
    "        self.global_step_count = 0\n",
    "        print(\"DeepPQLAgent initialized.\")\n",
    "\n",
    "\n",
    "    def _get_q_vectors_from_network(self, network_to_use: nn.Module, state_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Uses the provided network to predict Q-vectors for all actions given a state tensor.\n",
    "        Assumes the network's forward pass is: state_tensor -> all_action_q_vectors.\n",
    "        Input state_tensor: (batch_size, state_dim)\n",
    "        Output: (batch_size, num_actions, num_objectives)\n",
    "        \"\"\"\n",
    "        # Network is expected to handle the state_tensor directly and output appropriately.\n",
    "        # If network_input_type parameter was used (like in PQL_Offline_Deep), that logic would be here.\n",
    "        # For simplicity here, we assume the network itself is designed for state_only input.\n",
    "        with torch.inference_mode(): # Important when using target_network or just for inference\n",
    "            q_vectors_all_actions = network_to_use(state_tensor)\n",
    "        return q_vectors_all_actions\n",
    "    \n",
    "\n",
    "    def _calculate_target(self,\n",
    "                          rewards_batch: torch.Tensor,      # (batch_size, num_objectives)\n",
    "                          next_states_batch: torch.Tensor,  # (batch_size, state_dim)\n",
    "                          dones_batch: torch.Tensor,        # (batch_size, 1)\n",
    "                          # Optional: current_q_s_a_predictions for strategies like \"set_closest_from_nd\"\n",
    "                          current_q_s_a_predictions: Optional[torch.Tensor] = None\n",
    "                         ) -> torch.Tensor:\n",
    "        batch_size = rewards_batch.shape[0]\n",
    "        # Initialize target future Q-vectors with zeros (for terminal states)\n",
    "        target_future_q_vectors = torch.zeros_like(rewards_batch, device=self.device) # (batch_size, num_objectives)\n",
    "\n",
    "        # Identify non-terminal next states\n",
    "        non_terminal_mask = (dones_batch.squeeze(1) == 0) # In PQL_Offline_Deep, dones were float (0.0 for non-terminal)\n",
    "                                                        # If bool, use `~dones_batch.squeeze(1)`\n",
    "        \n",
    "        if torch.any(non_terminal_mask):\n",
    "            non_terminal_next_states = next_states_batch[non_terminal_mask]\n",
    "\n",
    "            # Get Q-vectors for all actions in non-terminal next_states using the TARGET NETWORK\n",
    "            # Output shape: (num_non_terminal, num_actions, num_objectives)\n",
    "            next_q_all_actions_target_nt = self._get_q_vectors_from_network(self.target_network, non_terminal_next_states)\n",
    "\n",
    "            if self.target_q_strat == \"dqn_max_per_objective\":\n",
    "                # For each objective k, find max_{a'} Q_k(s',a')\n",
    "                # This results in one target vector per non-terminal next state.\n",
    "                max_q_values_per_objective_nt, _ = torch.max(next_q_all_actions_target_nt, dim=1) # (num_non_terminal, num_objectives)\n",
    "                target_future_q_vectors[non_terminal_mask] = max_q_values_per_objective_nt\n",
    "            \n",
    "            # --- Other strategies would be implemented here ---\n",
    "            # Example for a conceptual \"scalarized_max\" (if preference_sampling_method was a class member)\n",
    "            # elif self.target_q_strat == \"scalarized_max\":\n",
    "            #     if self.preference_sampling_method is None:\n",
    "            #         raise ValueError(\"Preference sampling method needed for scalarized_max\")\n",
    "            #     temp_targets = torch.zeros_like(target_future_q_vectors[non_terminal_mask])\n",
    "            #     for i in range(non_terminal_next_states.shape[0]): # Loop over non-terminal samples\n",
    "            #         w_np = self.preference_sampling_method() # Get a weight vector\n",
    "            #         w = torch.tensor(w_np, dtype=torch.float32, device=self.device)\n",
    "            #         scalarized_q_next_for_sample = torch.sum(next_q_all_actions_target_nt[i] * w, dim=1) # (num_actions)\n",
    "            #         best_action_idx = torch.argmax(scalarized_q_next_for_sample)\n",
    "            #         temp_targets[i] = next_q_all_actions_target_nt[i, best_action_idx, :]\n",
    "            #     target_future_q_vectors[non_terminal_mask] = temp_targets\n",
    "\n",
    "            # Example for \"set_closest_from_nd\" (conceptual, requires current_q_s_a_predictions_nt)\n",
    "            # elif self.target_q_strat == \"set_closest_from_nd\":\n",
    "            #     if current_q_s_a_predictions is None:\n",
    "            #         raise ValueError(\"`current_q_s_a_predictions` needed for 'set_closest_from_nd'\")\n",
    "            #     current_q_s_a_preds_nt = current_q_s_a_predictions[non_terminal_mask]\n",
    "            #     temp_targets = torch.zeros_like(target_future_q_vectors[non_terminal_mask])\n",
    "            #     rewards_nt = rewards_batch[non_terminal_mask] # Need rewards for this specific calculation\n",
    "            #\n",
    "            #     for i in range(non_terminal_next_states.shape[0]):\n",
    "            #         q_vectors_s_prime_np = next_q_all_actions_target_nt[i].cpu().numpy() # (num_actions, num_objectives)\n",
    "            #         nd_set_future_q_tuples = get_non_dominated({tuple(qv) for qv in q_vectors_s_prime_np})\n",
    "            #\n",
    "            #         if not nd_set_future_q_tuples: # Fallback\n",
    "            #             temp_targets[i], _ = torch.max(next_q_all_actions_target_nt[i], dim=0) # Use dqn_max as fallback\n",
    "            #             continue\n",
    "            #\n",
    "            #         potential_full_targets = [\n",
    "            #             rewards_nt[i] + self.gamma * torch.tensor(v, dtype=torch.float32, device=self.device)\n",
    "            #             for v in nd_set_future_q_tuples\n",
    "            #         ]\n",
    "            #         current_q_pred_for_sample = current_q_s_a_preds_nt[i]\n",
    "            #\n",
    "            #         min_dist = float('inf')\n",
    "            #         chosen_full_target = potential_full_targets[0] # Default\n",
    "            #         for pt in potential_full_targets:\n",
    "            #             dist = torch.norm(current_q_pred_for_sample - pt, p=2)\n",
    "            #             if dist < min_dist:\n",
    "            #                 min_dist = dist\n",
    "            #                 chosen_full_target = pt\n",
    "            #         # We need to store the future part for the Bellman equation: (chosen_full_target - R) / gamma\n",
    "            #         temp_targets[i] = (chosen_full_target - rewards_nt[i]) / self.gamma\n",
    "            #\n",
    "            #     target_future_q_vectors[non_terminal_mask] = temp_targets\n",
    "\n",
    "            else:\n",
    "                # Fallback or error for unimplemented strategies\n",
    "                print(f\"Warning: Target Q strategy '{self.target_q_strat}' not fully implemented or recognized. Using dqn_max_per_objective as fallback.\")\n",
    "                max_q_values_per_objective_nt, _ = torch.max(next_q_all_actions_target_nt, dim=1)\n",
    "                target_future_q_vectors[non_terminal_mask] = max_q_values_per_objective_nt\n",
    "                \n",
    "        # Bellman equation: Target = R + gamma * Target_Future_Q (where future Q is 0 for terminal states)\n",
    "        # dones_batch is (batch_size, 1), 0 for non-terminal, 1 for terminal.\n",
    "        # So, (1.0 - dones_batch.squeeze(1)) is 1 for non-terminal, 0 for terminal.\n",
    "        # We need to unsqueeze it to (batch_size, 1) to multiply with (batch_size, num_objectives)\n",
    "        is_non_terminal_multiplier = (1.0 - dones_batch.squeeze(1)).unsqueeze(1)\n",
    "        final_target_q_batch = rewards_batch + (self.gamma * target_future_q_vectors * is_non_terminal_multiplier)\n",
    "        \n",
    "        return final_target_q_batch\n",
    "    \n",
    "\n",
    "    def train_batch(self,\n",
    "                    states_batch: torch.Tensor,         # (batch_size, state_dim)\n",
    "                    actions_batch: torch.Tensor,        # (batch_size, 1) holding action indices\n",
    "                    rewards_batch: torch.Tensor,        # (batch_size, num_objectives)\n",
    "                    next_states_batch: torch.Tensor,    # (batch_size, state_dim)\n",
    "                    dones_batch: torch.Tensor           # (batch_size, 1) bool or float (0 for non-terminal, 1 for terminal)\n",
    "                   ):\n",
    "        self.network.train() # Set network to training mode\n",
    "\n",
    "        # 1. Get current Q-vector predictions from the MAIN NETWORK\n",
    "        #    Output of _get_q_vectors_from_network: (batch_size, num_actions, num_objectives)\n",
    "        predicted_q_all_actions = self._get_q_vectors_from_network(self.network, states_batch)\n",
    "\n",
    "        # Select the Q-vector for the specific action taken in the batch\n",
    "        # actions_batch needs to be shaped correctly for gather: (batch_size, 1, 1) and expanded\n",
    "        # actions_batch (original): (batch_size, 1)\n",
    "        # Unsqueeze for gather: (batch_size, 1, 1)\n",
    "        # Expand to match num_objectives: (batch_size, 1, num_objectives)\n",
    "        action_indices_expanded = actions_batch.unsqueeze(-1).expand(-1, -1, self.num_objectives)\n",
    "        \n",
    "        # Q(s,a) for the actions taken\n",
    "        current_q_s_a_predictions = predicted_q_all_actions.gather(dim=1, index=action_indices_expanded).squeeze(1) # (batch_size, num_objectives)\n",
    "\n",
    "        # 2. Calculate target Q-vectors using `_calculate_target`\n",
    "        #    The target values should not have gradients flowing back through them.\n",
    "        with torch.inference_mode():\n",
    "            # Pass current_q_s_a_predictions if needed by the target strategy\n",
    "            target_q_values = self._calculate_target(rewards_batch, next_states_batch, dones_batch,\n",
    "                                                     current_q_s_a_predictions if self.target_q_strat == \"set_closest_from_nd\" else None)\n",
    "\n",
    "        # 3. Compute loss\n",
    "        loss = self.loss_fn(current_q_s_a_predictions, target_q_values) # target_q_values is already detached due to torch.no_grad()\n",
    "\n",
    "        # 4. Backpropagate and update the MAIN NETWORK\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Optional: Gradient Clipping\n",
    "        # torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.global_step_count += 1\n",
    "\n",
    "        # 5. Periodically update the TARGET NETWORK\n",
    "        if self.target_update_rule == \"hard_copy\" and self.global_step_count % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "            # print(f\"Step {self.global_step_count}: Target network updated (hard copy).\")\n",
    "        elif self.target_update_rule == \"polyak\":\n",
    "            for param, target_param in zip(self.network.parameters(), self.target_network.parameters()):\n",
    "                target_param.data.copy_(self.polyak_tau * param.data + (1.0 - self.polyak_tau) * target_param.data)\n",
    "            # No need to print polyak updates every time, they are frequent.\n",
    "\n",
    "        return loss.item()\n",
    "    \n",
    "    \n",
    "    def select_action(self, state_np: np.ndarray, evaluation_strategy: str = \"hypervolume\", eval_preference_vector: Optional[np.ndarray] = None) -> Tuple[int, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Selects an action for a given state during evaluation.\n",
    "        Input state_np: A single state as a NumPy array (state_dim,)\n",
    "        Returns: (selected_action_index, q_vector_for_selected_action)\n",
    "        \"\"\"\n",
    "        self.network.eval() # Set network to evaluation mode for consistent outputs\n",
    "        state_tensor = torch.tensor(state_np, dtype=torch.float32, device=self.device).unsqueeze(0) # Add batch dim: (1, state_dim)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            # Q-vectors for all actions: (1, num_actions, num_objectives)\n",
    "            q_vectors_all_actions_batch = self._get_q_vectors_from_network(self.network, state_tensor)\n",
    "            # Remove batch dim: (num_actions, num_objectives)\n",
    "            q_vectors_all_actions = q_vectors_all_actions_batch.squeeze(0)\n",
    "\n",
    "        # Convert to list of Python tuples of floats for helper functions\n",
    "        q_value_tuples_list = [tuple(qv.cpu().numpy()) for qv in q_vectors_all_actions]\n",
    "        \n",
    "        best_action_idx = 0 # Default action\n",
    "        \n",
    "        if evaluation_strategy == \"hypervolume\":\n",
    "            if not q_value_tuples_list: # Should not happen if num_actions > 0\n",
    "                return np.random.randint(self.num_actions), np.zeros(self.num_objectives)\n",
    "            \n",
    "            action_scores = np.array([hypervolume(self.ref_point, [vec]) if vec else -float('inf') for vec in q_value_tuples_list])\n",
    "            \n",
    "            if np.all(np.isinf(action_scores)) or np.all(np.isnan(action_scores)) or len(action_scores)==0:\n",
    "                best_action_idx = np.random.randint(self.num_actions) # Fallback\n",
    "            else:\n",
    "                best_action_idx = np.nanargmax(action_scores) # nanargmax handles NaNs by ignoring them\n",
    "\n",
    "        elif evaluation_strategy == \"random_from_nd\": # Get non-dominated set and pick randomly\n",
    "            if not q_value_tuples_list:\n",
    "                return np.random.randint(self.num_actions), np.zeros(self.num_objectives)\n",
    "            nd_set_tuples = get_non_dominated(set(q_value_tuples_list))\n",
    "            if not nd_set_tuples:\n",
    "                 best_action_idx = np.random.randint(self.num_actions) # Fallback\n",
    "            else:\n",
    "                nd_action_indices = [i for i, q_tuple in enumerate(q_value_tuples_list) if q_tuple in nd_set_tuples]\n",
    "                best_action_idx = np.random.choice(nd_action_indices) if nd_action_indices else np.random.randint(self.num_actions)\n",
    "\n",
    "        elif evaluation_strategy == \"scalarized_greedy\":\n",
    "            if eval_preference_vector is None:\n",
    "                raise ValueError(\"`eval_preference_vector` must be provided for 'scalarized_greedy' strategy.\")\n",
    "            w_eval = torch.tensor(eval_preference_vector, dtype=torch.float32, device=self.device)\n",
    "            # q_vectors_all_actions is (num_actions, num_objectives)\n",
    "            scalarized_q_values = torch.sum(q_vectors_all_actions * w_eval, dim=1) # (num_actions)\n",
    "            best_action_idx = torch.argmax(scalarized_q_values).item()\n",
    "        \n",
    "        # elif evaluation_strategy == \"custom\":\n",
    "            # best_action_idx = self._select_action_custom_eval_logic(...)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown evaluation_strategy: {evaluation_strategy}\")\n",
    "            \n",
    "        return best_action_idx, q_vectors_all_actions[best_action_idx].cpu().numpy()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c419299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9572cc0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36684fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a1bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepPQLAgent:\n",
    "    def __init__(self, num_states, num_actions, num_objectives, gamma=0.9):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.num_objectives = num_objectives\n",
    "        self.gamma = gamma      # Discount factor\n",
    "\n",
    "        # --- Core PQL Data Structures ---\n",
    "        # 1. Average immediate rewards: R_avg(s,a)\n",
    "        #    Stores a vector of rewards for each (s,a)\n",
    "        self.avg_immediate_rewards = [[np.zeros(num_objectives) for _ in range(num_actions)] for _ in range(num_states)]\n",
    "\n",
    "        # 2. Future value sets: ND_future(s,a)\n",
    "        #    Stores a set of non-dominated vectors from the next state,\n",
    "        #    after taking action 'a' in state 's'.\n",
    "        #    Each vector represents a possible future trade-off.\n",
    "        #    Initialize with a set containing one zero vector.\n",
    "        self.future_value_sets = [[{tuple(np.zeros(num_objectives))} for _ in range(num_actions)] for _ in range(num_states)]\n",
    "\n",
    "        # 3. Visit counts: N(s,a)\n",
    "        #    Counts how many times we've tried action 'a' in state 's'.\n",
    "        self.visit_counts = [[0 for _ in range(num_actions)] for _ in range(num_states)]\n",
    "\n",
    "    def get_q_set(self, state, action):\n",
    "        \"\"\"\n",
    "        Combines immediate reward with discounted future values\n",
    "        Returns a SET of Q-vectors for (state, action)\n",
    "        Q_set(s,a) = R_avg(s,a) + gamma * ND_future(s,a)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_best_future_values_from_state(self, state):\n",
    "        \"\"\"\n",
    "        Calculates V_ND(state) = ND ( Union_{all_actions_in_state} Q_set(state, action) )\n",
    "        Returns a SET of non-dominated Q-vectors achievable from 'state'\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update_from_experience(self, s, a, r_vector, s_prime, terminated):\n",
    "        \"\"\"\n",
    "        The main learning step. Updates the core data structures.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def select_action(self, state, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Chooses an action from 'state' using epsilon-greedy strategy\n",
    "        Needs a way to score the Q-sets for greedy selection.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def track_policy(self, target_q_vector, initial_state, env):\n",
    "        \"\"\"\n",
    "        Given a desired Q-vector (a specific trade-off),\n",
    "        tries to follow the policy that achieves it.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a3480e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e819c477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(6, -3, -1), (8, -2, -3), (10, -5, -3)}\n"
     ]
    }
   ],
   "source": [
    "def fn(all_vectors):\n",
    "    def dominates(vec1, vec2):\n",
    "        \"\"\"Check if vec1 Pareto dominates vec2\"\"\"\n",
    "        # vec1 dominates vec2 if:\n",
    "        # 1. vec1 is at least as good in all objectives\n",
    "        # 2. vec1 is strictly better in at least one objective\n",
    "        \n",
    "        at_least_as_good = all(v1 >= v2 for v1, v2 in zip(vec1, vec2))\n",
    "        strictly_better = any(v1 > v2 for v1, v2 in zip(vec1, vec2))\n",
    "        \n",
    "        return at_least_as_good and strictly_better\n",
    "    \n",
    "    non_dominated = set()\n",
    "    for vec1 in all_vectors:\n",
    "        is_dominated = False\n",
    "        for vec2 in all_vectors:\n",
    "            if vec1 != vec2 and dominates(vec2, vec1):\n",
    "                is_dominated = True\n",
    "                break\n",
    "        if not is_dominated:\n",
    "            non_dominated.add(vec1)\n",
    "    return non_dominated\n",
    "\n",
    "\n",
    "q_vecs = [\n",
    "    (10, -5, -3),\n",
    "    (8, -5, -3),\n",
    "    (8, -2, -3),\n",
    "    (6, -3, -1),\n",
    "]\n",
    "\n",
    "print(fn(q_vecs))  # Should print the non-dominated set from the provided vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5854597f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10695b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
