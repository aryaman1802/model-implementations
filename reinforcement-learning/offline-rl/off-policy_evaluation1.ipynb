{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c85cb0",
   "metadata": {},
   "source": [
    "## **Off-Policy Evaluation (OPE) Methods in Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d58ae9",
   "metadata": {},
   "source": [
    "References:\n",
    "- [A Review of Off-Policy Evaluation in Reinforcement Learning - Masatoshi Uehara](https://arxiv.org/pdf/2212.06355)\n",
    "- [discrete-off-policy-evaluation github implementation](https://github.com/theophilegervet/discrete-off-policy-evaluation)\n",
    "- [Ch-9: Importance Sampling - Art Owen](https://artowen.su.domains/mc/Ch-var-is.pdf). The entire book is [Monte Carlo theory, methods and examples - Art Owen](https://artowen.su.domains/mc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56090dca",
   "metadata": {},
   "source": [
    "In this notebook, we will implement 7 OPE methods:\n",
    "- **IS**: Importance Sampling\n",
    "- **WIS**: Weighted Importance Sampling\n",
    "- **PDIS**: Per-Decision Importance Sampling\n",
    "- **WPDIS**: Weighted Per-Decision Importance Sampling\n",
    "- **FQE**: Fitted Q-Evaluation\n",
    "- **DR**: Doubly Robust\n",
    "- **WDR**: Weighted Doubly Robust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a720d2c",
   "metadata": {},
   "source": [
    "### **Core Problem and Notation**\n",
    "\n",
    "**Goal:** We want to estimate the value (expected return) of a new evaluation policy ($\\pi_e$) using data collected by an old behavior policy ($\\pi_b$).\n",
    "\n",
    "This is crucial for real-world problems (like a recommender system, robotics, or healthcare) where testing a new, potentially bad policy ($\\pi_e$) live is too costly or dangerous.\n",
    "\n",
    "Key Notation:\n",
    "- **State** ($s$): The current situation or observation in the environment.\n",
    "- **Action** ($a$): The decision or move made by the agent.\n",
    "- **Reward** ($r$): The immediate feedback received after taking action $a$ in state $s$.\n",
    "- **Policy** ($\\pi$): A function $\\pi(a|s)$ that gives the probability of taking action $a$ in state $s$.\n",
    "- **Trajectory** ($\\tau$): A sequence of states, actions, and rewards from an episode: $\\displaystyle \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, ..., s_T, a_T, r_T)$.\n",
    "- **Discount Factor** ($\\gamma$): A number in $[0, 1)$ that reduces the importance of future rewards.\n",
    "- **Return** ($G(\\tau)$): The (discounted) sum of rewards for a trajectory: $\\displaystyle G(\\tau) = \\sum_{t=0}^T \\gamma^t r_t$. We often use $G_i$ for the return of trajectory $i$.\n",
    "- **Value** ($V(\\pi)$): The expected return of a policy: $\\displaystyle V(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}[G(\\tau)]$. This is what we want to estimate for $\\pi_e$.\n",
    "    - Note: $\\displaystyle \\mathbb{E}_{a \\sim b}[X]$ means the expected value of the random variable $X$, where the variable $a$ is sampled from the distribution $b$.\n",
    "- **Behavior Policy** ($\\pi_b$): The policy that generated the dataset.\n",
    "- **Evaluation (or Target) Policy** ($\\pi_e$): The new policy whose value we want to estimate.\n",
    "- **Dataset** ($\\mathcal{D}$): A collection of $N$ trajectories collected using $\\pi_b$: $\\displaystyle \\mathcal{D} = \\{\\tau_i\\}_{i=1}^N$.\n",
    "- **Importance Ratio** ($\\rho_t$): The ratio of probabilities for a single step: $\\displaystyle \\rho_t = \\frac{\\pi_e(a_t | s_t)}{\\pi_b(a_t | s_t)}$.\n",
    "- **Step-wise Ratio** ($\\rho_{0:t}$): The product of importance ratios up to time $t$: $\\displaystyle \\rho_{0:t} = \\prod_{k=0}^t \\rho_k$.\n",
    "- **Trajectory Ratio** ($\\rho_{0:T}$): The product of importance ratios for a full trajectory: $\\displaystyle \\rho_{0:T} = \\prod_{k=0}^T \\rho_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e532a746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "print(\"imports done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d1acb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f076585b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target policy and Behavior policy classes defined.\n"
     ]
    }
   ],
   "source": [
    "class BehaviorPolicy(nn.Module):\n",
    "    \"\"\"pi_b(a|s): A simple network to clone a policy pi(a|s).\"\"\"\n",
    "    def __init__(self, state_dim, num_actions):\n",
    "        super(BehaviorPolicy, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_actions)\n",
    "        )\n",
    "    def forward(self, state):\n",
    "        return self.net(state)  # return logits\n",
    "\n",
    "    def get_probs(self, state, return_log=False):\n",
    "        if return_log:\n",
    "            return torch.log_softmax(self.forward(state), dim=-1)\n",
    "        else:\n",
    "            return torch.softmax(self.forward(state), dim=-1)\n",
    " \n",
    "\n",
    "class TargetPolicy(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, device=\"cpu\", epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Initializes a DQN agent, which is the target policy.\n",
    "        \n",
    "        Args:\n",
    "            state_shape (tuple): Shape of the input state.\n",
    "            n_actions (int): Number of possible actions.\n",
    "            epsilon (float): Exploration rate for epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "        self.device = device\n",
    "        \n",
    "        # Define a simple feedforward neural network\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_shape[0], 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, states):\n",
    "        \"\"\"\n",
    "        Forward pass through the network to get Q-values for given states.\n",
    "        Args:\n",
    "            states (array-like): Batch of states.\n",
    "        Returns:\n",
    "            torch.Tensor: Q-values for each action.\n",
    "        \"\"\"\n",
    "        if isinstance(states, list):\n",
    "            states = torch.tensor(np.array(states), device=self.device, dtype=torch.float32)\n",
    "        if isinstance(states, np.ndarray):\n",
    "            states = torch.tensor(np.array(states), device=self.device, dtype=torch.float32)\n",
    "        elif isinstance(states, torch.Tensor):\n",
    "            states = states.to(self.device).float()   # float32\n",
    "        qvalues = self.network(states)\n",
    "        return qvalues\n",
    "\n",
    "    def get_action(self, states):\n",
    "        \"\"\"\n",
    "        Returns the action as per epsilon-greedy policy for a batch of states.\n",
    "        \n",
    "        Args:\n",
    "            states (array-like): Batch of states.\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Best actions.\n",
    "        \"\"\"\n",
    "        qvalues = self.forward(states)\n",
    "        random_numbers = np.random.rand()\n",
    "        batch_size = qvalues.shape[0]\n",
    "        if random_numbers < self.epsilon:\n",
    "            random_actions = np.random.choice(self.n_actions, size=batch_size)\n",
    "            return random_actions\n",
    "        else:\n",
    "            best_actions = qvalues.argmax(axis=-1)\n",
    "            return best_actions\n",
    "        \n",
    "    def get_probs(self, states, return_log=False):\n",
    "        if return_log:\n",
    "            return torch.log_softmax(self.forward(states), dim=-1)\n",
    "        else:\n",
    "            return torch.softmax(self.forward(states), dim=-1)\n",
    "\n",
    "print(\"Target policy and Behavior policy classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dff4bb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy dataset function defined.\n"
     ]
    }
   ],
   "source": [
    "def dummy_dataset():\n",
    "    # Dummy data creation\n",
    "    # batch_size=5, number of timesteps=20, number of features=49\n",
    "    # this means 5 trajectories of 20 timesteps each, with each timestep represented by a 49-dimensional feature vector\n",
    "    states = torch.rand((5,20,49))\n",
    "    next_states = torch.rand((5,20,49))\n",
    "    # corresponding rewards for each timestep in each trajectory\n",
    "    rewards = torch.rand((5,20,1))         \n",
    "    # corresponding done flags for each timestep in each trajectory\n",
    "    dones = torch.randint(0,2,(5,20,1))    \n",
    "    # corresponding actions taken at each timestep in each trajectory\n",
    "    actions = torch.randint(low=0, high=25, size=(5,20,1))\n",
    "    return states, next_states, rewards, dones, actions\n",
    "\n",
    "\n",
    "print(\"Dummy dataset function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f623b4",
   "metadata": {},
   "source": [
    "### **IS: Importance Sampling**\n",
    "\n",
    "**IS** is the most fundamental OPE method. It re-weights the returns from the behavior policy to look like they came from the evaluation policy.\n",
    "\n",
    "It's based on a simple identity:\n",
    "\n",
    "\n",
    "$$V(\\pi_e) = \\mathbb{E}_{\\tau \\sim \\pi_e}[G(\\tau)] = \\int P(\\tau | \\pi_e) G(\\tau) d\\tau$$\n",
    "\n",
    "\n",
    "We can multiply and divide by $P(\\tau | \\pi_b)$:\n",
    "\n",
    "\n",
    "$$V(\\pi_e) = \\int \\frac{P(\\tau | \\pi_e)}{P(\\tau | \\pi_b)} P(\\tau | \\pi_b) G(\\tau) d\\tau = \\mathbb{E}_{\\tau \\sim \\pi_b}\\left[\\frac{P(\\tau | \\pi_e)}{P(\\tau | \\pi_b)} G(\\tau)\\right]$$\n",
    "\n",
    "\n",
    "The ratio of trajectory probabilities is:\n",
    "\n",
    "\n",
    "$$\\frac{P(\\tau | \\pi_e)}{P(\\tau | \\pi_b)} = \\frac{\\prod_{t=0}^T \\pi_e(a_t | s_t) P(s_{t+1} | s_t, a_t)}{\\prod_{t=0}^T \\pi_b(a_t | s_t) P(s_{t+1} | s_t, a_t)} = \\prod_{t=0}^T \\frac{\\pi_e(a_t | s_t)}{\\pi_b(a_t | s_t)} = \\rho_{0:T}$$\n",
    "\n",
    "This gives the (Basic) Importance Sampling Estimator for $N$ trajectories:\n",
    "\n",
    "$$\\hat{V}_{IS} = \\frac{1}{N} \\sum_{i=1}^N \\rho_{0:T}^{(i)} G^{(i)}$$\n",
    "\n",
    "Log version of the estimator (to avoid numerical issues):\n",
    "\n",
    "let $z^{(i)} = \\log_e \\rho_{0:T}^{(i)} + \\log_e G^{(i)}$\n",
    "\n",
    "$$\\hat{V}_{IS} = \\exp(\\log_e \\hat{V}_{IS}) = \\exp\\left(z_{\\max} + \\log_e\\left(\\sum_{i=1}^N \\exp(z^{(i)} - z_{max})\\right) - \\log_e (N) \\right)$$\n",
    "\n",
    "where $z_{max} = \\max{\\{z^{(1)}, z^{(2)}, \\ldots, z^{(N)}\\}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8a820cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[25, 26, 27],\n",
      "        [28, 29, 30],\n",
      "        [31, 32, 33]])\n",
      "Idx1:\n",
      " tensor([[0, 1, 2],\n",
      "        [1, 2, 0]])\n",
      "Idx2:\n",
      " tensor([[0, 1],\n",
      "        [1, 2],\n",
      "        [2, 0]])\n",
      "Gathered1:\n",
      " tensor([[25, 26, 27],\n",
      "        [29, 30, 28]])\n",
      "Gathered2:\n",
      " tensor([[25, 29],\n",
      "        [28, 32],\n",
      "        [31, 26]])\n",
      "Gathered3:\n",
      " tensor([[25, 29],\n",
      "        [28, 32],\n",
      "        [31, 26]])\n",
      "Gathered4:\n",
      " tensor([[25, 26, 27],\n",
      "        [29, 30, 28]])\n"
     ]
    }
   ],
   "source": [
    "def fn():\n",
    "    inputs = torch.arange(25, 34).reshape(3,3)\n",
    "    print(\"Inputs:\\n\", inputs)\n",
    "    idx1 = torch.tensor([0,1,2,1,2,0]).reshape(2,3)\n",
    "    idx2 = torch.tensor([0,1,1,2,2,0]).reshape(3,2)\n",
    "    print(\"Idx1:\\n\", idx1)\n",
    "    print(\"Idx2:\\n\", idx2)\n",
    "    gathered1 = inputs.gather(1, idx1)\n",
    "    print(\"Gathered1:\\n\", gathered1)\n",
    "    gathered2 = inputs.gather(0, idx2)\n",
    "    print(\"Gathered2:\\n\", gathered2)\n",
    "    gathered3 = inputs.gather(-2, idx2)\n",
    "    print(\"Gathered3:\\n\", gathered3)\n",
    "    gathered4 = inputs.gather(-1, idx1)\n",
    "    print(\"Gathered4:\\n\", gathered4)\n",
    "\n",
    "\n",
    "fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "05f593da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi_b_probs.shape:\t\t torch.Size([5, 20, 25])\n",
      "pi_b_probs_actions.shape:\t torch.Size([5, 20, 1])\n",
      "pi_e_probs.shape:\t\t torch.Size([5, 20, 25])\n",
      "pi_e_probs_actions.shape:\t torch.Size([5, 20, 1])\n",
      "importance_ratio.shape:\t\t torch.Size([5, 20, 25])\n",
      "rewards.shape:\t\t\t torch.Size([5, 20, 1])\n",
      "prod.shape:\t\t\t torch.Size([5, 25, 1])\n",
      "IS Estimate of V(pi_e):\t\t 10.766754150390625\n"
     ]
    }
   ],
   "source": [
    "def is_estimate(states, actions, rewards, pi_b, pi_e, return_ir_sum=False):\n",
    "    \"\"\"\n",
    "    Returns the importance sampling ratio between target policy and behavior policy.\n",
    "    \"\"\"\n",
    "    pi_b_probs = pi_b.get_probs(states)\n",
    "    pi_b_probs_actions = pi_b_probs.gather(2, actions)     # \\pi_b(a_t | s_t)\n",
    "    print(\"pi_b_probs.shape:\\t\\t\", pi_b_probs.shape)\n",
    "    print(\"pi_b_probs_actions.shape:\\t\", pi_b_probs_actions.shape)\n",
    "    pi_e_probs = pi_e.get_probs(states)\n",
    "    pi_e_probs_actions = pi_e_probs.gather(2, actions)     # \\pi_e(a_t | s_t)\n",
    "    print(\"pi_e_probs.shape:\\t\\t\", pi_e_probs.shape)\n",
    "    print(\"pi_e_probs_actions.shape:\\t\", pi_e_probs_actions.shape)\n",
    "    importance_ratio = pi_e_probs / pi_b_probs             # \\rho_{0:T}\n",
    "    print(\"importance_ratio.shape:\\t\\t\", importance_ratio.shape)\n",
    "    print(\"rewards.shape:\\t\\t\\t\", rewards.shape)\n",
    "    prod = torch.matmul(importance_ratio.transpose(1,2), rewards)          # \\rho_{0:T}^{(i)} G^{(i)}\n",
    "    print(\"prod.shape:\\t\\t\\t\", prod.shape)\n",
    "    v_is = prod.mean().item()                              # \\hat{V}_{IS}\n",
    "    # print(\"importance_ratio.sum(dim=0).shape:\\t\", importance_ratio.sum(dim=0).shape)\n",
    "    # print(\"importance_ratio.sum(dim=1).shape:\\t\", importance_ratio.sum(dim=1).shape)\n",
    "    # print(\"importance_ratio.sum(dim=2).shape:\\t\", importance_ratio.sum(dim=2).shape)\n",
    "    if return_ir_sum:\n",
    "        # return both IS estimate and sum of importance ratios, which is needed for WIS\n",
    "        return v_is, importance_ratio.sum(dim=1)\n",
    "    else:\n",
    "        return v_is\n",
    "\n",
    "\n",
    "\n",
    "def fn():\n",
    "    states, _, rewards, dones, actions = dummy_dataset()\n",
    "    state_dim = states.shape[2]\n",
    "    action_dim = 25\n",
    "    pi_b = BehaviorPolicy(state_dim=state_dim, num_actions=action_dim)  # behavior policy (assumed it's trained)\n",
    "    pi_e = TargetPolicy(state_shape=(state_dim,), n_actions=action_dim)     # target policy (assumed it's trained)\n",
    "    v_is = is_estimate(states, actions, rewards, pi_b, pi_e)\n",
    "    print(\"IS Estimate of V(pi_e):\\t\\t\", v_is)\n",
    "\n",
    "\n",
    "fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "49bb08d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_estimate_logversion():\n",
    "#     \"\"\"\n",
    "#     Returns the importance sampling ratio between target policy and behavior policy using log probabilities.\n",
    "#     \"\"\"\n",
    "#     states, rewards, dones, actions = dummy_dataset()\n",
    "#     state_dim = states.shape[2]\n",
    "#     action_dim = 25\n",
    "#     pi_b = BehaviorPolicy(state_dim=state_dim, num_actions=action_dim)  # behavior policy (assumed it's trained)\n",
    "#     pi_b_log_probs = pi_b.get_probs(states, return_log=True)\n",
    "#     pi_b_log_probs_actions = pi_b_log_probs.gather(2, actions)\n",
    "#     print(pi_b_log_probs.shape, pi_b_log_probs_actions.shape)\n",
    "#     pi_e = DQNAgent(state_shape=(state_dim,), n_actions=action_dim)     # target policy (assumed it's trained)\n",
    "#     pi_e_log_probs = pi_e.get_probs(states, return_log=True)\n",
    "#     pi_e_log_probs_actions = pi_e_log_probs.gather(2, actions)\n",
    "#     print(pi_e_log_probs.shape, pi_e_log_probs_actions.shape)\n",
    "#     log_importance_ratio = pi_e_log_probs - pi_b_log_probs\n",
    "#     print(log_importance_ratio.shape)\n",
    "#     importance_ratio = torch.exp(log_importance_ratio)\n",
    "#     print(importance_ratio.shape)\n",
    "#     print(rewards.shape)\n",
    "#     prod = torch.matmul(importance_ratio.transpose(1,2), rewards)\n",
    "#     print(prod.shape)\n",
    "#     v_is = prod.mean().item()\n",
    "#     print(\"IS Estimate of V(pi_e) using log probs: \", v_is)\n",
    "\n",
    "\n",
    "# is_estimate_logversion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e3bdbe",
   "metadata": {},
   "source": [
    "### **WIS: Weighted Importance Sampling**\n",
    "\n",
    "WIS is a modification of IS that normalizes the weighted returns. This dramatically reduces variance, but introduces a small amount of bias.\n",
    "\n",
    "Instead of dividing by $N$, WIS divides by the sum of the importance weights:\n",
    "\n",
    "$$\\hat{V}_{WIS} = \\frac{\\sum_{i=1}^N \\rho_{0:T}^{(i)} G^{(i)}}{\\sum_{i=1}^N \\rho_{0:T}^{(i)}}$$\n",
    "\n",
    "You can think of this as a weighted average, where the weight for trajectory $i$ is $\\displaystyle w_i = \\frac{\\rho_{0:T}^{(i)}}{\\sum_{j=1}^N \\rho_{0:T}^{(j)}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "db1d1844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi_b_probs.shape:\t\t torch.Size([5, 20, 25])\n",
      "pi_b_probs_actions.shape:\t torch.Size([5, 20, 1])\n",
      "pi_e_probs.shape:\t\t torch.Size([5, 20, 25])\n",
      "pi_e_probs_actions.shape:\t torch.Size([5, 20, 1])\n",
      "importance_ratio.shape:\t\t torch.Size([5, 20, 25])\n",
      "rewards.shape:\t\t\t torch.Size([5, 20, 1])\n",
      "prod.shape:\t\t\t torch.Size([5, 25, 1])\n",
      "WIS Estimate of V(pi_e):\t 0.5270333836347529\n"
     ]
    }
   ],
   "source": [
    "def wis_estimate(states, actions, rewards, pi_b, pi_e):\n",
    "    \"\"\"\n",
    "    Returns the weighted importance sampling ratio between target policy and behavior policy.\n",
    "    \"\"\"\n",
    "    v_is, ir_sum = is_estimate(states, actions, rewards, pi_b, pi_e, return_ir_sum=True)\n",
    "    v_wis = v_is / ir_sum.mean().item()\n",
    "    return v_wis\n",
    "\n",
    "\n",
    "def fn():\n",
    "    states, _, rewards, dones, actions = dummy_dataset()\n",
    "    state_dim = states.shape[2]\n",
    "    action_dim = 25\n",
    "    pi_b = BehaviorPolicy(state_dim=state_dim, num_actions=action_dim)  # behavior policy (assumed it's trained)\n",
    "    pi_e = TargetPolicy(state_shape=(state_dim,), n_actions=action_dim)     # target policy (assumed it's trained)\n",
    "    v_wis = wis_estimate(states, actions, rewards, pi_b, pi_e)\n",
    "    print(\"WIS Estimate of V(pi_e):\\t\", v_wis)\n",
    "\n",
    "fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee93cb55",
   "metadata": {},
   "source": [
    "### **PDIS: Per-Decision Importance Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b616990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15683523",
   "metadata": {},
   "source": [
    "### **WPDIS: Weighted Per-Decision Importance Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb9e12c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05a6b6c5",
   "metadata": {},
   "source": [
    "### **FQE: Fitted Q-Evaluation**\n",
    "\n",
    "FQE abandons importance sampling. Instead, it tries to learn the Q-function of the target policy, $Q^{\\pi_e}(s, a)$, directly from the off-policy data.\n",
    "\n",
    "FQE uses a batch RL algorithm, similar to Fitted Q-Iteration.\n",
    "\n",
    "1. Initialize: Start with an arbitrary Q-function, $Q_0(s, a) = 0$.\n",
    "\n",
    "2. Iterate ($k=1...K$):\n",
    "\n",
    "- For every transition $(s_i, a_i, r_i, s'_i)$ in the dataset $\\mathcal{D}$, create a target value $y_i$:\n",
    "\n",
    "\n",
    "$$y_i = r_i + \\gamma \\mathbb{E}_{a' \\sim \\pi_e(a' | s'_i)}[Q_{k-1}(s'_i, a')]$$\n",
    "\n",
    "- This expectation is just a sum: $y_i = r_i + \\gamma \\sum_{a'} \\pi_e(a' | s'_i) Q_{k-1}(s'_i, a')$\n",
    "\n",
    "- Train a function approximator (eg: a neural network) $Q_k$ to solve the regression problem: $Q_k \\approx \\text{argmin}_Q \\sum_i (Q(s_i, a_i) - y_i)^2$.\n",
    "\n",
    "3. Estimate Value: After $K$ iterations, the final $Q_K$ approximates $Q^{\\pi_e}$. The policy value is the expected Q-value from the start states $s_0$:\n",
    "\n",
    "\n",
    "$$\\hat{V}_{FQE} = \\mathbb{E}_{s_0 \\sim \\mathcal{D}, a_0 \\sim \\pi_e(a_0|s_0)}[Q_K(s_0, a_0)]$$\n",
    "\n",
    "$$\\hat{V}_{FQE} = \\frac{1}{N} \\sum_{i=1}^N \\left( \\sum_{a_0} \\pi_e(a_0 | s_0^{(i)}) Q_K(s_0^{(i)}, a_0) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f695b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNet class defined.\n"
     ]
    }
   ],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, state_dim, num_actions):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "print(\"QNet class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d23f93a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states.shape:\t\t torch.Size([5, 20, 49])\n",
      "next_states.shape:\t torch.Size([5, 20, 49])\n",
      "actions.shape:\t\t torch.Size([5, 20, 1])\n",
      "rewards.shape:\t\t torch.Size([5, 20, 1])\n",
      "dones.shape:\t\t torch.Size([5, 20, 1])\n",
      "pi_e(next_states).shape:\t torch.Size([5, 20, 25])\n",
      "q_net(next_states).shape:\t torch.Size([5, 20, 25])\n",
      "expected_value.shape:\t\t torch.Size([5, 20, 1])\n"
     ]
    }
   ],
   "source": [
    "def fitted_q_evaluation(K, states, next_states, actions, rewards, dones, pi_e, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Fitted Q-evaluation using the given dataset and policy.\n",
    "\n",
    "    Args:\n",
    "        - K (int): Number of iterations.\n",
    "        - states (torch.Tensor): Batch of states.\n",
    "        - next_states (torch.Tensor): Batch of next states.\n",
    "        - actions (torch.Tensor): Batch of actions taken.\n",
    "        - rewards (torch.Tensor): Batch of rewards received.\n",
    "        - dones (torch.Tensor): Batch of done flags.\n",
    "        - pi_e (DQNAgent): Target policy.\n",
    "        - gamma (float): Discount factor.\n",
    "\n",
    "    Returns:\n",
    "        - q_net (QNet): Trained Q-network.\n",
    "    \"\"\"\n",
    "    qnet = QNet(state_dim=states.shape[2], num_actions=25)\n",
    "    for k in range(K):\n",
    "        # Update Q-values using the Bellman equation\n",
    "        with torch.no_grad():\n",
    "            expected_value = torch.sum(pi_e.get_probs(next_states) * qnet(next_states), dim=-1, keepdim=True)\n",
    "            y = rewards + gamma * (1 - dones) * expected_value   # target Q-values\n",
    "            qnet.update(states, actions, y)\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def fn():\n",
    "    states, next_states, rewards, dones, actions = dummy_dataset()\n",
    "    state_dim = states.shape[2]\n",
    "    action_dim = 25\n",
    "    print(\"states.shape:\\t\\t\", states.shape)\n",
    "    print(\"next_states.shape:\\t\", next_states.shape)\n",
    "    print(\"actions.shape:\\t\\t\", actions.shape)\n",
    "    print(\"rewards.shape:\\t\\t\", rewards.shape)\n",
    "    print(\"dones.shape:\\t\\t\", dones.shape)\n",
    "    pi_e = TargetPolicy(state_shape=(state_dim,), n_actions=action_dim)     # target policy (assumed it's trained)\n",
    "    print(\"pi_e(next_states).shape:\\t\", pi_e.get_probs(next_states).shape)\n",
    "    q_net = QNet(state_dim, action_dim)\n",
    "    print(\"q_net(next_states).shape:\\t\", q_net(next_states).shape)\n",
    "    expected_value = torch.sum(pi_e.get_probs(next_states) * q_net(next_states), dim=-1, keepdim=True)\n",
    "    print(\"expected_value.shape:\\t\\t\", expected_value.shape)\n",
    "\n",
    "fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74144b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "928da5bf",
   "metadata": {},
   "source": [
    "### **DR: Doubly Robust**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bea788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a550048",
   "metadata": {},
   "source": [
    "### **WDR: Weighted Doubly Robust**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242305b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f91ac0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
