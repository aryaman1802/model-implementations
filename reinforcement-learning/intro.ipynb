{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Super-Condensed Intro to RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Terminology and the Agent-Environment Framework\n",
    "\n",
    "At each time step, the agent observes the current **state** ($s$) of the environment, selects an **action** ($a$) based on its policy ($\\pi$), and receives a **reward** ($r$) and a new **state** ($s'$) from the environment. Note: it is actually the agent's state in the environment, ie, the environment is the entire world in which the agent lives, and the state is just one place in the environment where the agent is currently located. The agent-environment is formally known as a **Markov Decision Process (MDP)**.\n",
    "\n",
    "1. **Agent**: The learner or decision-maker.\n",
    "2. **Environment**: Everything the agent interacts with.\n",
    "3. **State ($S$)**: A representation of the environment at a specific time.\n",
    "4. **Action ($A$)**: A decision made by the agent.\n",
    "5. **Reward ($R$)**: A scalar feedback signal from the environment to the agent.\n",
    "6. **Episode**: A sequence of states, actions, and rewards that ends in a terminal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy\n",
    "\n",
    "- **Definition**: A policy defines the agent's behavior at a given time, ie, it tells the agent what action to take given the current state.\n",
    "- **Types of Policies**:\n",
    "  - **Deterministic**: $\\pi(s) = a$, where the action $a$ is fixed for a given state $s$.\n",
    "  - **Stochastic**: $\\pi(a|s) = P(a|s)$, where the policy outputs probabilities of actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Types\n",
    "\n",
    "1. **Episodic (Terminal)**:\n",
    "   - The interaction ends after a finite sequence of steps (episodes).\n",
    "   - Example: Games with a defined end, like Super Mario Bros.\n",
    "2. **Continuous**:\n",
    "   - The interaction does not have a defined endpoint.\n",
    "   - Example: Autonomous driving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration vs. Exploitation Tradeoff\n",
    "\n",
    "- **Exploration**: Trying new actions to discover their effects. This helps in discovering new actions with the hope that they yield better reward value.\n",
    "- **Exploitation**: Using known actions to maximize rewards. \n",
    "- **Challenge**: Balancing exploration and exploitation to ensure long-term success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discount Factor ($\\gamma$)\n",
    "\n",
    "- The discount factor is used to determine the importance of future rewards, where its domain is: 0 \\leq \\gamma \\leq 1.\n",
    "- The discount factor determines how future rewards are weighted relative to immediate rewards:\n",
    "  - $\\gamma = 0$: The agent is **short-sighted** and only considers immediate rewards.\n",
    "  - $\\gamma = 1$: The agent is **far-sighted** and values long-term rewards equally with immediate rewards.\n",
    "- In practice, $0 < \\gamma < 1$ balances these extremes by emphasizing both immediate and future rewards.\n",
    "- The discount factor is used to calculate the discounted reward: $(G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward Hypothesis\n",
    "\n",
    "All goals can be expressed as the maximization of the cumulative discounted reward. Mathematically, this can be written as:\n",
    "$$G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transition Dynamics\n",
    "\n",
    "- **Definition**: The probabilistic behavior of the environment.\n",
    "- **Transition Function**: $P(s', r | s, a)$, the probability of moving to state $s'$ and receiving reward $r$ given state $s$ and action $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taxonomy of RL Methods\n",
    "\n",
    "Reinforcement Learning methods can be categorized into the following groups:\n",
    "\n",
    "1. **Model-Free vs. Model-Based**:\n",
    "   - **Model-Free**: Learn directly from environment interactions without a model (e.g., Q-Learning).\n",
    "   - **Model-Based**: Build a model of the environment's dynamics for planning (e.g., Dyna-Q).\n",
    "2. **Value-Based vs. Policy-Based**:\n",
    "   - **Value-Based**: Learn value functions (e.g., Q-Learning).\n",
    "   - **Policy-Based**: Learn policies directly (e.g., Policy Gradient).\n",
    "   - **Actor-Critic**: Combines value-based and policy-based approaches.\n",
    "3. **On-Policy vs. Off-Policy**:\n",
    "   - **On-Policy**: Learn from actions taken by the current policy (e.g., SARSA).\n",
    "   - **Off-Policy**: Learn from actions taken by a different policy (e.g., Q-Learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value of a State and an Action\n",
    "\n",
    "- **Value of a State** $V(s)$: Expected return starting from state $s$ and following policy $\\pi$:\n",
    "\n",
    "$$V_\\pi(s) = \\mathbb{E}_\\pi \\left[ G_t \\middle| S_t = s \\right]$$\n",
    "\n",
    "- **Value of an Action** $Q(s, a)$: Expected return starting from state $s$, taking action $a$, and following policy $\\pi$: \n",
    "\n",
    "$$Q_\\pi(s, a) = \\mathbb{E}_\\pi \\left[ G_t \\; \\middle| \\; S_t = s, A_t = a \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Functions\n",
    "\n",
    "1. **State-Value Function $V(s)$**: Value of being in state $s$.\n",
    "2. **Action-Value Function $Q(s, a)$**: Value of taking action $a$ in state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Equations\n",
    "\n",
    "The Bellman equations provide a recursive way to compute value functions by breaking down the expected return into immediate rewards and the value of subsequent states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boxed{V_\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s', r} P(s', r|s, a) \\left[ r + \\gamma V_\\pi(s') \\right]} \\quad \\rightarrow \\quad \\text{State-Value Function}$$\n",
    "\n",
    "\n",
    "$$\\boxed{Q_\\pi(s, a) = \\sum_{s', r} P(s', r|s, a) \\left[ r + \\gamma \\sum_{a'} \\pi(a'|s') Q_\\pi(s', a') \\right]} \\quad \\rightarrow \\quad \\text{Action-Value Function}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: $$\\sum_{s', r} = \\sum_{s'} \\sum_r$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivation of the State-Value Bellman Equation\n",
    "\n",
    "\\begin{align*}\n",
    "V_\\pi(s) &= \\mathbb{E}_\\pi \\left[ G_t \\middle| S_t = s \\right] \\\\ \\\\\n",
    "\\because \\; G_t &= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\ \\\\\n",
    "V_\\pi(s) &= \\mathbb{E}_\\pi \\left[ R_{t+1} \\, + \\, \\gamma G_{t+1} \\; \\middle| \\; S_t = s \\right] \\\\ \\\\\n",
    "&= \\mathbb{E}_\\pi \\left[ R_{t+1} \\, + \\, \\gamma V_\\pi(s_{t+1}) \\; \\middle| \\; S_t = s \\right] \\\\ \\\\\n",
    "&= \\sum_{a} \\pi(a|s) \\sum_{s'} \\sum_r P(s', r|s, a) \\left[ r + \\gamma V_\\pi(s') \\right] \\qquad \\blacksquare\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivation of the Action-Value Bellman Equation\n",
    "\n",
    "\\begin{align*}\n",
    "Q_\\pi(s,a) &= \\mathbb{E}_\\pi \\left[ G_t \\; \\middle| \\; S_t = s, A_t = a \\right] \\\\ \\\\\n",
    "&= \\sum_{s'} \\sum_r P(s', r|s, a) \\left[ r + \\gamma \\mathbb{E}_\\pi \\left[ G_{t+1} \\; \\middle| \\; S_{t+1} = s' \\right] \\right] \\\\ \\\\\n",
    "&= \\sum_{s'} \\sum_r P(s', r|s, a) \\left[ r + \\gamma \\sum_{a'} \\pi(a' | s') \\mathbb{E}_\\pi \\left[ G_{t+1} \\; \\middle| \\; S_{t+1} = s', A_{t+1} = a' \\right] \\right] \\\\ \\\\\n",
    "&= \\sum_{s'} \\sum_r P(s', r|s, a) \\left[ r + \\gamma \\sum_{a'} \\pi(a'|s') Q_\\pi(s', a') \\right] \\qquad \\blacksquare\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimality Bellman Equations\n",
    "\n",
    "The optimality Bellman equations define the value of a state or action under an optimal policy, which maximizes the expected return. They are necessary for:\n",
    "1. Finding the optimal policy in RL problems.\n",
    "2. Deriving algorithms like Q-Learning and Value Iteration.\n",
    "\n",
    "$$\\boxed{V_*(s) = \\max_a \\sum_{s', r} P(s', r|s, a) \\left[ r + \\gamma V_*(s') \\right]} \\quad \\rightarrow \\quad \\text{Optimal State-Value Function}$$\n",
    "\n",
    "$$\\boxed{Q_*(s, a) = \\sum_{s', r} P(s', r|s, a) \\left[ r + \\gamma \\max_{a'} Q_*(s', a') \\right]} \\quad \\rightarrow \\quad \\text{Optimal Action-Value Function}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
