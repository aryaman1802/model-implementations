{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code (and other stuff like images) is heavily inspired and taken from the following sources:\n",
    "- [Attention is all you need - Vaswani, et al](https://arxiv.org/pdf/1706.03762)\n",
    "- [Harvard Transformer Implementation](https://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "- [Umar Jamil Transformer Implementation](https://github.com/hkproj/pytorch-transformer)\n",
    "- [Umar Jamil Transformer video](https://youtu.be/ISNdQcPhsts?si=_1mO7CBcvFHg15cJ)\n",
    "- [Datacamp transformer tutorial](https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook is used for trying out and finalizing the code, after which the relevant code is put in the corresponding python files, so that the model can be run from the terminal.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code skeleton:**\n",
    "- input and output embedding\n",
    "- positional encoding\n",
    "- encoder\n",
    "    - multi-head attention\n",
    "    - feed forward network\n",
    "    - layer normalization and residual connections\n",
    "- decoder\n",
    "    - masked multi-head attention\n",
    "    - multi-head attention\n",
    "    - feed forward network\n",
    "    - layer normalization and residual connections\n",
    "- final linear layer and softmax (head)\n",
    "- full transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Output Embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        # you can also do this:\n",
    "        # super().__init__()\n",
    "        self.d_model = d_model  # in this paper, it 512\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n",
    "        # check the last line on page 5: \n",
    "        # \"In the embedding layers, we multiply those weights by d model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputEmbedding(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "This is the one mentioned in the [Attention is all you need](https://arxiv.org/pdf/1706.03762) paper.\n",
    "- $\\displaystyle PE_{(pos, 2i)} = sin \\left( \\frac{pos}{10000^{\\frac{2i}{\\text{d\\_model}}}} \\right)$ \n",
    "    \n",
    "- &nbsp;  $\\displaystyle PE_{(pos, 2i+1)} = cos \\left( \\frac{pos}{10000^{\\frac{2i}{\\text{d\\_model}}}} \\right)$\n",
    "\n",
    "To make the above 2 formulae more numerically stable, we apply logarithms and then raise them to exponents. You can see PE_derivation.png for the derivation:\n",
    "\n",
    "- $\\displaystyle PE_{(pos, 2i)} = sin \\left( pos \\times e^{\\frac{-2i}{d} ln(10000)} \\right)$ \n",
    "    \n",
    "- &nbsp;  $\\displaystyle PE_{(pos, 2i+1)} = cos \\left( pos \\times e^{\\frac{-2i}{d} ln(10000)} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "TODO: check Amirhossein Kazamnejad's blog on positional encoding\n",
    "\n",
    "Umar Jamil uses the [Harvard pytorch transformer article implementation of positional encoding formula](https://nlp.seas.harvard.edu/annotated-transformer/#positional-encoding) mentioned in the paper by using log. He mentions in his video that applying log to an exponential nullifies the effect of log but makes the calculation more numerically stable. The value of the positional encoding calculated this way will be slightly different but the model will learn. Click [here](https://youtu.be/ISNdQcPhsts?si=HNaqDgkw6CfwgO-M&t=470) to watch that particular scene from the video.\n",
    "\n",
    "Click [here](https://youtu.be/ISNdQcPhsts?si=cvEfkDJyW7LiBqkn&t=720) to see the reasoning behind using `self.register_buffer(\"pe\", pe)`. The reasoning that when we want to save some variable not as a learned parameter (like weights and biases) but we want it to be saved when we save the file of the model, the we should register it as a buffer. This way it will be saved along with the state of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin_cos_pe(q_len, d_model, normalize=True):\n",
    "    \"\"\"\n",
    "    Applies the sin-cos positional encoding to the input and output embedding.\n",
    "    q_len: number of patches\n",
    "    d_model: dimension of the model\n",
    "    normalize: whether to normalize the positional encoding or not\n",
    "\n",
    "    Returns: A tensor of shape (q_len, d_model)\n",
    "    \"\"\"\n",
    "    # initialize the positional encoding matrix of size (q_len, d_model) with zeros\n",
    "    pe = torch.zeros(q_len, d_model)\n",
    "    \n",
    "    # this is the 'pos' in the formula\n",
    "    pos = torch.arange(0, q_len).unsqueeze(1)\n",
    "    \n",
    "    # this is the 'i' in the formula\n",
    "    # no need to multiply by 2, because we are considering only even numbers\n",
    "    i = torch.arange(0, d_model, 2)\n",
    "\n",
    "    # below we implement the numerically more stable version of the sin and cos \n",
    "    # positional encoding (you can check the PE_derivation.png for the derivation) \n",
    "    denominator = torch.exp(-(i/d_model) * math.log(10000.0))\n",
    "\n",
    "    # apply sin positional encoding to the even indices\n",
    "    pe[:, 0::2] = torch.sin(pos * denominator)\n",
    "    # apply cos positional encoding to the odd indices\n",
    "    pe[:, 1::2] = torch.cos(pos * denominator)\n",
    "\n",
    "    # normalize the positional encoding, ie, subtract the mean and \n",
    "    # divide by the standard deviation\n",
    "    if normalize:\n",
    "        # i think we are multiplying by 10 so as to make sure we don't get very small values\n",
    "        # this is because division by very small values can lead to very large values\n",
    "        pe = (pe - pe.mean()) / (pe.std() * 10)\n",
    "\n",
    "    return pe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model  # in this paper, it 512\n",
    "        self.seq_len = seq_len  # maximum length of the sequence\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # create a matrix of shape (seq_len, d_model)\n",
    "        # pe stands for positional encoding\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # create a vector of shape (seq_len, 1)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "        # now, we will create the denominator of the positional encoding formulae\n",
    "        # since it is a bit long, we will break it into a few lines\n",
    "        # first, we need a vector containing multiples of 2 from 0 to d_model (here, 512)\n",
    "        # this line is because of the 2i term which is the power of 10000\n",
    "        # thus, this vector provides for the numbers we need for 2i\n",
    "        vector = torch.arange(0, d_model, 2, dtype=torch.float32)\n",
    "        # now, we raise 10,000 to the power of 2i/d_model\n",
    "        denominator_original = torch.pow(10000, vector/d_model)\n",
    "        # this is the one used by Harvard Transformer article\n",
    "        denominator_harvard = torch.exp(vector * (-math.log(10000.0)/d_model))\n",
    "        # we apply sin for even dimension and cos for odd dimenion\n",
    "        # apply sin and store it in even indices of pe\n",
    "        pe[:, 0::2] = torch.sin(position * denominator_original)\n",
    "        # apply cos and store it in odd indices of pe\n",
    "        pe[:, 1::2] = torch.cos(position * denominator_original)\n",
    "        # we need to add the batch dimension so that we can apply it to \n",
    "        # batches of sentences\n",
    "        pe = pe.unsqueeze(0)  # new shape: (1, seq_len, d_model)\n",
    "        # register the pe tensor as a buffer so that it can be saved along with the\n",
    "        # state of the model\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # we don't want to train the positional encoding, ie, we don't want to make it\n",
    "        # a learnable parameter, so we set its requires_grad to False\n",
    "        x = x + self.pe[:, :x.size(1)].requires_grad_(False)  # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies the sin-cos positional encoding to the input and output embedding.\n",
    "    max_seq_len: The maximum length of the sequence for which positional encodings \n",
    "    are pre-computed.\n",
    "    d_model: dimension of the input\n",
    "    \n",
    "    Returns: A tensor of shape (batch_size, max_seq_len, d_model)\n",
    "    \"\"\"\n",
    "    def __init__(self, max_seq_len, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # initialize the positional encoding matrix of size (max_seq_len, d_model) \n",
    "        # with zeros\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        # this is the 'pos' in the formula\n",
    "        # we want it to be of shape (max_seq_len, 1)\n",
    "        pos = torch.arange(0, max_seq_len).unsqueeze(1)\n",
    "        # this is the 'i' in the formula\n",
    "        # no need to multiply by 2, because we are considering only even numbers\n",
    "        i = torch.arange(0, d_model, 2)\n",
    "        # below we implement the numerically more stable version of the sin and cos\n",
    "        # positional encoding (you can check the PE_derivation.png for the derivation)\n",
    "        denominator = torch.exp(-(i/d_model) * math.log(10000.0))\n",
    "        # apply sin positional encoding to the even indices\n",
    "        pe[:, 0::2] = torch.sin(pos * denominator)\n",
    "        # apply cos positional encoding to the odd indices\n",
    "        pe[:, 1::2] = torch.cos(pos * denominator)\n",
    "        # we need to add the batch dimension so that we can apply it to \n",
    "        # batches of sentences\n",
    "        pe = pe.unsqueeze(0)  # new shape: (1, max_seq_len, d_model)\n",
    "        # pe is registered as a buffer, which means it will be part of the module's \n",
    "        # state but will not be considered a trainable parameter.\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # we don't want to train the positional encoding, ie, we don't want to make it\n",
    "        # a learnable parameter, so we set its requires_grad to False\n",
    "        # shape of x is (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1)].requires_grad_(False)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the positional encoding works by doing it on a smaller example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  1.0000],\n",
      "        [ 0.8415,  0.5403,  0.0264,  0.9997,  0.8573, -0.5148, -0.1383,  0.9904,\n",
      "          0.9992,  0.0402],\n",
      "        [ 0.9093, -0.4161,  0.0528,  0.9986, -0.8827, -0.4699, -0.2739,  0.9618,\n",
      "          0.0803, -0.9968],\n",
      "        [ 0.1411, -0.9900,  0.0791,  0.9969,  0.0516,  0.9987, -0.4042,  0.9147,\n",
      "         -0.9927, -0.1205],\n",
      "        [-0.7568, -0.6536,  0.1054,  0.9944,  0.8296, -0.5584, -0.5268,  0.8500,\n",
      "         -0.1600,  0.9871],\n",
      "        [-0.9589,  0.2837,  0.1316,  0.9913, -0.9058, -0.4237, -0.6393,  0.7690,\n",
      "          0.9799,  0.1993],\n",
      "        [-0.2794,  0.9602,  0.1577,  0.9875,  0.1031,  0.9947, -0.7395,  0.6732,\n",
      "          0.2392, -0.9710],\n",
      "        [ 0.6570,  0.7539,  0.1837,  0.9830,  0.7997, -0.6005, -0.8254,  0.5645,\n",
      "         -0.9606, -0.2778],\n",
      "        [ 0.9894, -0.1455,  0.2095,  0.9778, -0.9265, -0.3764, -0.8955,  0.4450,\n",
      "         -0.3160,  0.9488],\n",
      "        [ 0.4121, -0.9111,  0.2353,  0.9719,  0.1543,  0.9880, -0.9485,  0.3168,\n",
      "          0.9354,  0.3536]])\n",
      "\n",
      "\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.8415,  0.0264,  0.8573, -0.1383,  0.9992],\n",
      "        [ 0.9093,  0.0528, -0.8827, -0.2739,  0.0803],\n",
      "        [ 0.1411,  0.0791,  0.0516, -0.4042, -0.9927],\n",
      "        [-0.7568,  0.1054,  0.8296, -0.5268, -0.1600],\n",
      "        [-0.9589,  0.1316, -0.9058, -0.6393,  0.9799],\n",
      "        [-0.2794,  0.1577,  0.1031, -0.7395,  0.2392],\n",
      "        [ 0.6570,  0.1837,  0.7997, -0.8254, -0.9606],\n",
      "        [ 0.9894,  0.2095, -0.9265, -0.8955, -0.3160],\n",
      "        [ 0.4121,  0.2353,  0.1543, -0.9485,  0.9354]])\n",
      "\n",
      "\n",
      "tensor([[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
      "        [ 0.5403,  0.9997, -0.5148,  0.9904,  0.0402],\n",
      "        [-0.4161,  0.9986, -0.4699,  0.9618, -0.9968],\n",
      "        [-0.9900,  0.9969,  0.9987,  0.9147, -0.1205],\n",
      "        [-0.6536,  0.9944, -0.5584,  0.8500,  0.9871],\n",
      "        [ 0.2837,  0.9913, -0.4237,  0.7690,  0.1993],\n",
      "        [ 0.9602,  0.9875,  0.9947,  0.6732, -0.9710],\n",
      "        [ 0.7539,  0.9830, -0.6005,  0.5645, -0.2778],\n",
      "        [-0.1455,  0.9778, -0.3764,  0.4450,  0.9488],\n",
      "        [-0.9111,  0.9719,  0.9880,  0.3168,  0.3536]])\n"
     ]
    }
   ],
   "source": [
    "def dummyfn1():\n",
    "    seq_len = 10\n",
    "    d_model = 10\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    position = torch.arange(0, seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "    vector = torch.arange(0, d_model, 2, dtype=torch.float32)\n",
    "    denominator_original = torch.pow(10000, vector/d_model)\n",
    "    denominator_harvard = torch.exp(vector * (-math.log(10000.0)/d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * denominator_original)\n",
    "    pe[:, 1::2] = torch.cos(position * denominator_original)\n",
    "    print(pe, pe[:, 0::2], pe[:, 1::2], sep='\\n\\n\\n')\n",
    "\n",
    "dummyfn1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.4086,  3.1091,  1.1259, -0.0000],\n",
       "         [ 1.8999, -0.8678, -0.6868, -0.9279],\n",
       "         [ 0.1965,  1.5407, -1.5822, -0.0000],\n",
       "         [-0.0000, -1.9368, -2.2107,  0.9254]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dummyfn2():\n",
    "    torch.manual_seed(42)\n",
    "    seq_len = 4\n",
    "    d_model = 4\n",
    "    dropout = 0.2\n",
    "    x = torch.randn(d_model, seq_len)\n",
    "    obj = PositionalEncoding(d_model, seq_len, dropout)\n",
    "    return obj(x)\n",
    "\n",
    "dummyfn2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "Queries, Keys, and Values are all just the duplication of the input for the encoder. In other words, in the encoder block, we store the same value of input in queries, keys, and values. So, they are all the same thing. You can also think of them as just the input used 3 times.\n",
    "\n",
    "&nbsp;  \n",
    "\n",
    "Check [this](https://sentry.io/answers/difference-between-staticmethod-and-classmethod-function-decorators-in-python/#:~:text=We%20can%20decorate%20a%20function,object%20to%20it%2C%20as%20below.&text=This%20can%20be%20useful%20when,the%20instance%20it's%20called%20on.) article for information on `@staticmethod`. Basically, when you put `@staticmethod` on top of a method in a class, then that method does not take the `self` argument, which is the object of the class.\n",
    "\n",
    "&nbsp;  \n",
    "\n",
    "**Scaled dot-product attention:**\n",
    "\n",
    "$$\\text{Attention(Q,K,V)} = \\text{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) \\cdot V$$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Multi-head attention:**\n",
    "\n",
    "$$\\text{MultiHead(Q,K,V)} = \\text{Concat}(\\text{head}_1, \\; ..., \\; \\text{head}_h) \\cdot W^O$$\n",
    "\n",
    "$$, \\text{where head}_i = \\text{Attention}(Q \\cdot W_i^Q, \\;\\;\\; K \\cdot W_i^K, \\;\\;\\; V \\cdot W_i^V)$$\n",
    "\n",
    ", where the projections are parameter matrices:\n",
    "- $W_i^Q \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$\n",
    "- $W_i^K \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$\n",
    "- $W_i^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$\n",
    "- $W_i^O \\in \\mathbb{R}^{(h \\cdot d_v) \\times d_{\\text{model}}}$\n",
    "- $\\text{h = number of heads} = 8 = \\text{number of parallel attention layers}$\n",
    "- $d_{model} = 512$\n",
    "- $d_k = d_v = d_{model} // h = 64, \\quad \\text{where `//' is integer division}$\n",
    "\n",
    "\n",
    "&nbsp;  \n",
    "\n",
    "- The encoder has 6 identical layers.\n",
    "- Each layer has 2 sub-layers:\n",
    "    1. multi-head attention\n",
    "    2. feed-forward network\n",
    "- Each of the sub-layer is connected by a residual connection, followed by layer normalization, ie, `LayerNorm(x + sub-layer(x))`, where `sub-layer` is multi-head attention and feed-forward network, ie,\n",
    "    - `LayerNorm(x + multihead_attention(x))`\n",
    "    - `LayerNorm(x + feed_forward_network(x))`\n",
    "- To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension `d_model = 512`.\n",
    "\n",
    "&nbsp;  \n",
    "\n",
    "**TODO**: understand why `dim=-1` in the function `def scaled_dotproduct_attention` on the line `attn_weights = F.softmax(attn_scores, dim=-1)  # why dim=-1?`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10).reshape(2,-1).transpose(-2,-1).shape\n",
    "# .transpose(-2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fn ():\n",
    "    a = torch.arange(40).reshape(2,5,4)\n",
    "    return a.size(-1)\n",
    "\n",
    "fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n",
      "torch.Size([10, 5])\n",
      "torch.Size([1, 12, 5])\n",
      "torch.Size([5, 4])\n",
      "torch.Size([10, 4])\n",
      "torch.Size([1, 12, 4])\n"
     ]
    }
   ],
   "source": [
    "def fn():\n",
    "    d_model = 5\n",
    "    d_k = 4\n",
    "    m = nn.Linear(d_model, d_k)\n",
    "    x1 = torch.randn(5, d_model)\n",
    "    x2 = torch.randn(10, d_model)\n",
    "    x3 = torch.randn(1, 12, d_model)\n",
    "    print(x1.size())\n",
    "    print(x2.size())\n",
    "    print(x3.size())\n",
    "    # note: the number of input features of m should be equal to the last dimension of x\n",
    "    print(m(x1).size())\n",
    "    print(m(x2).size())\n",
    "    print(m(x3).size())    \n",
    "\n",
    "fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=512, h=8, dropout=0.0):\n",
    "        \"\"\"\n",
    "        d_model: dimension of the model (size of the embedding vector)\n",
    "        num_heads: number of heads in multihead attention\n",
    "        dropout: dropout probability\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model           # embedding vector size\n",
    "        self.h = h                       # number of heads\n",
    "        # make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model must be divisible by h\"\n",
    "        # we assume d_v is always equal to d_k, so we just write d_k insead of d_v\n",
    "        self.d_k = d_model // h          # dimension of vector seen by each head\n",
    "        # weight matrices for Q, K, V, and O\n",
    "        # The shapes of these matrices are mentioned in the paper\n",
    "        # I have written them above for reference \n",
    "        \n",
    "        self.wq = nn.Linear(d_model, self.d_k, bias=False)     # Wq\n",
    "        self.wk = nn.Linear(d_model, self.d_k, bias=False)     # Wk\n",
    "        self.wv = nn.Linear(d_model, self.d_k, bias=False)     # Wv\n",
    "        # self.wo = nn.Linear(h*self.d_k, d_model, bias=False)   # Wo\n",
    "        \n",
    "        # self.wq = nn.Linear(d_model, d_model, bias=False)  # Wq\n",
    "        # self.wk = nn.Linear(d_model, d_model, bias=False)  # Wk\n",
    "        # self.wv = nn.Linear(d_model, d_model, bias=False)  # Wv\n",
    "        self.wo = nn.Linear(d_model, d_model, bias=False)  # Wo\n",
    "\n",
    "        # if dropout is zero, then nn.Dropout does not do anything\n",
    "        self.dropout = nn.Dropout(p=dropout)  \n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def scaled_dotproduct_attention(query, key, value, mask=None, dropout=None):\n",
    "        \"\"\"\n",
    "        Compute the scaled dot-product attention.\n",
    "        \n",
    "        query: the query tensor\n",
    "        key: the key tensor\n",
    "        value: the value tensor\n",
    "        mask: the mask tensor\n",
    "        dropout: the dropout probability\n",
    "        \"\"\"\n",
    "        d_k = query.size(-1)\n",
    "        attn_scores = torch.matmul(query, key.transpose(-2,-1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # write a very low value (indicating -infinity) to the positions where mask == 0, \n",
    "            # this will tell softmax to replace those values with zero\n",
    "            attn_scores = attn_scores.masked_fill(mask==0, -1e9)\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)  # why dim=-1?\n",
    "        attn_weights = dropout(attn_weights)\n",
    "        output = torch.matmul(attn_weights, value)\n",
    "        return output, attn_weights\n",
    "\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        q: query matrix of shape (batch_size, q_len, d_model)\n",
    "        k: key matrix of shape (batch_size, k_len, d_model)\n",
    "        v: value matrix of shape (batch_size, v_len, d_model)\n",
    "        mask: mask to prevent attention to certain positions\n",
    "\n",
    "        Returns: output matrix of shape (batch_size, q_len, d_model)\n",
    "        \"\"\"\n",
    "        # linear transformation for Q, K, and V\n",
    "        # multiply Wq matrix by q\n",
    "        # this matrix multiplication does not change the shape of q\n",
    "        query = self.wq(q)              # (batch_size, q_len, d_model)\n",
    "        print(1,query.shape,sep='\\t-->\\t')\n",
    "        # similary for key and value\n",
    "        key = self.wk(k)\n",
    "        print(2,key.shape,sep='\\t-->\\t')\n",
    "        value = self.wv(v)\n",
    "        print(3,value.shape,sep='\\t-->\\t')\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, num_heads, d_k) \n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k)\n",
    "        print(4,query.shape,sep='\\t-->\\t')\n",
    "        # (batch, seq_len, num_heads, d_k) --> (batch, num_heads, seq_len, d_k)\n",
    "        query = query.transpose(1,2) # interchange the indices 1 and 2 with each other\n",
    "        print(5,query.shape,sep='\\t-->\\t')\n",
    "        # similarly, we will change the dimensions of key and value\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k)\n",
    "        print(6,key.shape,sep='\\t-->\\t')\n",
    "        key = key.transpose(1,2)\n",
    "        print(7,key.shape,sep='\\t-->\\t')\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k)\n",
    "        print(8,value.shape,sep='\\t-->\\t')\n",
    "        value = value.transpose(1,2)\n",
    "        print(9,value.shape,sep='\\t-->\\t')\n",
    "        x, self.attn_scores = MultiHeadAttention.scaled_dotproduct_attention(query, key, \n",
    "                                                                             value, mask,\n",
    "                                                                             self.dropout)\n",
    "        print(10,x.shape,self.attn_scores.shape,sep='\\t-->\\t')\n",
    "        # combine all the heads together\n",
    "        # (batch, num_heads, seq_len, d_k) --> (batch, seq_len, num_heads, d_k)\n",
    "        x = x.transpose(1,2)\n",
    "        print(11,x.shape,sep='\\t-->\\t')\n",
    "        # (batch, seq_len, num_heads, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "        print(12,x.shape,sep='\\t-->\\t')\n",
    "        # now, multiply by Wo\n",
    "        # this matrix multiplication does not change the shape of x\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        x = self.wo(x)\n",
    "        print(13,x.shape,sep='\\t-->\\t')\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second way of doing multihead attention\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        # Recall that d_model = num_heads * d_k   OR   d_k = d_model // num_heads\n",
    "        assert d_model == (self.num_heads * self.d_k), \"d_model must be equal \\\n",
    "            to self.num_heads * self.d_k\"\n",
    "        x = x.view(batch_size, seq_length, self.num_heads, self.d_k)\n",
    "        x = x.transpose(1, 2)\n",
    "        # shape of x becomes: [batch_size, self.num_heads, seq_length, self.d_k]\n",
    "        return x\n",
    "        \n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        x: tensor of shape [batch_size, num_heads, seq_length, d_k]\n",
    "\n",
    "        Returns a tensor of shape [batch_size, seq_length, d_model]\n",
    "\n",
    "        Description:\n",
    "        Combine the multiple heads back to original shape, ie,\n",
    "        we want the shape of x to be [batch_size, seq_length, d_model]\n",
    "        \"\"\"\n",
    "        # After calling split_heads function, the shape of x became:\n",
    "        # [batch_size, self.num_heads, seq_length, self.d_k]\n",
    "        batch_size, num_heads, seq_length, d_k = x.size()\n",
    "        \n",
    "        assert num_heads == self.num_heads, \"Number of heads must be equal to self.num_heads\"\n",
    "        assert d_k == self.d_k, \"d_k must be equal to self.d_k\"\n",
    "        assert self.d_model == (num_heads * d_k), \"d_model must be equal to \\\n",
    "            self.num_heads * self.d_k\"\n",
    "        \n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        # shape of x becomes: [batch_size, seq_length, self.num_heads, self.d_k]\n",
    "        x = x.view(batch_size, seq_length, self.d_model)\n",
    "        return x\n",
    "        \n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations to Q, K, V\n",
    "        Q = self.W_q(Q)\n",
    "        K = self.W_k(K)\n",
    "        V = self.W_v(V)\n",
    "        \n",
    "        # Split heads\n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "\n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads \n",
    "        heads_combined = self.combine_heads(attn_output)\n",
    "        \n",
    "        # Apply output transformation\n",
    "        output = self.W_o(heads_combined)\n",
    "        return output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 5, 4])\n",
      "torch.Size([4, 8, 5])\n"
     ]
    }
   ],
   "source": [
    "print(torch.randn(4,5,8).T.size())\n",
    "print(torch.randn(4,5,8).transpose(-2,-1).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]]) \n",
      "\n",
      "tensor([[   1,    2, -100],\n",
      "        [   4, -100,    6],\n",
      "        [-100,    8,    9]])\n",
      "\n",
      "tensor([[100, 100,   3],\n",
      "        [100,   5, 100],\n",
      "        [  7, 100, 100]])\n"
     ]
    }
   ],
   "source": [
    "def fn():\n",
    "    a = torch.arange(1,10).reshape(3,3)\n",
    "    print(a, \"\\n\")\n",
    "    mask = torch.tensor([[1,1,0], [1,0,1], [0,1,1]])\n",
    "    b = a.masked_fill(mask==0, -100)\n",
    "    c = a.masked_fill(mask==1, 100)\n",
    "    print(b,c,sep='\\n\\n')\n",
    "\n",
    "fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 5])\n",
      "torch.Size([1, 3, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "def fn():\n",
    "    x = torch.randn(1,2,15)\n",
    "    x = x.view(1,2,3,5)\n",
    "    print(x.size())\n",
    "    x = x.transpose(1,2)\n",
    "    print(x.size())\n",
    "    \n",
    "fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t-->\ttorch.Size([1, 64, 64])\n",
      "2\t-->\ttorch.Size([1, 64, 64])\n",
      "3\t-->\ttorch.Size([1, 64, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 64, 8, 64]' is invalid for input of size 4096",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5004/1787311638.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#         row_settings=[\"var_names\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5004/1787311638.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# return summary(model=mha,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#         input_size=([(1, d_k, d_model), (1, d_k, d_model), (1, d_k, d_model)]),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5004/2380295109.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t-->\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# (batch, seq_len, d_model) --> (batch, seq_len, num_heads, d_k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t-->\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# (batch, seq_len, num_heads, d_k) --> (batch, num_heads, seq_len, d_k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 64, 8, 64]' is invalid for input of size 4096"
     ]
    }
   ],
   "source": [
    "def fn():\n",
    "    d_model = 512\n",
    "    h = 8\n",
    "    d_k = d_model // h\n",
    "    dropout = 0.0\n",
    "    mha = MultiHeadAttention(d_model, h, dropout)\n",
    "    x = torch.randn(1, d_k, d_model)\n",
    "    mha(x, x, x)\n",
    "    # return summary(model=mha,\n",
    "    #         input_size=([(1, d_k, d_model), (1, d_k, d_model), (1, d_k, d_model)]),\n",
    "    #         dtypes=[torch.float32, torch.float32, torch.float32],\n",
    "    #         col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"),\n",
    "    #         col_width=20,\n",
    "    #         row_settings=[\"var_names\"])\n",
    "\n",
    "fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.view()` is used to reshape a tensor. We can reshape a tensor using `.view()` and stored the reshaped version in another variable. We must note that `.view()` passes the reference of a tensor, ie, memory address of the tensor. So, if we make changes to one tensor, then they get reflected in the other tensor as well. \n",
    "\n",
    "\n",
    "Check [this](https://stackoverflow.com/questions/48915810/what-does-contiguous-do-in-pytorch) for information on the use of `.contiguous()` in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the following line of code: \n",
    "```python\n",
    "x = x.contiguous().view(x.shape[0], -1, self.num_heads * self.d_k)\n",
    "```\n",
    "\n",
    "we can use the following:\n",
    "\n",
    "```python\n",
    "x = x.reshape(x.shape[0], -1, self.num_heads * self.d_k)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [0., 1., 1.]])\n",
      "\n",
      "tensor([[1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "def dummyfn1():\n",
    "    A = torch.ones(2,3)\n",
    "    B = A.view(1,6)\n",
    "    B[0,3] = 0  # change 1 at index [0,3] in B, this will also change the \n",
    "    # the 1 in A at the corresponding index\n",
    "    print(A, B, sep='\\n\\n')\n",
    "    pass\n",
    "    \n",
    "dummyfn1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5004/371450271.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdummyfn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5004/371450271.py\u001b[0m in \u001b[0;36mdummyfn1\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# we get error because we didn't use contiguous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "def dummyfn1():\n",
    "    A = torch.ones(3,4)\n",
    "    A = A.transpose(-1,-2)\n",
    "    A = A.view(A.shape[1], -1)  # we get error because we didn't use contiguous\n",
    "    return A\n",
    "\n",
    "dummyfn1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dummyfn1():\n",
    "    A = torch.ones(3,4)\n",
    "    A = A.transpose(-1,-2)\n",
    "    A = A.contiguous().view(A.shape[1], -1)\n",
    "    return A\n",
    "\n",
    "dummyfn1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dummyfn1():\n",
    "    A = torch.ones(3,4)\n",
    "    A = A.transpose(-1,-2)\n",
    "    # instead of using contiguous and view, we can use reshape \n",
    "    A = A.reshape(A.shape[1], -1)\n",
    "    return A\n",
    "\n",
    "dummyfn1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 ways of doing transpose in PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.ones(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.transpose(torch.ones(2,3), -1, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.ones(2,3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Position-wise) Feed-Forward Network\n",
    "\n",
    "$$\\text{FFN}(x) = max(0, \\;\\; xW_1 + b_1)W_2 \\; + \\; b_2$$\n",
    "\n",
    ", where:\n",
    "- $W_1 \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{ff}}$\n",
    "- $W_1 \\in \\mathbb{R}^{d_{ff} \\times d_{\\text{model}}}$\n",
    "\n",
    "&nbsp;  \n",
    "\n",
    "See section 3.3 on page 5 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"Implementation of the FFN equation.\"\"\"\n",
    "    def __init__(self, d_model = 512, d_ff = 2048, dropout: float = 0.0):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        # the shapes are mentioned in the paper\n",
    "        # I have written them above for reference\n",
    "        self.w1 = nn.Linear(d_model, d_ff, bias=True)\n",
    "        self.w2 = nn.Linear(d_ff, d_model, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        # if dropout is zero, then nn.Dropout does not do anything\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.w1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.w2(x)\n",
    "        return x\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "\n",
    "In layer normalization, we calculate the mean and variance of each data point independently from other data points. Then, we calculate new values for each data point using their own mean and their own variance.\n",
    "\n",
    "Note: $\\text{variance} = \\text{(standard deviation)}^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this formula:\n",
    "\n",
    "$$\\hat{x}_j = \\alpha \\times \\left(\\frac{x_j - \\mu_j}{\\sqrt{\\sigma_j^2 + \\epsilon}}\\right) + \\beta $$\n",
    "\n",
    ", where:\n",
    "- $\\alpha$ is the multiplicative factor\n",
    "- $\\beta$ is the additive factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15., 16., 17., 18., 19.])\n",
      "tensor([9.5000])\n",
      "\n",
      "\n",
      "tensor(9.5000)\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "def fn():\n",
    "    a = torch.arange(20, dtype=torch.float32)\n",
    "    print(a)\n",
    "    mu1 = a.mean(dim=-1, keepdim=True)\n",
    "    mu2 = a.mean()\n",
    "    print(mu1, mu2, sep='\\n\\n\\n')\n",
    "    b = torch.sqrt(a[16])\n",
    "    print(b)\n",
    "\n",
    "fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    \"\"\"This is the 'norm' part in the 'add & norm' block in the paper.\"\"\"\n",
    "    def __init__(self, features: int, eps: float = 1e-6) -> None:\n",
    "        super(LayerNormalization, self).__init__()\n",
    "        self.eps = eps\n",
    "        # instead of simply doing self.alpha = torch.ones(1)\n",
    "        # we use nn.Parameter() so that when we call the state dict of the model\n",
    "        # we are able to see this alpha\n",
    "        # only using torch.ones() won't allow us to see this alpha\n",
    "        self.alpha = nn.Parameter(torch.ones(features))  # multiplied\n",
    "        self.beta = nn.Parameter(torch.zeros(features))  # added\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # apply mean after the batch dimension\n",
    "        # mean usually cancels the dimension to which it is applied,  \n",
    "        # but we want to keep it\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        # similarly for standard deviation\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        # apply the layer normalization\n",
    "        fraction = (x - mean) / (torch.sqrt(std**2 + self.eps))\n",
    "        x_normalized = self.alpha * fraction + self.beta\n",
    "        return x_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \"\"\"This is the 'add' part in the 'add & norm' block.\"\"\"\n",
    "    def __init__(self, d_model: int, dropout: float = 0.0) -> None:\n",
    "        super(ResidualConnection, self).__init__()\n",
    "        # if dropout is zero, then nn.Dropout does not do anything\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.norm = LayerNormalization(d_model)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        x: input\n",
    "        sublayer: different layers of the transformer architecture (eg: multi-head\n",
    "        attention, feed-forward network, etc.), we will pass these layers as\n",
    "        functions to this class.\n",
    "        \n",
    "        Returns the skip or residual connection.\n",
    "        \"\"\"\n",
    "        # most implementations first do normalization and then pass x to the sublayer\n",
    "        # we will also do this way\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "        # however, the paper first passes x to the sublayer and then does the norm\n",
    "        # return x + self.dropout(self.norm(sublayer(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model=512, h=8, d_ff=2048, dropout=0.0):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, h, dropout)\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout)\n",
    "        self.rc = ResidualConnection(d_model, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: position-aware embedding (positional encoding + input embedding)\n",
    "        \"\"\"\n",
    "        sublayer1 = lambda x: self.mha(x,x,x)  # q=x, k=x, v=x\n",
    "        x = self.rc(x, sublayer1)\n",
    "        x = self.rc(x, self.ffn)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.0):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.mha(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [LayerNormalization: 2, Linear: 2, Linear: 2, Linear: 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1128\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24898/3246856178.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msublayer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# q=x, k=x, v=x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msublayer1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1128\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24898/3086844280.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# we will also do this way\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# however, the paper first passes x to the sublayer and then does the norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24898/3246856178.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \"\"\"\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0msublayer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# q=x, k=x, v=x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msublayer1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1128\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24898/3100091691.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# (batch, seq_len, d_model) --> (batch, seq_len, num_heads, d_k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;31m# (batch, seq_len, num_heads, d_k) --> (batch, num_heads, seq_len, d_k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1186\u001b[0m             type(self).__name__, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultiHeadAttention' object has no attribute 'num_heads'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24898/491169220.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m             row_settings=[\"var_names\"])\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_24898/491169220.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0menb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     summary(model=enb,\n\u001b[0m\u001b[1;32m      4\u001b[0m             \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mcol_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input_size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_params\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"trainable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     )\n\u001b[0;32m--> 218\u001b[0;31m     summary_list = forward_pass(\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_forward_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mexecuted_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msummary_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuted\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    300\u001b[0m             \u001b[0;34m\"Failed to run torchinfo. See above stack traces for more details. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;34mf\"Executed layers up to: {executed_layers}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [LayerNormalization: 2, Linear: 2, Linear: 2, Linear: 2]"
     ]
    }
   ],
   "source": [
    "def fn():\n",
    "    enb = EncoderBlock(512, 8, 2048, 0.0)\n",
    "    summary(model=enb,\n",
    "            input_size=(64, 100, 512),\n",
    "            col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"),\n",
    "            col_width=20,\n",
    "            row_settings=[\"var_names\"])\n",
    "\n",
    "fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fn():\n",
    "    sublayer1 = lambda x: x + 1\n",
    "    return sublayer1(x=2)\n",
    "\n",
    "fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, h: int, dropout: float) -> None:\n",
    "        self.mha = MultiHeadAttention(d_model, h, dropout)\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout)\n",
    "        self.rc = ResidualConnection(d_model, dropout)\n",
    "        # store 2 residual connection layers\n",
    "        # we'l use one after self-attention layer and the other after feed-forward \n",
    "        # network as shown in figure 1 of the paper\n",
    "        self.res_con = nn.ModuleList([ResidualConnection(d_model, dropout)\n",
    "                                      for _ in range(2)])\n",
    "        \n",
    "    def forward(self, x, src_mask):\n",
    "        # we apply the source mask because we don't want the padding word to \n",
    "        # interact with other words\n",
    "        x = self.res_con[0](x, lambda x: self.mha(x,x,x,src_mask))\n",
    "        x = self.res_con[1](x, self.ffn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "cannot assign module before Module.__init__() call",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24898/821568048.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_24898/821568048.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0menb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24898/2703936541.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, d_model, d_ff, h, dropout)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mEncoderBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedForwardNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResidualConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1212\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmodules\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m                     raise AttributeError(\n\u001b[0m\u001b[1;32m   1215\u001b[0m                         \"cannot assign module before Module.__init__() call\")\n\u001b[1;32m   1216\u001b[0m                 \u001b[0mremove_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_persistent_buffers_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: cannot assign module before Module.__init__() call"
     ]
    }
   ],
   "source": [
    "def fn():\n",
    "    enb = EncoderBlock(512, 2048, 8, 0.0)\n",
    "    print(enb)\n",
    "\n",
    "fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, features: int, selfattn_block: MultiHeadAttention,\n",
    "                 crossattn_block: MultiHeadAttention, dropout: float,\n",
    "                 feedforward_block: FeedForwardNetwork) -> None:\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.selfattn_block = selfattn_block\n",
    "        self.crossattn_block = crossattn_block\n",
    "        self.feedforward_block = feedforward_block\n",
    "        self.res_con = nn.ModuleList([ResidualConnection(features, dropout) \n",
    "                                      for _ in range(3)])\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.res_con[0](x, lambda x: self.selfattn_block(x, x, x, tgt_mask))\n",
    "        x = self.res_con[1](x, lambda x: self.crossattn_block(x, encoder_output, \n",
    "                                                              encoder_output, src_mask))\n",
    "        x = self.res_con[2](x, self.feedforward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, features: int, layers: nn.ModuleList):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x, enc_out, src_mask=None, tgt_mask=None):\n",
    "        # multi-head attention\n",
    "        selfattn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(selfattn_output))\n",
    "        # masked multi-head attention\n",
    "        # query from decoder\n",
    "        # key and value from encoder\n",
    "        crossattn_output = self.cross_attn(x, enc_out, enc_out, src_mask)\n",
    "        x = self.norm2(x + self.dropout(crossattn_output))\n",
    "        # feed forward network\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm3(x + self.dropout(ffn_output))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super(ProjectionLayer, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformer Class (collection of all the above methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbedding,\n",
    "                 tgt_embed: InputEmbedding, src_pos: PositionalEncoding, \n",
    "                 tgt_pos: PositionalEncoding, proj_layer: ProjectionLayer) -> None:\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.proj_layer = proj_layer\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.proj_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderLayer(\n",
       "  (mha): MultiHeadAttention(\n",
       "    (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (ffn): FeedForwardNetwork(\n",
       "    (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EncoderLayer(512,8,2048,0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): EncoderLayer(\n",
       "    (mha): MultiHeadAttention(\n",
       "      (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (ffn): FeedForwardNetwork(\n",
       "      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (1): EncoderLayer(\n",
       "    (mha): MultiHeadAttention(\n",
       "      (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (ffn): FeedForwardNetwork(\n",
       "      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (2): EncoderLayer(\n",
       "    (mha): MultiHeadAttention(\n",
       "      (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (ffn): FeedForwardNetwork(\n",
       "      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (3): EncoderLayer(\n",
       "    (mha): MultiHeadAttention(\n",
       "      (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (ffn): FeedForwardNetwork(\n",
       "      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (4): EncoderLayer(\n",
       "    (mha): MultiHeadAttention(\n",
       "      (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (ffn): FeedForwardNetwork(\n",
       "      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (5): EncoderLayer(\n",
       "    (mha): MultiHeadAttention(\n",
       "      (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (ffn): FeedForwardNetwork(\n",
       "      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.ModuleList([\n",
    "    EncoderLayer(512,8,2048,0.0) \n",
    "    for _ in range(6)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderLayer(\n",
       "  (self_attn): MultiHeadAttention(\n",
       "    (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (cross_attn): MultiHeadAttention(\n",
       "    (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (ffn): FeedForwardNetwork(\n",
       "    (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecoderLayer(512,8,2048,0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(5000, 512)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Embedding(5000,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 1, 1],\n",
      "        [0, 0, 1, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 0]]) \n",
      "\n",
      "tensor([[1, 1, 1, 1],\n",
      "        [0, 1, 1, 1],\n",
      "        [0, 0, 1, 1],\n",
      "        [0, 0, 0, 1]]) \n",
      "\n",
      "tensor([[0, 1, 1, 1],\n",
      "        [0, 0, 1, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "def fn():\n",
    "    a = torch.triu(torch.ones(4, 4, dtype=torch.long), diagonal=1)\n",
    "    print(a,\"\\n\")\n",
    "    b = torch.triu(torch.ones(4, 4, dtype=torch.long), diagonal=0)\n",
    "    print(b,\"\\n\")\n",
    "    c = a & b\n",
    "    print(c)\n",
    "\n",
    "fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.shape = torch.Size([3, 3])\n",
      "tgt.shape = torch.Size([3, 3])\n",
      "src_mask.shape = torch.Size([3, 3])\n",
      "src_mask.shape = torch.Size([3, 1, 3])\n",
      "src_mask.shape = torch.Size([3, 1, 1, 3])\n",
      "tgt_mask.shape = torch.Size([3, 3])\n",
      "tgt_mask.shape = torch.Size([3, 1, 3])\n",
      "tgt_mask.shape = torch.Size([3, 1, 3, 1])\n",
      "seq_length = 3\n",
      "ones_vector.shape = torch.Size([1, 3, 3])\n",
      "ones_vector = \n",
      "tensor([[[1, 1, 1],\n",
      "         [1, 1, 1],\n",
      "         [1, 1, 1]]])\n",
      "peak_mask.shape = torch.Size([1, 3, 3])\n",
      "peak_mask = \n",
      "tensor([[[0, 1, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 0]]])\n",
      "peak_mask_bool.shape = torch.Size([1, 3, 3])\n",
      "peak_mask_bool = \n",
      "tensor([[[False,  True,  True],\n",
      "         [False, False,  True],\n",
      "         [False, False, False]]])\n",
      "nopeak_mask.shape = torch.Size([1, 3, 3])\n",
      "nopeak_mask = \n",
      "tensor([[[1, 0, 0],\n",
      "         [1, 1, 0],\n",
      "         [1, 1, 1]]])\n",
      "nopeak_mask_bool.shape = torch.Size([1, 3, 3])\n",
      "nopeak_mask_bool = \n",
      "tensor([[[ True, False, False],\n",
      "         [ True,  True, False],\n",
      "         [ True,  True,  True]]])\n",
      "tgt_mask = \n",
      "tensor([[[[ True],\n",
      "          [False],\n",
      "          [ True]]],\n",
      "\n",
      "\n",
      "        [[[False],\n",
      "          [ True],\n",
      "          [ True]]],\n",
      "\n",
      "\n",
      "        [[[ True],\n",
      "          [ True],\n",
      "          [False]]]])\n",
      "\n",
      "nopeak_mask = \n",
      "tensor([[[1, 0, 0],\n",
      "         [1, 1, 0],\n",
      "         [1, 1, 1]]])\n",
      "tgt_mask = \n",
      "tensor([[[[1, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]]]])\n"
     ]
    }
   ],
   "source": [
    "def fn():\n",
    "    src = torch.tensor([[11,0,13], [0,15,16], [17,18,0]], dtype=torch.long)\n",
    "    tgt = torch.tensor([[1,0,3], [0,5,6], [7,8,0]], dtype=torch.long)\n",
    "    print(f\"src.shape = {src.shape}\")\n",
    "    print(f\"tgt.shape = {tgt.shape}\")\n",
    "    # Create a mask for the source sequence (src)\n",
    "    src_mask = (src != 0)\n",
    "    print(f\"src_mask.shape = {src_mask.shape}\")\n",
    "    src_mask = src_mask.unsqueeze(1)\n",
    "    print(f\"src_mask.shape = {src_mask.shape}\")\n",
    "    src_mask = src_mask.unsqueeze(2)\n",
    "    print(f\"src_mask.shape = {src_mask.shape}\")\n",
    "    # Create a mask for the target sequence (tgt)\n",
    "    tgt_mask = (tgt != 0)\n",
    "    print(f\"tgt_mask.shape = {tgt_mask.shape}\")\n",
    "    tgt_mask = tgt_mask.unsqueeze(1)\n",
    "    print(f\"tgt_mask.shape = {tgt_mask.shape}\")\n",
    "    tgt_mask = tgt_mask.unsqueeze(3)\n",
    "    print(f\"tgt_mask.shape = {tgt_mask.shape}\")\n",
    "    # Determine the length of the target sequence\n",
    "    seq_length = tgt.size(1)\n",
    "    print(f\"seq_length = {seq_length}\")\n",
    "    # Create a no-peak mask for the target sequence to prevent peeking into future tokens\n",
    "    ones_vector = torch.ones(1, seq_length, seq_length, dtype=torch.long)\n",
    "    print(f\"ones_vector.shape = {ones_vector.shape}\")\n",
    "    print(f\"ones_vector = \\n{ones_vector}\")\n",
    "    peak_mask = torch.triu(ones_vector, diagonal=1)\n",
    "    peak_mask_bool = peak_mask.bool()\n",
    "    print(f\"peak_mask.shape = {peak_mask.shape}\")\n",
    "    print(f\"peak_mask = \\n{peak_mask}\")\n",
    "    print(f\"peak_mask_bool.shape = {peak_mask_bool.shape}\")\n",
    "    print(f\"peak_mask_bool = \\n{peak_mask_bool}\")\n",
    "    nopeak_mask = (1 - peak_mask)\n",
    "    nopeak_mask_bool = nopeak_mask.bool()\n",
    "    print(f\"nopeak_mask.shape = {nopeak_mask.shape}\")\n",
    "    print(f\"nopeak_mask = \\n{nopeak_mask}\")\n",
    "    print(f\"nopeak_mask_bool.shape = {nopeak_mask_bool.shape}\")\n",
    "    print(f\"nopeak_mask_bool = \\n{nopeak_mask_bool}\")\n",
    "    # Combine the padding mask and the no-peak mask for the target sequence\n",
    "    print(f\"tgt_mask = \\n{tgt_mask}\\n\")\n",
    "    print(f\"nopeak_mask = \\n{nopeak_mask}\")\n",
    "    tgt_mask = tgt_mask & nopeak_mask\n",
    "    print(f\"tgt_mask = \\n{tgt_mask}\")\n",
    "    \n",
    "\n",
    "fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4148, 0.3033, 0.2819])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.tensor([0.667, 0.354, 0.281]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, \n",
    "                 num_layers, d_ff, max_seq_len, dropout=0.0):\n",
    "        \"\"\"\n",
    "        src_vocab_size: Source vocabulary size.\n",
    "        tgt_vocab_size: Target vocabulary size.\n",
    "        d_model: The dimensionality of the model's embeddings.\n",
    "        num_heads: Number of attention heads in the multi-head attention mechanism.\n",
    "        num_layers: Number of layers for both the encoder and the decoder.\n",
    "        d_ff: Dimensionality of the inner layer in the feed-forward network.\n",
    "        max_seq_len: Maximum sequence length for positional encoding.\n",
    "        dropout: Dropout rate for regularization.\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        self.enc_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(max_seq_len, d_model)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout) \n",
    "            for _ in range(num_layers)\n",
    "            ])\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout) \n",
    "            for _ in range(num_layers)\n",
    "            ])\n",
    "        \n",
    "        self.proj = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        \"\"\"\n",
    "        This method is used to create masks for the source and target sequences, \n",
    "        ensuring that padding tokens are ignored and that future tokens are not \n",
    "        visible during training for the target sequence.\n",
    "        \"\"\"\n",
    "        # Create a mask for the source sequence (src)\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        # Create a mask for the target sequence (tgt)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        # Determine the length of the target sequence\n",
    "        seq_length = tgt.size(1)\n",
    "        # Create a no-peak mask for the target sequence to prevent peeking into future tokens\n",
    "        # create a vector of ones of shape (1, seq_length, seq_length)\n",
    "        ones_vector = torch.ones(1, seq_length, seq_length)\n",
    "        # Create an upper triangular mask starting above the diagonal (i.e., diagonal=1). \n",
    "        # This results in a matrix where elements above the diagonal are set to one, and \n",
    "        # elements on and below the diagonal are zero.\n",
    "        peak_mask = torch.triu(ones_vector, diagonal=1)\n",
    "        # Invert the peak mask to create a no-peak mask\n",
    "        nopeak_mask = (1 - peak_mask)\n",
    "        # Convert the nopeak mask to a boolean mask\n",
    "        nopeak_mask = nopeak_mask.bool()\n",
    "        # Combine the padding mask and the no-peak mask for the target sequence\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        This method defines the forward pass for the Transformer, taking source \n",
    "        and target sequences and producing the output predictions. Below we outline\n",
    "        the steps involved in the forward pass:\n",
    "        1. Input Embedding and Positional Encoding: The source and target sequences \n",
    "        are first embedded using their respective embedding layers and then added to \n",
    "        their positional encodings.\n",
    "        2. Encoder Layers: The source sequence is passed through the encoder layers, \n",
    "        with the final encoder output representing the processed source sequence.\n",
    "        3. Decoder Layers: The target sequence and the encoder's output are passed \n",
    "        through the decoder layers, resulting in the decoder's output.\n",
    "        4. Final Linear Layer: The decoder's output is mapped to the target vocabulary \n",
    "        size using a fully connected (linear) layer.\n",
    "        \n",
    "        Output: The final output is a tensor representing the model's predictions for \n",
    "        the target sequence.\n",
    "        \"\"\"\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.pos_enc(self.enc_emb(src)))\n",
    "        tgt_embedded = self.dropout(self.pos_enc(self.dec_emb(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.proj(dec_output)\n",
    "        # calculate output probabilities\n",
    "        output_prob = F.softmax(output, dim=-1)\n",
    "        return output_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Transformer Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, \n",
    "                      tgt_seq_len: int, d_model: int = 512, Nx: int = 6, h: int = 8,\n",
    "                      dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
    "    # Create the input and output embedding layers\n",
    "    src_embed = InputEmbedding(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbedding(d_model, tgt_vocab_size)\n",
    "    \n",
    "    # Create the input and output positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "    \n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(Nx):\n",
    "        encoder_selfattn_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        encoder_feedforward_block = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_selfattn_block, \n",
    "                                     encoder_feedforward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "        \n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(Nx):\n",
    "        decoder_selfttn_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_crossattn_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_feedforward_block = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(d_model, decoder_selfttn_block, \n",
    "                                     decoder_crossattn_block, decoder_feedforward_block,\n",
    "                                     dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "        \n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    # Create the projection layer\n",
    "    proj_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "    \n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, \n",
    "                              tgt_pos, projection_layer)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
