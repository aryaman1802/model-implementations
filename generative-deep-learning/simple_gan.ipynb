{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network (GAN)\n",
    "\n",
    "Paper: [Generative Adversarial Nets](https://arxiv.org/pdf/1406.2661)\n",
    "\n",
    "Helpful Resources:\n",
    "- [Aladdin Persson's playlist on GANs](https://youtube.com/playlist?list=PLhhyoLH6IjfwIp8bZnzX8QR30TRcHO8Va&si=8ooImkbbXhCUC1xB)\n",
    "- [GANs specialization on coursera](https://www.coursera.org/specializations/generative-adversarial-networks-gans)\n",
    "- [Stanford's Deep Generative Models playlist](https://youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8&si=N_TpTe1bPIhte-t8)\n",
    "- [AssemblyAI's GAN tutorial](https://youtu.be/_pIMdDWK5sc?si=Mtx2oWh1ZO9tqWYg)\n",
    "- [The Math Behind Generative Adversarial Networks Clearly Explained! - Normalized Nerd](https://youtu.be/Gib_kiXgnvA?si=wi7mSBZ7uUCsWBn6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print(\"Imports done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(img_tensor, num_imgs=25, size=(1,28,28)):\n",
    "    \"\"\"\n",
    "    Given a tensor of images, number of images, and size per image, \n",
    "    this function plots and prints the images in a uniform grid.\n",
    "    \"\"\"\n",
    "    img_unflat = img_tensor.detach().cpu().view(-1, *size)\n",
    "    img_grid = make_grid(img_unflat[:num_imgs], nrow=5)\n",
    "    plt.imshow(img_grid.permute(1,2,0).squeeze())\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results):\n",
    "    \"\"\"\n",
    "    results is dictionary with keys: \"gen_train_loss\", \"gen_test_loss\", \n",
    "        \"disc_train_loss\", \"disc_test_loss\", \"gen_train_acc\", \"gen_test_acc\", \n",
    "        \"disc_train_acc\", \"disc_test_acc\".\n",
    "    This function plots the train and test losses and accuracies.\n",
    "\n",
    "    However, for now, we'll only plot the train losses for the generator and discriminator.\n",
    "    \"\"\"\n",
    "    plt.plot(results[\"gen_train_loss\"], label=\"Generator train loss\")\n",
    "    plt.plot(results[\"disc_train_loss\"], label=\"Discriminator train loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_dim):\n",
    "        \"\"\"\n",
    "        - param img_dim: dimension of image (eg: 28x28x1 = 784 for \n",
    "            grayscale MNIST images)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(img_dim, 128),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()   # output between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use leaky ReLU instead of ReLU?**\n",
    "\n",
    "We use leaky ReLU to prevent the \"dying ReLU\" problem, which refers to the phenomenon where the parameters stop changing due to consistently negative values passed to a ReLU, which result in a zero gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_dim):\n",
    "        \"\"\"\n",
    "        - param z_dim: dimension of latent noise vector\n",
    "        - param img_dim: dimension of image (eg: 28x28x1 = 784 for \n",
    "            grayscale MNIST images)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, img_dim),\n",
    "            nn.Tanh()   # output between -1 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 3e-4           # Karpathy constant\n",
    "z_dim = 64          # latent noise dimension\n",
    "img_dim = 28*28*1   # 1 means grayscale image\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "display_step = 500   # after how many steps to display loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# we can use the actual mean and std of the MNIST dataset, ie,\n",
    "# transforms.Normalize((0.1307,), (0.3081,))\n",
    "# but these don't help in model convergence\n",
    "\n",
    "train_dataset = MNIST(root=\"dataset/\", transform=transformations, download=True, train=True)\n",
    "test_dataset = MNIST(root=\"dataset/\", transform=transformations, download=True, train=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn():\n",
    "    for item in train_loader:\n",
    "        print(len(item))\n",
    "        print(item[0].shape, item[1].shape)\n",
    "        break\n",
    "\n",
    "fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Discriminator(img_dim).to(device)\n",
    "gen = Generator(z_dim, img_dim).to(device)\n",
    "\n",
    "# fixed_noise is the latent noise vector\n",
    "# torch.randn generates random numbers from a normal distribution\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
    "\n",
    "# separate optimizers for generator and discriminator\n",
    "optim_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "optim_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.BCELoss()  # binary cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importance of the (latent) noise vector**\n",
    "\n",
    "The noise vector is a key component of GANs. It is a random vector that is used as input to the generator. The generator uses this noise vector to generate fake images. The noise vector is important because it allows the generator to generate a wide variety of images. If the noise vector was not used, the generator would only be able to generate a single image.\n",
    "\n",
    "To create the noise vector, we use `torch.randn` to sample random numbers from the normal distribution, ie, \n",
    "\n",
    "`torch.randn((batch_size, z_dim)).to(device)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall: \n",
    "\n",
    "The training of the discriminator was to ***maximize*** the following:\n",
    "\n",
    "$$\\text{log}(D(\\text{real\\_img})) \\; + \\; \\text{log}(1 - D(G(z)))$$\n",
    "\n",
    ", where:\n",
    "\n",
    "- $D$ is the discriminator\n",
    "- $G$ is the generator\n",
    "- $z$ is the latent noise vector\n",
    "- $\\text{real\\_img}$ is the real image\n",
    "- $G(z)$ is the image generated by the generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A note about using BCELoss** \n",
    "\n",
    "The formula for BCELoss in PyTorch is:\n",
    "\n",
    "$$\\text{BCELoss} = -w_n [y_i \\cdot \\text{log}(x_i) + (1 - y_i) \\cdot \\text{log}(1 - x_i)]$$\n",
    "\n",
    "We will set $w_n = 1$ for now, so no need to worry about that. The formula for BCELoss becomes:\n",
    "\n",
    "$$\\text{BCELoss} = -[y_i \\cdot \\text{log}(x_i) + (1 - y_i) \\cdot \\text{log}(1 - x_i)]$$\n",
    "\n",
    "Notice, the negative sign at the beginning. We will minimize this BCE loss, which is the same as maximizing the discriminator's loss.\n",
    "\n",
    "Our discriminator's loss is:\n",
    "\n",
    "$$\\text{log}(D(\\text{real\\_img})) \\; + \\; \\text{log}(1 - D(G(z)))$$\n",
    "\n",
    "So, in the BCELoss formula, if we set $y_i = 1$ and $x_i = D(\\text{real\\_img})$, we get:\n",
    "\n",
    "$$-[\\text{log}(D(\\text{real\\_img}))]$$\n",
    "\n",
    "We can get the above term by using `criterion(disc_real, torch.ones_like(disc_real))`\n",
    ", where: \n",
    "- `disc_real` is $D(\\text{real\\_img})$\n",
    "- `criterion` is the `BCELoss` function\n",
    "\n",
    "This was the first term in the discriminator's loss. Now, before moving to the second term, there's one thing to note. We passed `torch.ones_like(disc_real)` instead of `torch.ones(disc_real)`. Why? Because, if we use `torch.ones`, we'll need to pass the device to it, ie, `torch.ones(disc_real.size(0), device=device)`. But, we can avoid this by using `torch.ones_like(disc_real)`, which will create a tensor of ones with the same shape as `disc_real`.\n",
    "\n",
    "\n",
    "For the second term, if we set $y_i = 0$ and $x_i = D(G(z))$, we get:\n",
    "\n",
    "$$-[\\text{log}(1 - D(G(z)))]$$\n",
    "\n",
    "Now, if we add these two terms, we get the discriminator's loss, ie,\n",
    "\n",
    "$$-[\\text{log}(D(\\text{real\\_img})) \\; + \\; \\text{log}(1 - D(G(z)))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to re-use the fake images generated by the generator, ie, `fake_img = gen(noise)` or mathematically, $G(z)$, but when we call `lossD.backward()`, the gradients are cleared from memory to save space. This means that we will need to re-generate the fake images, which is computationally expensive. We have 2 options to solve this problem:\n",
    "1. We can detach the fake images from the computational graph by calling `disc_fake_preds = disc(fake_images.detach())`.\n",
    "2. We can call `disc_loss.backward(retain_graph=True)` to save the gradients for the generator.\n",
    "\n",
    "Both the options are equivalent, and going ahead with either of them is fine. But, we'll use both of them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disc_loss(gen, disc, criterion, real, num_images, z_dim, device):\n",
    "    \"\"\"\n",
    "    Returns the loss of the discriminator.\n",
    "    Parameters:\n",
    "        - gen: the generator model, which returns an image given \n",
    "               z-dimensional noise\n",
    "        - disc: the discriminator model, which returns a single-dimensional \n",
    "                prediction of real/fake\n",
    "        - criterion: the loss function, which should be used to compare \n",
    "                     the discriminator's predictions to the ground truth \n",
    "                     reality of the images (e.g. fake = 0, real = 1)\n",
    "        - real: a batch of real images\n",
    "        - num_images: the number of images the generator should produce, \n",
    "                      which is also the length of the real images\n",
    "        - z_dim: the dimension of the noise vector, a scalar\n",
    "        - device: the device type (eg: cuda or cpu)\n",
    "    Returns:\n",
    "        disc_loss: a torch scalar loss value for the current batch\n",
    "\n",
    "    The following is the mathematical formula for the discriminator loss:\n",
    "        max(log(D(x)) + log(1 - D(G(z))))\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1) Create a noise vector and generate a batch (ie, num_images) of fake images.\n",
    "    noise_vector = torch.randn(num_images, z_dim).to(device)  # z\n",
    "    fake_images = gen(noise_vector)                           # G(z)\n",
    "\n",
    "    # 2) Get the discriminator's prediction of the fake image \n",
    "    #    and calculate the loss. Don't forget to detach the generator!\n",
    "    #    (Remember the loss function you set earlier -- criterion. You need a \n",
    "    #    'ground truth' tensor in order to calculate the loss. \n",
    "    #    For example, a ground truth tensor for a fake image is all zeros.)\n",
    "    disc_fake_preds = disc(fake_images.detach())                   # D(G(z))\n",
    "    disc_fake_loss = criterion(disc_fake_preds, \n",
    "                               torch.zeros_like(disc_fake_preds))  # log(1 - D(G(z)))\n",
    "    \n",
    "    # 3) Get the discriminator's prediction of the real image and calculate the loss.\n",
    "    disc_real_preds = disc(real)                                   # D(x)\n",
    "    disc_real_loss = criterion(disc_real_preds, \n",
    "                               torch.ones_like(disc_real_preds))   # log(D(x))\n",
    "\n",
    "    # 4) Calculate the discriminator's loss by averaging the real and fake loss\n",
    "    #    and set it to disc_loss.\n",
    "    disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "    \n",
    "    return disc_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall: \n",
    "\n",
    "The training of the generator was to ***minimize*** the following:\n",
    "\n",
    "$$\\text{log}(1 - D(G(z)))$$\n",
    "\n",
    "However, this causes the vanishing gradient problem, which leads to slower training, and sometimes even no training. To solve this, we can use an equivalent form of the above, which is to **maximize** the following:\n",
    "\n",
    "$$\\text{log}(D(G(z)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the BCELoss for the generator in a similar fashion as we did for the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_loss(gen, disc, criterion, num_images, z_dim, device):\n",
    "    \"\"\"\n",
    "    Returns the loss of the generator.\n",
    "    Parameters:\n",
    "        - gen: the generator model, which returns an image given \n",
    "               z-dimensional noise\n",
    "        - disc: the discriminator model, which returns a single-dimensional \n",
    "                prediction of real/fake\n",
    "        - criterion: the loss function, which should be used to compare \n",
    "                     the discriminator's predictions to the ground truth \n",
    "                     reality of the images (e.g. fake = 0, real = 1)\n",
    "        - num_images: the number of images the generator should produce, \n",
    "                      which is also the length of the real images\n",
    "        - z_dim: the dimension of the noise vector, a scalar\n",
    "        - device: the device type (eg: cuda or cpu)\n",
    "    Returns:\n",
    "        gen_loss: a torch scalar loss value for the current batch\n",
    "\n",
    "    The following is the mathematical formula for the generator loss:\n",
    "        max(log(D(G(z))))\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Create noise vectors and generate a batch of fake images.\n",
    "    noise_vector = torch.randn(num_images, z_dim).to(device)  # z\n",
    "    fake_images = gen(noise_vector)                           # G(z)\n",
    "    \n",
    "    # 2) Get the discriminator's prediction of the fake image.\n",
    "    disc_fake_preds = disc(fake_images)                       # D(G(z))\n",
    "    \n",
    "    # 3) Calculate the generator's loss. Remember the generator wants\n",
    "    #    the discriminator to think that its fake images are real\n",
    "    gen_loss = criterion(disc_fake_preds, \n",
    "                         torch.ones_like(disc_fake_preds))    # log(D(G(z)))\n",
    "\n",
    "    return gen_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A note about GAN training**\n",
    "\n",
    "The training of GANs is a bit tricky. The discriminator and the generator are trained alternately. The discriminator is trained first, and then the generator is trained. This process is repeated for a fixed number of iterations. The discriminator is trained to catch the generator when it generates fake images, and the generator is trained to fool the discriminator into thinking that the fake images are real.\n",
    "\n",
    "For each epoch, we will process the entire dataset in batches. For every batch, we will need to update the discriminator and generator using their loss. Batches are sets of images that will be predicted on before the loss functions are calculated (instead of calculating the loss function after each image). Note that you may see a loss to be greater than 1, this is okay since binary cross entropy loss can be any positive number for a sufficiently confident wrong guess.\n",
    "\n",
    "Itâ€™s also often the case that the discriminator will outperform the generator, especially at the start, because its job is easier. It's important that neither one gets too good (that is, near-perfect accuracy), which would cause the entire model to stop learning. Balancing the two models is actually remarkably hard to do in a standard GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "def fn():\n",
    "    noise = torch.randn(batch_size, z_dim).to(device)  # z\n",
    "    fake_img = gen(noise)\n",
    "    disc_fake = disc(fake_img).view(-1)\n",
    "    print(torch.zeros_like(disc_fake))\n",
    "    print(torch.zeros(disc_fake.size(0), device=device))\n",
    "\n",
    "fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_step = 0\n",
    "mean_generator_loss = 0\n",
    "mean_discriminator_loss = 0\n",
    "results = {\n",
    "    \"gen_train_loss\": [],\n",
    "    \"disc_train_loss\": [],\n",
    "}\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    \n",
    "    # we iterate over the training dataloader\n",
    "    # we only need the images, and not the labels\n",
    "    for real_img, _ in train_loader:\n",
    "        \n",
    "        curr_batch_size = len(real_img)\n",
    "        # Flatten the batch of real images\n",
    "        real_img = real_img.view(curr_batch_size, -1).to(device)\n",
    "\n",
    "        # Update discriminator (Notice that we first train the discriminator)\n",
    "        # Zero out the gradients before backpropagation\n",
    "        optim_disc.zero_grad()\n",
    "        # Calculate the discriminator loss\n",
    "        disc_loss = get_disc_loss(gen, disc, criterion, real_img, curr_batch_size, z_dim, device)\n",
    "        # Update gradients\n",
    "        disc_loss.backward(retain_graph=True)  # we need to re-use the gradients for the generator\n",
    "        # Update optimizer\n",
    "        optim_disc.step()\n",
    "\n",
    "        # Update generator\n",
    "        # Zero out the gradients before backpropagation\n",
    "        optim_gen.zero_grad()\n",
    "        # Calculate the generator loss\n",
    "        gen_loss = get_gen_loss(gen, disc, criterion, curr_batch_size, z_dim, device)\n",
    "        # Update gradients\n",
    "        gen_loss.backward()   # we have re-used the gradients for the generator, so no need to save the gradients\n",
    "        # Update optimizer\n",
    "        optim_gen.step()\n",
    "\n",
    "        # Keep track of the average discriminator loss\n",
    "        mean_discriminator_loss += disc_loss.item()\n",
    "        # Keep track of the average generator loss\n",
    "        mean_generator_loss += gen_loss.item()\n",
    "\n",
    "        # Visualization code\n",
    "        if current_step % display_step == 0 and current_step > 0:\n",
    "            mean_discriminator_loss = mean_discriminator_loss / display_step\n",
    "            mean_generator_loss = mean_generator_loss / display_step\n",
    "            results[\"gen_train_loss\"].append(mean_generator_loss)\n",
    "            results[\"disc_train_loss\"].append(mean_discriminator_loss)\n",
    "            print(f\"Step {current_step}: Generator loss: {mean_generator_loss}, Discriminator loss: {mean_discriminator_loss}\")\n",
    "            fake_noise = torch.randn(curr_batch_size, z_dim).to(device)\n",
    "            fake_img = gen(fake_noise)\n",
    "            plot_images(fake_img)\n",
    "            plot_images(real_img)\n",
    "            mean_generator_loss = 0\n",
    "            mean_discriminator_loss = 0\n",
    "        current_step += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
