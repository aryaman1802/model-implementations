{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Generative Adversarial Network (CGAN)\n",
    "\n",
    "Paper:\n",
    "- [Conditional Generative Adversarial Nets](https://arxiv.org/pdf/1411.1784)\n",
    "\n",
    "Helpful Resources:\n",
    "- [Aladdin Persson's playlist on GANs](https://youtube.com/playlist?list=PLhhyoLH6IjfwIp8bZnzX8QR30TRcHO8Va&si=8ooImkbbXhCUC1xB)\n",
    "- [GANs specialization on Coursera](https://www.coursera.org/specializations/generative-adversarial-networks-gans)  $\\; \\rightarrow \\;$ most of the content of this notebook has been borrowed from this course\n",
    "- [Stanford's Deep Generative Models playlist](https://youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8&si=N_TpTe1bPIhte-t8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook just includes the implementation of the CGAN model and its training loop. The results are not shown here.\n",
    "\n",
    "Feel free to check the results on my Kaggle notebook: https://www.kaggle.com/code/aryamanbansal/conditional_gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and some helpful utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torchinfo import summary\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import gc\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Imports done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions created!\n"
     ]
    }
   ],
   "source": [
    "def get_torch_version():\n",
    "    \"\"\"\n",
    "    Returns the version of PyTorch installed.\n",
    "    \"\"\"\n",
    "    torch_version = torch.__version__.split(\"+\")[0]\n",
    "    torch_number = torch_version.split(\".\")[:2]\n",
    "    torch_number_float = torch_number[0] + \".\" + torch_number[1]\n",
    "    torch_number_float = float(torch_number_float)\n",
    "    return torch_number_float\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Seeds basic parameters for reproducibility of results\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        if get_torch_version() <= 1.7:\n",
    "            torch.set_deterministic(True)\n",
    "        else:\n",
    "            torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "def set_scheduler(scheduler, results, scheduler_on):\n",
    "    \"\"\"\n",
    "    Makes the neccessary updates to the scheduler.\n",
    "    \n",
    "    Parameters:\n",
    "    - scheduler: torch.optim.lr_scheduler, the scheduler to update.\n",
    "    - results: dict, the results dictionary containing the training and test metric values.\n",
    "        Keys in the results dictionary are: \"gen_train_loss\", \"gen_val_loss\", \n",
    "                                            \"disc_train_loss\", \"disc_val_loss\".\n",
    "    - scheduler_on: str, the metric to use for the scheduler update.\n",
    "    \"\"\"\n",
    "    if scheduler_on == \"gen_val_loss\":\n",
    "        scheduler.step(results[\"gen_val_loss\"][-1])\n",
    "    if scheduler_on == \"disc_val_loss\":\n",
    "        scheduler.step(results[\"disc_val_loss\"][-1])\n",
    "    elif scheduler_on == \"gen_train_loss\":\n",
    "        scheduler.step(results[\"gen_train_loss\"][-1])\n",
    "    elif scheduler_on == \"disc_train_loss\":\n",
    "        scheduler.step(results[\"disc_train_loss\"][-1])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid `scheduler_on` choice.\")\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "def save_model_info(path: str, model, model_name, optimizer, optimizer_name, \n",
    "                    scheduler=None, scheduler_name=\"\"):\n",
    "    \"\"\"\n",
    "    Saves the model and optimizer weights to the specified path.\n",
    "\n",
    "    Parameters:\n",
    "    - path: str, the path to the directory to save the model and optimizer weights.\n",
    "    - model: nn.Module, the model to save.\n",
    "    - model_name: str, the name of the model weights file.\n",
    "    - optimizer: torch.optim, the optimizer to save.\n",
    "    - optimizer_name: str, the name of the optimizer weights file.\n",
    "    - scheduler: torch.optim.lr_scheduler, the scheduler to save.\n",
    "    - scheduler_name: str, the name of the scheduler weights file.\n",
    "\n",
    "    Free advice:\n",
    "    A good practice is to transfer the model to the CPU before calling torch.save as this \n",
    "    will save tensors as CPU tensors and not as CUDA tensors. This will help in loading\n",
    "    the model onto any machine, whether it contains CUDA capabilities or not.\n",
    "    \"\"\"\n",
    "    model.to(\"cpu\")\n",
    "    torch.save(model.state_dict(), os.path.join(path,model_name))\n",
    "    torch.save(optimizer.state_dict(), os.path.join(path,optimizer_name))\n",
    "    if scheduler is not None:\n",
    "        torch.save(scheduler.state_dict(), os.path.join(path,scheduler_name))    \n",
    "    print(\"Model info saved!\")\n",
    "    \n",
    "    \n",
    "def load_model_info(path, device, model, model_name, optimizer, optimizer_name, \n",
    "                    scheduler=None, scheduler_name=\"\"):\n",
    "    \"\"\"\n",
    "    Loads the model and optimizer weights from the specified path.\n",
    "\n",
    "    Parameters:\n",
    "    - path: str, the path to the directory containing the model and optimizer weights.\n",
    "    - device: str, the device to load the model and optimizer weights onto.\n",
    "    - model: nn.Module, the model to load the weights into.\n",
    "    - model_name: str, the name of the model weights file.\n",
    "    - optimizer: torch.optim, the optimizer to load the weights into.\n",
    "    - optimizer_name: str, the name of the optimizer weights file.\n",
    "    - scheduler: torch.optim.lr_scheduler, the scheduler to load the weights into.\n",
    "    - scheduler_name: str, the name of the scheduler weights file.\n",
    "    \"\"\"\n",
    "    model.load_state_dict(torch.load(os.path.join(path,model_name)))\n",
    "    model.to(device)\n",
    "    optimizer.load_state_dict(torch.load(os.path.join(path,optimizer_name)))\n",
    "    if scheduler is not None:\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(path,scheduler_name)))\n",
    "    print(\"Model info loaded!\")\n",
    "    \n",
    "    \n",
    "def get_current_time():\n",
    "    \"\"\"Returns the current time in Toronto.\"\"\"\n",
    "    now = datetime.now(pytz.timezone('Canada/Eastern'))\n",
    "    current_time = now.strftime(\"%d_%m_%Y__%H_%M_%S\")\n",
    "    return current_time\n",
    "\n",
    "\n",
    "def show_tensor_images(image_tensor, num_images=25, size=(1,28,28)):\n",
    "    \"\"\"\n",
    "    Function for visualizing images: Given a tensor of images, \n",
    "    number of images, and size per image, plots and prints the \n",
    "    images in an uniform grid.\n",
    "    \"\"\"\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_results(results):\n",
    "    \"\"\"\n",
    "    Plots the train and test losses of the generator and the discriminator.\n",
    "\n",
    "    results is dictionary with keys: \n",
    "        \"gen_train_loss\", \"gen_val_loss\", \"disc_train_loss\", \"disc_val_loss\".\n",
    "    \"\"\"\n",
    "    gen_train_loss = results[\"gen_train_loss\"]\n",
    "    gen_val_loss = results[\"gen_val_loss\"]\n",
    "    disc_train_loss = results[\"disc_train_loss\"]\n",
    "    disc_val_loss = results[\"disc_val_loss\"]\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "    ax[0].set_ylabel(\"Training Loss\")\n",
    "    ax[0].plot(gen_train_loss, label=\"generator\", color=\"orange\")\n",
    "    ax[0].plot(disc_train_loss, label=\"discriminator\", color=\"blue\")\n",
    "    ax[0].legend()\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    ax[1].set_ylabel(\"Validation Loss\")\n",
    "    ax[1].plot(gen_val_loss, label=\"generator\", color=\"orange\")\n",
    "    ax[1].plot(disc_val_loss, label=\"discriminator\", color=\"blue\")\n",
    "    ax[1].legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Utility functions created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some theory about CGANs\n",
    "\n",
    "Before we jump in to the code, let's understand the theory behind CGANs, and why we need them. \n",
    "\n",
    "Note: the following notes are heavily inspired from the [GANs specialization on Coursera](https://www.coursera.org/specializations/generative-adversarial-networks-gans).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Motivation**\n",
    "\n",
    "Let's first understand the difference between unconditional and conditional generation.\n",
    "\n",
    "Up till now, the models we built (simple GAN, DCGAN, and WGAN) were all unconditional GANs. This means that we input some random noise vector into the generator, and it generates a random image. So, we don't have any control over what our GAN is generating. In addition, we don't need the dataset to be labelled, as an unconditional GAN just needs the datapoints and not their corresponding labels.\n",
    "\n",
    "On the other hand, in conditional generation, we can control what our GAN generates. We can input some information to the generator, and it will generate an image based on that information. For example, we can input a label to the generator, and it will generate an image of that label. However, the generator can generator any variation of that label, and not just the exact same image of what you have in mind. Consider this analogy: we go to a vending machine and insert some money along with the itemcode of the item we want. The vending machine will give us the item corresponding to that itemcode, but we can't choose all the features (say, expiration date or manufacturing date) of that item. So, when using a conditional GAN, we input the label of the class we want, and the generator generates some example of that class. In addition, the dataset needs to be labelled, as we need to input the label to the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to make a GAN conditional?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The input to the generator of a CGAN.**\n",
    "\n",
    "We've already seen with unconditional generation that the generator needs a noise vector to produce random examples. For conditional generation, we also need a vector to tell the generator from which class the generated examples should come from. This new vector is called a class vector, which is usually a one-hot vector, which means that there are zeros in every position except for one position corresponding to the class we want. The input to the generator in a conditional GAN is actually a concatenated vector of both the noise vector and the class vector. Here, the class vector tells the generator which class to generate and the noise vector tells it what variation of that class to generate. Of course, since the noise vector is random, so we cannot control the exact variation of the class that the generator generates. If we use a different noise vector every time but the same class vector, the generator will generate different variations of the same class. \n",
    "\n",
    "![\"cgan_gen_input\"](./imgs/cgan_gen_input.png \"cgan_gen_input\")\n",
    "\n",
    "Source: [Course 1 Week 4 of GANs specialization on Coursera](https://www.coursera.org/learn/build-basic-generative-adversarial-networks-gans?specialization=generative-adversarial-networks-gans)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The input to the discriminator of a CGAN.**\n",
    "\n",
    "In the case of unconditional GANs, the input to the discriminator was just the image. To make the GAN conditional, we need some way of conveying the class information to discriminator. We saw above that the input to the generator was a concatenated vector of the noise vector and the class vector, where the class vector was a one-hot vector. Since we're feeding the discriminator an image (which can either be RGB or grayscale) and not just a noise vector, so we'll need to adjust the class vector to be the same size as the image. We can do this by conveying class information in the form of additional channels, where all the newly added channels are matrices filled with zeros except for one channel corresponding to the class we want that is filled with ones. We can see how this can be done as in the below diagram.\n",
    "\n",
    "![\"cgan_disc_input\"](./imgs/cgan_disc_input.png \"cgan_disc_input\")\n",
    "\n",
    "Source: [Course 1 Week 4 of GANs specialization on Coursera](https://www.coursera.org/learn/build-basic-generative-adversarial-networks-gans?specialization=generative-adversarial-networks-gans)\n",
    "\n",
    "There are many other less space consuming ways of conveying class information to the discriminator, like compressing the class information in another format. We can even create a separate neural network head to do that for us, which would be wise if we had many different classes. However, when the number of classes is around 10 or so, then it's alright to just use the above described method.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **How does the generator learn to generate the correct class?**\n",
    "\n",
    "The discriminator receives both the image and its corresponding class label and will classify the images based on if they look like real images from that specific class or not. For example, consider that we're building a CGAN to generate different species of dogs. Now, if the generator is asked to generate an image of a golden retriever, but if it generates an image of a poodle (no matter how realistic-looking that image might be), then the discriminator will give a very low probability of the generated image being real as it's not a golden retriever. So, the generator will learn to generate images that belong to whatever class is input to it, and not just any random image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alright enough theory, let's write some code now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SomeDataClass:\n",
    "    def __init__(self, batch_size=128):\n",
    "        self.batch_size = batch_size\n",
    "        self.transformations = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "            ])\n",
    "        self.train_dataset = MNIST(root=\"dataset/\", \n",
    "                              transform=self.transformations, \n",
    "                              download=True, train=True)\n",
    "        self.valid_dataset = MNIST(root=\"dataset/\", \n",
    "                                  transform=self.transformations, \n",
    "                                  download=True, train=False)\n",
    "\n",
    "    def get_trainloader(self):\n",
    "        \"\"\"Returns the training dataloader.\"\"\"\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "    def get_validloader(self):\n",
    "        \"\"\"Returns the validation dataloader.\"\"\"\n",
    "        return DataLoader(self.valid_dataset, batch_size=self.batch_size, \n",
    "                          shuffle=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([128, 1, 28, 28]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "def fn():\n",
    "    data = SomeDataClass()\n",
    "    train_loader = data.get_trainloader()\n",
    "    for item in train_loader:\n",
    "        print(len(item))\n",
    "        print(item[0].shape, item[1].shape)\n",
    "        break\n",
    "\n",
    "fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no major changes in the architectures of the generator and the discriminator of CGAN as compared to that of DCGAN. The only change is that we now also pass the class information to both the models. This means that:\n",
    "- the generator now takes in both the noise vector and the class vector as input.\n",
    "- the discriminator now takes in the image along with the class matrices as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architectures and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim=10, img_channel=1, hidden_dim=64):\n",
    "        \"\"\"\n",
    "        The Generator class.\n",
    "\n",
    "        Parameters:\n",
    "            - input_dim: the dimension of the input vector, a scalar\n",
    "            - img_channel: the number of channels of the output image, a scalar\n",
    "                (MNIST is grayscale, so default value is img_channel=1)\n",
    "            - hidden_dim: the inner dimension, a scalar\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.gen = nn.Sequential(\n",
    "            self.gen_block(input_dim, hidden_dim*4),\n",
    "            self.gen_block(hidden_dim*4, hidden_dim*2, kernel_size=4, stride=1),\n",
    "            self.gen_block(hidden_dim*2, hidden_dim),\n",
    "            self.gen_block(hidden_dim, img_channel, kernel_size=4, final_layer=True)\n",
    "        )\n",
    "\n",
    "    def gen_block(self, in_channel, out_channel, kernel_size=3, stride=2, \n",
    "                  final_layer=False):\n",
    "        \"\"\"\n",
    "        Returns the layers of a generator block.\n",
    "\n",
    "        Parameters:\n",
    "        - in_channel: the number of channels in the input, a scalar\n",
    "        - out_channel: the number of channels in the output, a scalar\n",
    "        - kernel_size: the size of the kernel, a scalar\n",
    "        - stride: the stride of the kernel, a scalar\n",
    "        - final_layer: a boolean, True if this is the final layer and False otherwise\n",
    "        \"\"\"\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channel, out_channel, \n",
    "                                   kernel_size=kernel_size, stride=stride),\n",
    "                nn.BatchNorm2d(out_channel),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channel, out_channel, \n",
    "                                   kernel_size=kernel_size, stride=stride),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "    \n",
    "    def forward(self, noise):\n",
    "        \"\"\"\n",
    "        Given a noise tensor, returns the generated image.\n",
    "        \"\"\"\n",
    "        x = noise.view(len(noise), self.input_dim, 1, 1)\n",
    "        return self.gen(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channel=1, hidden_dim=16):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - img_channel: the number of channels of the input image, a scalar\n",
    "            (MNIST is grayscale, so default value is img_channel=1)\n",
    "        - hidden_dim: the inner dimension, a scalar\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            self.disc_block(img_channel, hidden_dim),\n",
    "            self.disc_block(hidden_dim, hidden_dim*2),\n",
    "            self.disc_block(hidden_dim*2, 1, final_layer=True)\n",
    "        )\n",
    "\n",
    "    def disc_block(self, in_channel, out_channel, kernel_size=4, stride=2,\n",
    "                   final_layer=False):\n",
    "          \"\"\"\n",
    "          Returns the layers of a discriminator block.\n",
    "    \n",
    "          Parameters:\n",
    "          - in_channel: the number of channels in the input, a scalar\n",
    "          - out_channel: the number of channels in the output, a scalar\n",
    "          - kernel_size: the size of the kernel, a scalar\n",
    "          - stride: the stride of the kernel, a scalar\n",
    "          - final_layer: a boolean, True if this is the final layer and False otherwise\n",
    "          \"\"\"\n",
    "          if not final_layer:\n",
    "                return nn.Sequential(\n",
    "                 nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, \n",
    "                           stride=stride),\n",
    "                 nn.BatchNorm2d(out_channel),\n",
    "                 nn.LeakyReLU(0.2)\n",
    "                )\n",
    "          else:\n",
    "                return nn.Sequential(\n",
    "                 nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, \n",
    "                           stride=stride),\n",
    "                )\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Given an image tensor, returns a 1-dimension tensor \n",
    "        representing fake/real.\n",
    "        Parameters:\n",
    "            image: a flattened image tensor\n",
    "        \"\"\"\n",
    "        disc_pred = self.disc(image)\n",
    "        return disc_pred.view(len(disc_pred), -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class information\n",
    "\n",
    "Now, we need a way to pass class information to the generator. We do this by creating a class vector, which is a one-hot vector, where the length of the vector is the number of classes and each index represents a class. The vector is filled with zeroes except at one index, which corresponds to the class label, where we put a 1. For example, if we have 10 classes, then the class vector for the class label 2 (note: class labels start from 0 till n-1, where n is the number of classes) would be `[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first arugment is tensor containing the different class labels\n",
    "# the second argument is the number of classes\n",
    "F.one_hot(torch.tensor([0,1,2,3,4,5,6,7,8,9]), num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so now that we have figured out how to create a class vector, let's now concatenate the class vector with the noise vector to create the input vector that we finally send to the generator. In addition, we must that both the noise vector and the class vector are of the same type, ie, float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_vectors(x, y):\n",
    "    \"\"\"\n",
    "    Function for combining two vectors with shapes (n_samples, ?) \n",
    "    and (n_samples, ?).\n",
    "    Note: ? as the second dimension simply means that we do not \n",
    "    need to know the 2nd dimension of the 2 vectors x and y.  \n",
    "    Parameters:\n",
    "      x: (n_samples, ?) the first vector. This should typically be\n",
    "      the noise vector with the shape (n_samples, z_dim).\n",
    "      y: (n_samples, ?) the second vector. This should typically be\n",
    "      the class vector with the shape (n_samples, n_classes).\n",
    "    \"\"\"\n",
    "    # when concatenating x and y vectors, we need to ensure that \n",
    "    # both of them are floats, no matter their previous datatype\n",
    "    return torch.cat((x.float(), y.float()), dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we previously mentioned, the input the discriminator is the image along with the class channel matrices. The class channel matrices are matrices filled with zeros except for one channel corresponding to the class we want that is filled with ones. We can see how this can be done as in the below diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_dim(z_dim, dataset_shape, n_classes):\n",
    "    \"\"\"\n",
    "    Function for getting the size of the conditional input \n",
    "    dimensions from z_dim, the image shape, and no. of classes.\n",
    "    Parameters:\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        dataset_shape: the shape of each dataset image as (C, W, H).\n",
    "            For eg: the shape of images in the MNIST dataset is (1,28,28).\n",
    "        n_classes: the total number of classes in the dataset.\n",
    "            For eg: the no. of classes in the MNIST dataset is 10.\n",
    "    Returns: \n",
    "        generator_input_dim: the input dimensionality of the conditional generator, \n",
    "                          which takes the noise and class vectors\n",
    "        discriminator_im_chan: the number of input channels to the discriminator\n",
    "                            (e.g. C x 28 x 28 for MNIST)\n",
    "    \"\"\"\n",
    "    gen_input_dim = z_dim + n_classes\n",
    "    disc_img_channels = dataset_shape[0] + n_classes\n",
    "    return gen_input_dim, disc_img_channels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss functions and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_loss(disc_fake_pred, criterion):\n",
    "    \"\"\"\n",
    "    Return the loss of a generator given the discriminator's scores of \n",
    "    the generator's fake images.\n",
    "    Parameters:\n",
    "        - disc_fake_pred: the discriminator's scores of the fake images\n",
    "        - criterion: the loss function (BCEWithLogitsLoss)\n",
    "    Returns:\n",
    "        gen_loss: a scalar loss value for the current batch of the generator\n",
    "    \"\"\"\n",
    "    gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "    return gen_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disc_loss(disc_fake_pred, disc_real_pred, criterion):\n",
    "    \"\"\"\n",
    "    Return the loss of a discriminator given the discriminator's scores \n",
    "    for fake and real images.\n",
    "    Parameters:\n",
    "        - disc_fake_pred: the discriminator's scores of the fake images\n",
    "        - disc_real_pred: the discriminator's scores of the real images\n",
    "        - criterion: the loss function (BCEWithLogitsLoss) \n",
    "    Returns:\n",
    "        disc_loss: a scalar for the discriminator's loss, accounting \n",
    "            for the relevant factors\n",
    "    \"\"\"\n",
    "    disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "    disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "    disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "    return disc_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_discriminator(real, labels, n_classes, dataset_shape, \n",
    "                         device, discriminator, generator, criterion,\n",
    "                         z_dim, n_samples, disc_optim=None, test=False):\n",
    "    \"\"\"\"\n",
    "    Updates the discriminator model.\n",
    "    Parameters:\n",
    "        - real: the real images\n",
    "        - labels: the labels for the real images\n",
    "        - n_classes: the no. of classes in the dataset\n",
    "        - dataset_shape: the shape of the dataset images\n",
    "        - device: the device to train the model on\n",
    "        - discriminator: the discriminator model\n",
    "        - generator: the generator model\n",
    "        - criterion: the loss function (BCELossWithLogitsLoss)\n",
    "        - disc_optim: the optimizer for the discriminator\n",
    "        - z_dim: the dimension of the noise vector\n",
    "        - n_samples: the number of samples to generate\n",
    "        - test: a boolean, True if the function is called during validation\n",
    "    Returns: average loss for the discriminator\n",
    "    \"\"\"\n",
    "    if not test:\n",
    "        # zero out the discriminator gradients\n",
    "        disc_optim.zero_grad()\n",
    "\n",
    "        # generate the noise vector\n",
    "        fake_noise = torch.randn(n_samples, z_dim, device=device)\n",
    "\n",
    "        # class vector for the generator\n",
    "        one_hot_labels = F.one_hot(labels.to(device), n_classes)\n",
    "\n",
    "        # add extra dimensions to the one-hot labels\n",
    "        img_ohl = one_hot_labels[:, :, None, None]\n",
    "        \n",
    "        # class matrices for the discriminator\n",
    "        img_ohl = img_ohl.repeat(1, 1, dataset_shape[1], dataset_shape[2])\n",
    "        \n",
    "        # combine the noise vectors and the class vector \n",
    "        # this is the new input for the generator\n",
    "        noise_class_vector = combine_vectors(fake_noise, one_hot_labels)\n",
    "        \n",
    "        # generate fake images by conditioned fake noise, ie, \n",
    "        # noise and class vector\n",
    "        fake = generator(noise_class_vector)\n",
    "        \n",
    "        # add class matrices to the fake images\n",
    "        fake_img_class = combine_vectors(fake, img_ohl)\n",
    "\n",
    "        # add class matrices to the real images\n",
    "        real_img_class = combine_vectors(real, img_ohl)\n",
    "        \n",
    "        # get the discriminator scores for conditioned fake images\n",
    "        disc_fake_pred = discriminator(fake_img_class.detach())\n",
    "        \n",
    "        # get the discriminator scores for conditioned real images\n",
    "        disc_real_pred = discriminator(real_img_class)\n",
    "        \n",
    "        # get the discriminator loss\n",
    "        disc_loss = get_disc_loss(disc_fake_pred, disc_real_pred, criterion)\n",
    "        \n",
    "        # Update gradients\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        \n",
    "        # Update optimizer\n",
    "        disc_optim.step()\n",
    "    else:\n",
    "        # generate the noise vector\n",
    "        fake_noise = torch.randn(n_samples, z_dim, device=device)\n",
    "\n",
    "        # class vector for the generator\n",
    "        one_hot_labels = F.one_hot(labels.to(device), n_classes)\n",
    "\n",
    "        # add extra dimensions to the one-hot labels\n",
    "        img_ohl = one_hot_labels[:, :, None, None]\n",
    "        \n",
    "        # class matrices for the discriminator\n",
    "        img_ohl = img_ohl.repeat(1, 1, dataset_shape[1], dataset_shape[2])\n",
    "        \n",
    "        # combine the noise vectors and the class vector \n",
    "        # this is the new input for the generator\n",
    "        noise_class_vector = combine_vectors(fake_noise, one_hot_labels)\n",
    "        \n",
    "        # generate fake images by conditioned fake noise, ie, \n",
    "        # noise and class vector\n",
    "        fake = generator(noise_class_vector)\n",
    "        \n",
    "        # add class matrices to the fake images\n",
    "        fake_img_class = combine_vectors(fake, img_ohl)\n",
    "\n",
    "        # add class matrices to the real images\n",
    "        real_img_class = combine_vectors(real, img_ohl)\n",
    "        \n",
    "        # get the discriminator scores for conditioned fake images\n",
    "        disc_fake_pred = discriminator(fake_img_class.detach())\n",
    "        \n",
    "        # get the discriminator scores for conditioned real images\n",
    "        disc_real_pred = discriminator(real_img_class)\n",
    "        \n",
    "        # get the discriminator loss\n",
    "        disc_loss = get_disc_loss(disc_fake_pred, disc_real_pred, criterion)\n",
    "    \n",
    "    return disc_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_generator(labels, n_classes, dataset_shape, device, discriminator, \n",
    "                     generator, criterion, z_dim, n_samples, gen_optim=None, \n",
    "                     test=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - labels: the labels for the real images\n",
    "        - n_classes: the no. of classes in the dataset\n",
    "        - dataset_shape: the shape of the dataset images\n",
    "        - device: the device to train the model on\n",
    "        - discriminator: the discriminator model\n",
    "        - generator: the generator model\n",
    "        - criterion: the loss function (BCELossWithLogitsLoss)\n",
    "        - gen_optim: the optimizer for the generator\n",
    "        - z_dim: the dimension of the noise vector\n",
    "        - n_samples: the number of samples to generate\n",
    "        - test: a boolean, True if the function is called during validation\n",
    "    Returns: generator loss\n",
    "    \"\"\"\n",
    "    if not test:\n",
    "        # zero out the generator gradients\n",
    "        gen_optim.zero_grad()\n",
    "\n",
    "        # generate the noise vector\n",
    "        fake_noise = torch.randn(n_samples, z_dim, device=device)\n",
    "        \n",
    "        # class vector for the generator\n",
    "        one_hot_labels = F.one_hot(labels.to(device), n_classes)\n",
    "\n",
    "        # add extra dimensions to the one-hot labels\n",
    "        img_ohl = one_hot_labels[:, :, None, None]\n",
    "        \n",
    "        # class matrices for the discriminator\n",
    "        img_ohl = img_ohl.repeat(1, 1, dataset_shape[1], dataset_shape[2])\n",
    "        \n",
    "        # combine the noise vectors and the class vector \n",
    "        # this is the new input for the generator\n",
    "        noise_class_vector = combine_vectors(fake_noise, one_hot_labels)\n",
    "        \n",
    "        # generate fake images by conditioned fake noise, ie, \n",
    "        # noise and class vector\n",
    "        fake = generator(noise_class_vector)\n",
    "\n",
    "        # add class matrices to the fake images\n",
    "        fake_img_class = combine_vectors(fake, img_ohl)\n",
    "        \n",
    "        # get the discriminator scores for conditioned fake images\n",
    "        disc_fake_pred = discriminator(fake_img_class)\n",
    "        \n",
    "        # get the generator loss\n",
    "        gen_loss = get_gen_loss(disc_fake_pred, criterion)\n",
    "        gen_loss.backward()\n",
    "        \n",
    "        # Update the weights\n",
    "        gen_optim.step()\n",
    "    else:\n",
    "        # generate the noise vector\n",
    "        fake_noise = torch.randn(n_samples, z_dim, device=device)\n",
    "        \n",
    "        # class vector for the generator\n",
    "        one_hot_labels = F.one_hot(labels.to(device), n_classes)\n",
    "\n",
    "        # add extra dimensions to the one-hot labels\n",
    "        img_ohl = one_hot_labels[:, :, None, None]\n",
    "        \n",
    "        # class matrices for the discriminator\n",
    "        img_ohl = img_ohl.repeat(1, 1, dataset_shape[1], dataset_shape[2])\n",
    "        \n",
    "        # combine the noise vectors and the class vector \n",
    "        # this is the new input for the generator\n",
    "        noise_class_vector = combine_vectors(fake_noise, one_hot_labels)\n",
    "        \n",
    "        # generate fake images by conditioned fake noise, ie, \n",
    "        # noise and class vector\n",
    "        fake = generator(noise_class_vector)\n",
    "\n",
    "        # add class matrices to the fake images\n",
    "        fake_img_class = combine_vectors(fake, img_ohl)\n",
    "        \n",
    "        # get the discriminator scores for conditioned fake images\n",
    "        disc_fake_pred = discriminator(fake_img_class)\n",
    "        \n",
    "        # get the generator loss\n",
    "        gen_loss = get_gen_loss(disc_fake_pred, criterion)\n",
    "        \n",
    "    return gen_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(train_dataloader, device, discriminator, \n",
    "               generator, criterion, disc_optim, gen_optim, \n",
    "               z_dim, dataset_shape, n_classes):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - train_dataloader: the training dataloader\n",
    "        - device: the device to train the model on\n",
    "        - discriminator: the discriminator model\n",
    "        - generator: the generator model\n",
    "        - criterion: the loss function (BCELossWithLogitsLoss)\n",
    "        - disc_optim: the optimizer for the discriminator\n",
    "        - gen_optim: the optimizer for the generator\n",
    "        - z_dim: the dimension of the noise vector\n",
    "        - dataset_shape: the shape of the dataset images\n",
    "        - n_classes: no. of classes in the dataset\n",
    "    \"\"\"\n",
    "    generator_losses_per_batch = []\n",
    "    discriminator_losses_per_batch = []\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    # We need both the real images and their labels\n",
    "    for real, labels in train_dataloader:\n",
    "        curr_batch_size = len(real)\n",
    "        real = real.to(device)\n",
    "\n",
    "        # Update the discriminator\n",
    "        disc_loss = update_discriminator(real, labels, n_classes, dataset_shape, device, \n",
    "                                         discriminator, generator, criterion, \n",
    "                                         z_dim, curr_batch_size, disc_optim)\n",
    "        discriminator_losses_per_batch.append([disc_loss])\n",
    "    \n",
    "        # Update the generator\n",
    "        gen_loss = update_generator(labels, n_classes, dataset_shape, device, discriminator, \n",
    "                                    generator, criterion, z_dim, curr_batch_size, gen_optim)\n",
    "        generator_losses_per_batch.append([gen_loss])\n",
    "\n",
    "    return np.mean(generator_losses_per_batch), np.mean(discriminator_losses_per_batch) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot_labels:\n",
      "tensor([[0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0]])\n",
      "\n",
      "one_hot_labels.shape:\ttorch.Size([8, 5])\n",
      "\n",
      "img_ohl.shape.shape:\ttorch.Size([8, 5, 1, 1])\n",
      "\n",
      "img_ohl.shape.shape:\ttorch.Size([8, 5, 28, 28])\n",
      "torch.Size([5, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "def fn():\n",
    "    labels = torch.tensor([1,2,0,1,4,0,2,3])\n",
    "    one_hot_labels = F.one_hot(labels, num_classes=5)\n",
    "    print(f\"one_hot_labels:\\n{one_hot_labels}\")\n",
    "    print(f\"\\none_hot_labels.shape:\\t{one_hot_labels.shape}\")\n",
    "    img_ohl = one_hot_labels[:, :, None, None]\n",
    "    print(f\"\\nimg_ohl.shape.shape:\\t{img_ohl.shape}\")\n",
    "    img_ohl = img_ohl.repeat(1, 1, 28, 28)\n",
    "    print(f\"\\nimg_ohl.shape.shape:\\t{img_ohl.shape}\")\n",
    "    print(img_ohl[0].shape)\n",
    "\n",
    "\n",
    "fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(valid_dataloader, device, discriminator, generator, \n",
    "             criterion, z_dim, dataset_shape, n_classes):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - valid_dataloader: the validation dataloader\n",
    "        - device: the device to train the model on\n",
    "        - discriminator: the discriminator model\n",
    "        - generator: the generator model\n",
    "        - criterion: the loss function (BCELossWithLogitsLoss)\n",
    "        - dataset_shape: the shape of the dataset images\n",
    "        - n_classes: no. of classes in the dataset\n",
    "    \"\"\"\n",
    "    generator_losses_per_batch = []\n",
    "    discriminator_losses_per_batch = []\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    # We need both the real images and their labels\n",
    "    for real, labels in valid_dataloader:\n",
    "        curr_batch_size = len(real)\n",
    "        real = real.to(device)\n",
    "    \n",
    "        # Update the discriminator\n",
    "        disc_loss = update_discriminator(real=real, labels=labels, n_classes=n_classes, \n",
    "                                         dataset_shape=dataset_shape, device=device, \n",
    "                                         discriminator=discriminator, generator=generator, \n",
    "                                         criterion=criterion, z_dim=z_dim, n_samples=curr_batch_size, \n",
    "                                         test=True)\n",
    "        discriminator_losses_per_batch.append([disc_loss]) \n",
    "    \n",
    "        # Update the generator\n",
    "        gen_loss = update_generator(labels=labels, n_classes=n_classes, \n",
    "                                    dataset_shape=dataset_shape, device=device, \n",
    "                                    discriminator=discriminator, \n",
    "                                    generator=generator, criterion=criterion, \n",
    "                                    z_dim=z_dim, n_samples=curr_batch_size, test=True)\n",
    "        generator_losses_per_batch.append([gen_loss])\n",
    "\n",
    "    return np.mean(generator_losses_per_batch), np.mean(discriminator_losses_per_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_fn(n_epochs, train_dataloader, valid_dataloader, device, \n",
    "                discriminator, generator, criterion, z_dim, disc_optim, \n",
    "                gen_optim, dataset_shape, n_classes):\n",
    "    \"\"\"\n",
    "    Entire training loop with the validation loop.\n",
    "    Parameters:\n",
    "        - n_epochs: the number of epochs to train the model\n",
    "        - train_dataloader: the DataLoader for the training data\n",
    "        - valid_dataloader: the DataLoader for the validation data\n",
    "        - device: the device to train the model on\n",
    "        - discriminator: the discriminator model\n",
    "        - generator: the generator model\n",
    "        - criterion: the loss function (BCELossWithLogitsLoss)\n",
    "        - z_dim: the dimension of the noise vector\n",
    "        - disc_optim: the optimizer for the discriminator\n",
    "        - gen_optim: the optimizer for the generator\n",
    "        - dataset_shape: the shape of the dataset images\n",
    "        - n_classes: no. of classes in the dataset\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"gen_train_loss\": [],\n",
    "        \"gen_val_loss\": [],\n",
    "        \"disc_train_loss\": [],\n",
    "        \"disc_val_loss\": []\n",
    "    }\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):   \n",
    "        # Train step\n",
    "        gen_train_loss, disc_train_loss = train_step(train_dataloader, device, \n",
    "                                                     discriminator, generator, \n",
    "                                                     criterion, disc_optim, \n",
    "                                                     gen_optim, z_dim,\n",
    "                                                     dataset_shape, \n",
    "                                                     n_classes)\n",
    "        results[\"gen_train_loss\"].append(gen_train_loss)\n",
    "        results[\"disc_train_loss\"].append(disc_train_loss)\n",
    "            \n",
    "        # Validation step\n",
    "        gen_val_loss, disc_val_loss = val_step(valid_dataloader, device, \n",
    "                                               discriminator, generator, \n",
    "                                               criterion, z_dim, dataset_shape, \n",
    "                                               n_classes)\n",
    "        results[\"gen_val_loss\"].append(gen_val_loss) \n",
    "        results[\"disc_val_loss\"].append(disc_val_loss)\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 2e-4           \n",
    "z_dim = 64            # latent noise dimension\n",
    "img_dim = 1           # 1 means grayscale image\n",
    "batch_size = 128      # no. of images in each forward and backward passes\n",
    "num_epochs = 200\n",
    "dataset_shape = (1, 28, 28)   # this is the shape of each image in the MNIST dataset\n",
    "n_classes = 10        # no. of classes in the MNIST dataset\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_input_dim, disc_img_channels = get_input_dim(z_dim, dataset_shape, n_classes)\n",
    "\n",
    "gen = Generator(input_dim=gen_input_dim).to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "disc = Discriminator(img_channel=disc_img_channels).to(device)\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
    "\n",
    "# Initialize the weights to the normal distribution\n",
    "# with mean 0 and standard deviation 0.02\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "gen = gen.apply(weights_init)\n",
    "disc = disc.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed_noise is the latent noise vector\n",
    "# torch.randn generates random numbers from a normal distribution\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SomeDataClass()\n",
    "train_loader = data.get_trainloader()\n",
    "valid_loader = data.get_validloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = training_fn(n_epochs=num_epochs, train_dataloader=train_loader, \n",
    "                      valid_dataloader=valid_loader, device=device, \n",
    "                      discriminator=disc, generator=gen, criterion=criterion,\n",
    "                      z_dim=z_dim, disc_optim=disc_opt, gen_optim=gen_opt,\n",
    "                      dataset_shape=dataset_shape, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the models' weights and Inferring on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_time = get_current_time()\n",
    "curr_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), curr_time)\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_info(path, model=disc, model_name=\"disc1.pt\", optimizer=disc_opt, \n",
    "                optimizer_name=\"disc_optim1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_info(path, model=gen, model_name=\"gen1.pt\", optimizer=gen_opt, \n",
    "                optimizer_name=\"gen_optim1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_gen1 = Generator(z_dim, img_dim).to(device)\n",
    "loaded_gen_optim1 = optim.Adam(loaded_gen1.parameters(), lr=lr)\n",
    "load_model_info(path, device, model=loaded_gen1, model_name=\"gen1.pt\", \n",
    "                optimizer=loaded_gen_optim1, optimizer_name=\"gen_optim1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
