{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is inspired from the book [Deep Generative Modeling - Jakub M. Tomczak](https://link.springer.com/book/10.1007/978-3-031-64087-2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Causal Convolutions - a replacement to RNNs for long-range memory** \n",
    "\n",
    "- **Causal Convolutions:** To harness parallel computation, one can use convolutional layers that are \"masked\" or designed so that the output at position $d$ depends only on inputs from positions less than (or, in some layers, up to) $d$.  \n",
    "  - **Option A:** In the very first layer, the convolution kernel is masked so that it does not see the current $x_d$.  \n",
    "  - **Option B:** In later layers, the network may use a kernel that can include the current value.\n",
    "- **Dilation:** By using dilated convolutions (i.e., skipping over certain positions), the receptive field - the range of input positions that affect a given outputâ€”can be increased without a proportional increase in the number of layers.\n",
    "- **Advantages:**  \n",
    "  - **Parameter Sharing:** Convolution kernels are reused across positions, making the model efficient.\n",
    "  - **Parallel Computation:** Unlike RNNs, convolutions can be computed in parallel.\n",
    "- **Downside:**  \n",
    "  - **Sampling Speed:** When generating new data (ie, sampling), the autoregressive nature forces one to iterate one position at a time. Even though a forward pass is parallel when evaluating likelihoods, sampling requires a loop over positions, making it slower.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Let's see how to implement a 1D causal convolutional layer in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    A causal 1D convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, A=False, **kwargs):\n",
    "        super(CausalConv1d, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "        self.A = A\n",
    "        self.padding = (kernel_size - 1) * dilation + A * 1\n",
    "        self.conv1d = torch.nn.Conv1d(in_channels, out_channels,\n",
    "                                      kernel_size, stride=1,\n",
    "                                      padding=0,\n",
    "                                      dilation=dilation,\n",
    "                                      **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.pad(x, (self.padding, 0))\n",
    "        conv1d_out = self.conv1d(x)\n",
    "        if self.A:\n",
    "            return conv1d_out[:, :, : -1]\n",
    "        else:\n",
    "            return conv1d_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
