{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abb4f9c4",
   "metadata": {},
   "source": [
    "References:\n",
    "- [Let's build GPT: from scratch, in code, spelled out - Andrej Karpathy](https://youtu.be/kCc8FmEb1nY?si=eJvE37IA6l754PY_)\n",
    "- [Google Colab notebook](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=Yw1LKNCgwjj1)\n",
    "- [Github repo](https://github.com/karpathy/ng-video-lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d194cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "print(\"imports done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51cbfc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameters set!\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64       # how many independent sequences will we process in parallel?\n",
    "block_size = 256      # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "print(\"hyperparameters set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67af6321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file read!\n"
     ]
    }
   ],
   "source": [
    "with open('../tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"file read!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f33c2827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic utility functions defined!\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split, device):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, device):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, device)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "print(\"basic utility functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d39ba7a",
   "metadata": {},
   "source": [
    "**Causal Constraint in Self-Attention** is a core component of the **Transformer Decoder** architecture, particularly in auto-regressive models like **GPT** (Generative Pre-trained Transformer). This is often referred to as **Masked Self-Attention**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Causal Constraint in Transformer Self-Attention**\n",
    "\n",
    "The purpose of this mechanism is to ensure that information flow is **uni-directional** (causal) during the generation or prediction of a sequence, which is essential for language models like GPT that predict the next token based on the previous ones.\n",
    "\n",
    "#### **1. The Necessity of Causal Flow**\n",
    "\n",
    "- **Prediction Context:** When a model is \"about to try to predict the future,\" it must only use information from the past.\n",
    "- **Uni-directional Communication:** Any given token in the sequence (e.g., the fifth token) should only be able to **communicate with tokens at previous locations** (the fourth, third, second, and first) and **not** with future tokens (the sixth, seventh, and eighth).\n",
    "- **Preventing Cheating:** Accessing \"future tokens in the sequence\" would give away the answer and invalidate the training goal of predicting what comes next.\n",
    "\n",
    "#### **2. The Simplified Implementation (Cumulative Average)**\n",
    "\n",
    "Here's a basic, conceptual way to achieve historical context aggregation:\n",
    "- **Simple Aggregation:** The \"easiest way\" for a token to incorporate information from its past is to simply calculate the **average of all the preceding elements** and the current element's vectors (channels).\n",
    "- **Context Vector:** This results in a \"feature vector that summarizes me in the context of my history\".\n",
    "- **Weak Interaction:** This simple sum/average is a \"weak form of interaction\" because it's \"extremely lossy,\" losing information about the relative positioning (\"spatial arrangements\") of the tokens, a limitation that is addressed later in the full self-attention mechanism.\n",
    "\n",
    "---\n",
    "\n",
    "### **Below are 4 ways of incorporating historical context aggregation into the current token**\n",
    "\n",
    "#### **1. For Loop Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf6c48d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4d5b683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "702d5f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))  # bow stands for \"bag of words\"\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]    # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "310bb131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9da034c",
   "metadata": {},
   "source": [
    "#### **2. Matrix Multiplication for Weighted Aggregation**\n",
    "\n",
    "The first challenge is how to calculate a running average (or sum) of all preceding tokens efficiently, without using slow Python `for` loops, as done in the above step (for loop implementation). The solution leverages the power of **matrix multiplication** to achieve **vectorized causal aggregation**.\n",
    "\n",
    "##### **The Lower Triangular Matrix**\n",
    "\n",
    "Any cumulative sum or average can be expressed as a matrix multiplication between the data and a special weight matrix.\n",
    "\n",
    "1.  **Creation of the Matrix (A):** An $N \\times N$ matrix (where $N$ is the sequence length, $T$) is created. It is a **Lower Triangular Matrix**, meaning all values on or below the main diagonal are non-zero (or 1), while all values above the diagonal are zero.\n",
    "2.  **Matrix Multiplication:** The weight matrix $\\mathbf{A}$ is multiplied by the input token embeddings $\\mathbf{X}$. The output is a new matrix $\\mathbf{C}$ containing the aggregated information.\n",
    "    $$\\mathbf{C} = \\mathbf{A} \\cdot \\mathbf{X}$$\n",
    "3.  **Result:** When calculating the $i$-th row of $\\mathbf{C}$, the $i$-th row of $\\mathbf{A}$ is dot-producted with all of $\\mathbf{X}$. Because the first $i-1$ entries of $\\mathbf{A}$'s $i$-th row are non-zero (1s) and the rest are zero, the operation only sums up (or averages) the first $i$ rows of $\\mathbf{X}$ (the current token and all previous tokens). This is also known as a **Causal Mask**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e012c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3,3))   # lower triangular matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c149fcca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [4, 5, 0],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.arange(1,10).reshape(3,3))   # lower triangular matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0f3948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)   # normalize rows so that they sum to 1 (used for averaging)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3378067c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "\n",
    "xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecc860e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if xbow and xbow2 are the same\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b88c2bb",
   "metadata": {},
   "source": [
    "#### **3. Softmax Function**\n",
    "\n",
    "To turn the aggregation into a proper, normalized attention mechanism, the weight matrix $\\mathbf{A}$ is produced using the **Softmax** function:\n",
    "\n",
    "1.  **Masking:** A lower triangular matrix of ones and zeros (the causal mask) is used to fill the upper triangular part of the *unnormalized attention scores* with **negative infinity**.\n",
    "2.  **Softmax:** Applying the **Softmax** function to these scores along the row dimension:\n",
    "    * The negative infinities become **zero** after exponentiation, ensuring future tokens are ignored.\n",
    "    * The non-zero entries (representing the past and current token) are normalized, ensuring the weights for each row sum to **one**, making it a true weighted average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae4183c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "\n",
    "# raise the entries in wei to -inf where we don't want attention weights\n",
    "# this is because softmax(-inf) = 0\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd564749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fc778d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow3 = wei @ x\n",
    "\n",
    "xbow3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43ce4541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if xbow and xbow3 are the same\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca7bd1",
   "metadata": {},
   "source": [
    "#### **4. Self-Attention Mechanism: The Data-Dependent Aggregation**\n",
    "\n",
    "While the matrix trick solved the *efficiency* problem, the simple cumulative average still suffers from being **uniform**â€”it treats all past tokens as equally important. **Self-Attention** solves this by making the aggregation **data-dependent**.\n",
    "\n",
    "The mechanism relies on every token generating three distinct vectors:\n",
    "\n",
    "| Vector | Analogy | Purpose |\n",
    "| :--- | :--- | :--- |\n",
    "| **Query (Q)** | **\"What am I looking for?\"** | Generated by the current token, it is used to interrogate all other tokens. |\n",
    "| **Key (K)** | **\"What do I contain?\"** | Generated by a past token, it is a descriptor of its content, used to align with the Query. |\n",
    "| **Value (V)** | **\"What information will I share?\"** | Generated by a past token, it holds the actual information that will be aggregated by other tokens. |\n",
    "\n",
    "### **The Core Attention Calculation**\n",
    "\n",
    "1.  **Affinity Calculation (Attention Scores):** The attention scores (affinities) between tokens are calculated by taking the **dot product** of the **Query (Q)** of the current token with the **Key (K)** of every other token.\n",
    "    * A high dot product (high score) means the Query found a relevant Key, indicating a strong *affinity* or *interaction strength*.\n",
    "2.  **Scaling and Masking:** The attention scores are normalized by dividing by the square root of the head size ($\\sqrt{d_k}$) to prevent the Softmax function from becoming too \"peaky\" (sharpening to a single token). The causal mask (lower triangular) is applied to ignore future tokens.\n",
    "3.  **Weighted Aggregation:** The masked, scaled scores are passed through **Softmax** to create the final weights, which are then multiplied by the **Value (V)** vectors. The results are summed up to form the output vector for the current token.\n",
    "\n",
    "This final calculation is the **Scaled Dot-Product Attention**:\n",
    "$$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}} + \\text{Mask}\\right) \\mathbf{V}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d326fb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We've seen the below code in version 3 above, and\n",
    "the problem with this is that it assigns equal \"importance\"\n",
    "to all the previous tokens.\n",
    "This is not desirable because we want to assign more importance\n",
    "to some tokens and less to others.\n",
    "\"\"\"\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ x\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e278f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equal \"normalized\" importance to all previous tokens (normalized as the importances sum up to 1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3ffe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Self-attention solves the above problem like so:\n",
    "Every token emits 3 vectors: a key, a query, and a value vector.\n",
    "\n",
    "query: what i'm interested in\n",
    "key: what i have\n",
    "value: what i can give you\n",
    "\n",
    "The dot product of the query vector of the current token with \n",
    "the key vectors of every other token determines the attention weights.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# version 4: self-attention\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32    # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# let's see a single head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "v = value(x) # (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1)    # (B, T, 16) @ (B, 16, T)  --->  (B, T, T)\n",
    "# k.transpose(-2,-1) means transpose the last 2 dimensions\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "# out = wei @ x\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8bc25fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the below weights vector assigns different importances to the previous tokens\n",
    "# as opposed to the previous 3 versions that assigned same importance to the previous tokens\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eea5b2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n",
       "         -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n",
       "        [ 0.6764, -0.5477, -0.2478,  0.3143, -0.1280, -0.2952, -0.4296, -0.1089,\n",
       "         -0.0493,  0.7268,  0.7130, -0.1164,  0.3266,  0.3431, -0.0710,  1.2716],\n",
       "        [ 0.4823, -0.1069, -0.4055,  0.1770,  0.1581, -0.1697,  0.0162,  0.0215,\n",
       "         -0.2490, -0.3773,  0.2787,  0.1629, -0.2895, -0.0676, -0.1416,  1.2194],\n",
       "        [ 0.1971,  0.2856, -0.1303, -0.2655,  0.0668,  0.1954,  0.0281, -0.2451,\n",
       "         -0.4647,  0.0693,  0.1528, -0.2032, -0.2479, -0.1621,  0.1947,  0.7678],\n",
       "        [ 0.2510,  0.7346,  0.5939,  0.2516,  0.2606,  0.7582,  0.5595,  0.3539,\n",
       "         -0.5934, -1.0807, -0.3111, -0.2781, -0.9054,  0.1318, -0.1382,  0.6371],\n",
       "        [ 0.3428,  0.4960,  0.4725,  0.3028,  0.1844,  0.5814,  0.3824,  0.2952,\n",
       "         -0.4897, -0.7705, -0.1172, -0.2541, -0.6892,  0.1979, -0.1513,  0.7666],\n",
       "        [ 0.1866, -0.0964, -0.1430,  0.3059,  0.0834, -0.0069, -0.2047, -0.1535,\n",
       "         -0.0762,  0.3269,  0.3090,  0.0766,  0.0992,  0.1656,  0.1975,  0.7625],\n",
       "        [ 0.1301, -0.0328, -0.4965,  0.2865,  0.2704, -0.2636, -0.0738,  0.3786,\n",
       "          0.0746,  0.0338,  0.0147,  0.3194,  0.2993, -0.1653, -0.0386,  0.3375]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1d162",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9be6e665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1)    # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "# k.transpose(-2,-1) means transpose the last 2 dimensions\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c13353a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494da631",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a2877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21cf22bb",
   "metadata": {},
   "source": [
    "### **The Full Transformer Decoder Block**\n",
    "\n",
    "The full GPT-style decoder block is built by combining this self-attention mechanism with other stabilizing and computational layers.\n",
    "\n",
    "#### **Multi-Head Attention (MHA)**\n",
    "\n",
    "**Multi-Head Attention** improves the model's ability to focus on different types of information simultaneously.\n",
    "\n",
    "* Instead of running one large attention calculation (one \"Head\"), MHA runs **multiple smaller attention heads in parallel**.\n",
    "* Each head learns a different set of Q, K, and V transformations, allowing it to focus on a unique relationship (e.g., one head finds nouns, another finds preceding punctuation).\n",
    "* The output from all parallel heads is then **concatenated** back together.\n",
    "\n",
    "#### **Position-wise Feed Forward Network (FFN)**\n",
    "\n",
    "After the communication (MHA) is complete, the tokens need time to process the newly gathered information. The FFN is a simple, two-layer **Multi-Layer Perceptron (MLP)** applied **independently and identically** to every token's vector.\n",
    "\n",
    "#### **Key Optimizations for Deep Networks**\n",
    "\n",
    "Two crucial techniques make it possible to stack multiple blocks (layers) of MHA and FFN to form a deep Transformer:\n",
    "\n",
    "1.  **Residual Connections (Skip Connections):** These connections ensure that the data input ($\\mathbf{X}$) is added directly back to the output of the sub-layer (MHA or FFN), creating a path for gradients to flow unimpeded directly from the output back to the input layers during training. This prevents the gradients from vanishing in deep networks.\n",
    "    $$\\mathbf{X}_{\\text{out}} = \\mathbf{X}_{\\text{in}} + \\text{SubLayer}(\\mathbf{X}_{\\text{in}})$$\n",
    "2.  **Layer Normalization:** This technique stabilizes the numerical activations within the network. It calculates the mean and variance of the features **across the channels of a single token** (i.e., along the row dimension) and normalizes them, ensuring inputs to sub-layers are consistently distributed. In the modern \"pre-Norm\" architecture, it is applied **before** the MHA and FFN sub-layers.\n",
    "\n",
    "#### **Decoder-Only Architecture (GPT)**\n",
    "\n",
    "The architecture implemented in the video is a **Decoder-Only Transformer**, as used in GPT. This means it only has the blocks that enforce the **causal (masked)** communication. It lacks the separate **Encoder** block and the **Cross-Attention** layers that would be used in sequence-to-sequence tasks like translation, where the model needs to condition its output on an entirely separate source of information (like a French sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd8146d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2772fbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d2004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57468ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
